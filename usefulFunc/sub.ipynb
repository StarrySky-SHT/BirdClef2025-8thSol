{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "394276c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('/root/projects/BirdClef2025/BirdCLEF2023-30th-place-solution-master')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "525d5ad2",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2023-05-24T20:56:06.096182Z",
     "iopub.status.busy": "2023-05-24T20:56:06.095491Z",
     "iopub.status.idle": "2023-05-24T20:56:11.382492Z",
     "shell.execute_reply": "2023-05-24T20:56:11.381532Z"
    },
    "papermill": {
     "duration": 5.298607,
     "end_time": "2023-05-24T20:56:11.385283",
     "exception": false,
     "start_time": "2023-05-24T20:56:06.086676",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import timm\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "import soundfile as sf\n",
    "from  soundfile import SoundFile\n",
    "import librosa as lb\n",
    "import librosa.display as lbd\n",
    "import concurrent.futures\n",
    "from torchaudio.transforms import MelSpectrogram,AmplitudeToDB\n",
    "from torchvision import transforms\n",
    "from transformers import EfficientNetForImageClassification\n",
    "import torchaudio\n",
    "from transformers import AutoConfig\n",
    "from torch.optim.swa_utils import AveragedModel, SWALR\n",
    "from model import BirdClefCNNFCModel,BirdClefCNNModel\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "3529a959",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-24T20:56:11.400513Z",
     "iopub.status.busy": "2023-05-24T20:56:11.400136Z",
     "iopub.status.idle": "2023-05-24T20:56:11.411221Z",
     "shell.execute_reply": "2023-05-24T20:56:11.410152Z"
    },
    "papermill": {
     "duration": 0.021075,
     "end_time": "2023-05-24T20:56:11.413425",
     "exception": false,
     "start_time": "2023-05-24T20:56:11.392350",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# config\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class CFG:\n",
    "    seed = 42\n",
    "    num_classes = 206\n",
    "    batch_size = 128\n",
    "    epochs = 30\n",
    "    PRECISION = 16    \n",
    "    PATIENCE = 8    \n",
    "    img_size = [224,192]\n",
    "    model = \"eca_nfnet_l0\"\n",
    "    pretrained = False            \n",
    "    weight_decay = 1e-3\n",
    "    use_mixup = True\n",
    "    mixup_alpha = 0.2   \n",
    "    use_spec_aug = False\n",
    "    p_spec_aug = 0.5\n",
    "    time_shift_prob = 0.5\n",
    "    gn_prob = 0.5\n",
    "    infer_duration = 5\n",
    "    fmin = 40\n",
    "    fmax = 14000\n",
    "    mel_bins = 192\n",
    "    window_size = 1024\n",
    "    n_fft = 2048\n",
    "    hop_size = 512\n",
    "    use_fsr = False\n",
    "\n",
    "    device = torch.device('cuda')  \n",
    "\n",
    "    train_path = \"/root/projects/BirdClef2025/data/train.csv\"\n",
    "    valid_path = \"/root/projects/BirdClef2025/data/valid.csv\"\n",
    "    test_path = '/root/projects/BirdClef2025/data/test_soundscapes/'\n",
    "    sample_rate = 32000\n",
    "    duration = 5\n",
    "    max_read_samples = 10\n",
    "    lr = 5e-5\n",
    "\n",
    "    scheduler     = 'CosineAnnealingLR'\n",
    "    min_lr        = 5e-7\n",
    "    T_max         = int(30000/batch_size*epochs)+50\n",
    "    T_0           = 25\n",
    "\n",
    "    n_accumulate = 4\n",
    "    \n",
    "    target_columns = \"abethr1 abhori1 abythr1 afbfly1 afdfly1 afecuc1 affeag1 afgfly1 afghor1 afmdov1 afpfly1 afpkin1 afpwag1 afrgos1 afrgrp1 afrjac1 afrthr1 amesun2 augbuz1 bagwea1 barswa bawhor2 bawman1 bcbeat1 beasun2 bkctch1 bkfruw1 blacra1 blacuc1 blakit1 blaplo1 blbpuf2 blcapa2 blfbus1 blhgon1 blhher1 blksaw1 blnmou1 blnwea1 bltapa1 bltbar1 bltori1 blwlap1 brcale1 brcsta1 brctch1 brcwea1 brican1 brobab1 broman1 brosun1 brrwhe3 brtcha1 brubru1 brwwar1 bswdov1 btweye2 bubwar2 butapa1 cabgre1 carcha1 carwoo1 categr ccbeat1 chespa1 chewea1 chibat1 chtapa3 chucis1 cibwar1 cohmar1 colsun2 combul2 combuz1 comsan crefra2 crheag1 crohor1 darbar1 darter3 didcuc1 dotbar1 dutdov1 easmog1 eaywag1 edcsun3 egygoo equaka1 eswdov1 eubeat1 fatrav1 fatwid1 fislov1 fotdro5 gabgos2 gargan gbesta1 gnbcam2 gnhsun1 gobbun1 gobsta5 gobwea1 golher1 grbcam1 grccra1 grecor greegr grewoo2 grwpyt1 gryapa1 grywrw1 gybfis1 gycwar3 gyhbus1 gyhkin1 gyhneg1 gyhspa1 gytbar1 hadibi1 hamerk1 hartur1 helgui hipbab1 hoopoe huncis1 hunsun2 joygre1 kerspa2 klacuc1 kvbsun1 laudov1 lawgol lesmaw1 lessts1 libeat1 litegr litswi1 litwea1 loceag1 lotcor1 lotlap1 luebus1 mabeat1 macshr1 malkin1 marsto1 marsun2 mcptit1 meypar1 moccha1 mouwag1 ndcsun2 nobfly1 norbro1 norcro1 norfis1 norpuf1 nubwoo1 pabspa1 palfly2 palpri1 piecro1 piekin1 pitwhy purgre2 pygbat1 quailf1 ratcis1 raybar1 rbsrob1 rebfir2 rebhor1 reboxp1 reccor reccuc1 reedov1 refbar2 refcro1 reftin1 refwar2 rehblu1 rehwea1 reisee2 rerswa1 rewsta1 rindov rocmar2 rostur1 ruegls1 rufcha2 sacibi2 sccsun2 scrcha1 scthon1 shesta1 sichor1 sincis1 slbgre1 slcbou1 sltnig1 sobfly1 somgre1 somtit4 soucit1 soufis1 spemou2 spepig1 spewea1 spfbar1 spfwea1 spmthr1 spwlap1 squher1 strher strsee1 stusta1 subbus1 supsta1 tacsun1 tafpri1 tamdov1 thrnig1 trobou1 varsun2 vibsta2 vilwea1 vimwea1 walsta1 wbgbir1 wbrcha2 wbswea1 wfbeat1 whbcan1 whbcou1 whbcro2 whbtit5 whbwea1 whbwhe3 whcpri2 whctur2 wheslf1 whhsaw1 whihel1 whrshr1 witswa1 wlwwar wookin1 woosan wtbeat1 yebapa1 yebbar1 yebduc1 yebere1 yebgre1 yebsto1 yeccan1 yefcan yelbis1 yenspu1 yertin1 yesbar1 yespet1 yetgre1 yewgre1\".split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "d5c76daa",
   "metadata": {
    "_kg_hide-input": false,
    "execution": {
     "iopub.execute_input": "2023-05-24T20:56:11.429158Z",
     "iopub.status.busy": "2023-05-24T20:56:11.428754Z",
     "iopub.status.idle": "2023-05-24T20:56:11.589773Z",
     "shell.execute_reply": "2023-05-24T20:56:11.588404Z"
    },
    "papermill": {
     "duration": 0.172374,
     "end_time": "2023-05-24T20:56:11.592399",
     "exception": false,
     "start_time": "2023-05-24T20:56:11.420025",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import math\n",
    "import argparse\n",
    "\n",
    "import librosa\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.nn.parameter import Parameter\n",
    "\n",
    "\n",
    "class DFTBase(nn.Module):\n",
    "    def __init__(self):\n",
    "        r\"\"\"Base class for DFT and IDFT matrix.\n",
    "        \"\"\"\n",
    "        super(DFTBase, self).__init__()\n",
    "\n",
    "    def dft_matrix(self, n):\n",
    "        (x, y) = np.meshgrid(np.arange(n), np.arange(n))\n",
    "        omega = np.exp(-2 * np.pi * 1j / n)\n",
    "        W = np.power(omega, x * y)  # shape: (n, n)\n",
    "        return W\n",
    "\n",
    "    def idft_matrix(self, n):\n",
    "        (x, y) = np.meshgrid(np.arange(n), np.arange(n))\n",
    "        omega = np.exp(2 * np.pi * 1j / n)\n",
    "        W = np.power(omega, x * y)  # shape: (n, n)\n",
    "        return W\n",
    "\n",
    "\n",
    "class DFT(DFTBase):\n",
    "    def __init__(self, n, norm):\n",
    "        r\"\"\"Calculate discrete Fourier transform (DFT), inverse DFT (IDFT, \n",
    "        right DFT (RDFT) RDFT, and inverse RDFT (IRDFT.) \n",
    "\n",
    "        Args:\n",
    "          n: fft window size\n",
    "          norm: None | 'ortho'\n",
    "        \"\"\"\n",
    "        super(DFT, self).__init__()\n",
    "\n",
    "        self.W = self.dft_matrix(n)\n",
    "        self.inv_W = self.idft_matrix(n)\n",
    "\n",
    "        self.W_real = torch.Tensor(np.real(self.W))\n",
    "        self.W_imag = torch.Tensor(np.imag(self.W))\n",
    "        self.inv_W_real = torch.Tensor(np.real(self.inv_W))\n",
    "        self.inv_W_imag = torch.Tensor(np.imag(self.inv_W))\n",
    "\n",
    "        self.n = n\n",
    "        self.norm = norm\n",
    "\n",
    "    def dft(self, x_real, x_imag):\n",
    "        r\"\"\"Calculate DFT of a signal.\n",
    "\n",
    "        Args:\n",
    "            x_real: (n,), real part of a signal\n",
    "            x_imag: (n,), imag part of a signal\n",
    "\n",
    "        Returns:\n",
    "            z_real: (n,), real part of output\n",
    "            z_imag: (n,), imag part of output\n",
    "        \"\"\"\n",
    "        z_real = torch.matmul(x_real, self.W_real) - torch.matmul(x_imag, self.W_imag)\n",
    "        z_imag = torch.matmul(x_imag, self.W_real) + torch.matmul(x_real, self.W_imag)\n",
    "        # shape: (n,)\n",
    "\n",
    "        if self.norm is None:\n",
    "            pass\n",
    "        elif self.norm == 'ortho':\n",
    "            z_real /= math.sqrt(self.n)\n",
    "            z_imag /= math.sqrt(self.n)\n",
    "\n",
    "        return z_real, z_imag\n",
    "\n",
    "    def idft(self, x_real, x_imag):\n",
    "        r\"\"\"Calculate IDFT of a signal.\n",
    "\n",
    "        Args:\n",
    "            x_real: (n,), real part of a signal\n",
    "            x_imag: (n,), imag part of a signal\n",
    "        Returns:\n",
    "            z_real: (n,), real part of output\n",
    "            z_imag: (n,), imag part of output\n",
    "        \"\"\"\n",
    "        z_real = torch.matmul(x_real, self.inv_W_real) - torch.matmul(x_imag, self.inv_W_imag)\n",
    "        z_imag = torch.matmul(x_imag, self.inv_W_real) + torch.matmul(x_real, self.inv_W_imag)\n",
    "        # shape: (n,)\n",
    "\n",
    "        if self.norm is None:\n",
    "            z_real /= self.n\n",
    "        elif self.norm == 'ortho':\n",
    "            z_real /= math.sqrt(n)\n",
    "            z_imag /= math.sqrt(n)\n",
    "\n",
    "        return z_real, z_imag\n",
    "\n",
    "    def rdft(self, x_real):\n",
    "        r\"\"\"Calculate right RDFT of signal.\n",
    "\n",
    "        Args:\n",
    "            x_real: (n,), real part of a signal\n",
    "            x_imag: (n,), imag part of a signal\n",
    "\n",
    "        Returns:\n",
    "            z_real: (n // 2 + 1,), real part of output\n",
    "            z_imag: (n // 2 + 1,), imag part of output\n",
    "        \"\"\"\n",
    "        n_rfft = self.n // 2 + 1\n",
    "        z_real = torch.matmul(x_real, self.W_real[..., 0 : n_rfft])\n",
    "        z_imag = torch.matmul(x_real, self.W_imag[..., 0 : n_rfft])\n",
    "        # shape: (n // 2 + 1,)\n",
    "\n",
    "        if self.norm is None:\n",
    "            pass\n",
    "        elif self.norm == 'ortho':\n",
    "            z_real /= math.sqrt(self.n)\n",
    "            z_imag /= math.sqrt(self.n)\n",
    "\n",
    "        return z_real, z_imag\n",
    "\n",
    "    def irdft(self, x_real, x_imag):\n",
    "        r\"\"\"Calculate IRDFT of signal.\n",
    "        \n",
    "        Args:\n",
    "            x_real: (n // 2 + 1,), real part of a signal\n",
    "            x_imag: (n // 2 + 1,), imag part of a signal\n",
    "\n",
    "        Returns:\n",
    "            z_real: (n,), real part of output\n",
    "            z_imag: (n,), imag part of output\n",
    "        \"\"\"\n",
    "        n_rfft = self.n // 2 + 1\n",
    "\n",
    "        flip_x_real = torch.flip(x_real, dims=(-1,))\n",
    "        flip_x_imag = torch.flip(x_imag, dims=(-1,))\n",
    "        # shape: (n // 2 + 1,)\n",
    "\n",
    "        x_real = torch.cat((x_real, flip_x_real[..., 1 : n_rfft - 1]), dim=-1)\n",
    "        x_imag = torch.cat((x_imag, -1. * flip_x_imag[..., 1 : n_rfft - 1]), dim=-1)\n",
    "        # shape: (n,)\n",
    "\n",
    "        z_real = torch.matmul(x_real, self.inv_W_real) - torch.matmul(x_imag, self.inv_W_imag)\n",
    "        # shape: (n,)\n",
    "\n",
    "        if self.norm is None:\n",
    "            z_real /= self.n\n",
    "        elif self.norm == 'ortho':\n",
    "            z_real /= math.sqrt(n)\n",
    "\n",
    "        return z_real\n",
    "\n",
    "\n",
    "class STFT(DFTBase):\n",
    "    def __init__(self, n_fft=2048, hop_length=None, win_length=None,\n",
    "        window='hann', center=True, pad_mode='reflect', freeze_parameters=True):\n",
    "        r\"\"\"PyTorch implementation of STFT with Conv1d. The function has the \n",
    "        same output as librosa.stft.\n",
    "\n",
    "        Args:\n",
    "            n_fft: int, fft window size, e.g., 2048\n",
    "            hop_length: int, hop length samples, e.g., 441\n",
    "            win_length: int, window length e.g., 2048\n",
    "            window: str, window function name, e.g., 'hann'\n",
    "            center: bool\n",
    "            pad_mode: str, e.g., 'reflect'\n",
    "            freeze_parameters: bool, set to True to freeze all parameters. Set\n",
    "                to False to finetune all parameters.\n",
    "        \"\"\"\n",
    "        super(STFT, self).__init__()\n",
    "\n",
    "        assert pad_mode in ['constant', 'reflect']\n",
    "\n",
    "        self.n_fft = n_fft\n",
    "        self.hop_length = hop_length\n",
    "        self.win_length = win_length\n",
    "        self.window = window\n",
    "        self.center = center\n",
    "        self.pad_mode = pad_mode\n",
    "\n",
    "        # By default, use the entire frame.\n",
    "        if self.win_length is None:\n",
    "            self.win_length = n_fft\n",
    "\n",
    "        # Set the default hop, if it's not already specified.\n",
    "        if self.hop_length is None:\n",
    "            self.hop_length = int(self.win_length // 4)\n",
    "\n",
    "        fft_window = librosa.filters.get_window(window, self.win_length, fftbins=True)\n",
    "\n",
    "        # Pad the window out to n_fft size.\n",
    "        fft_window = librosa.util.pad_center(fft_window, size=n_fft)\n",
    "\n",
    "        # DFT & IDFT matrix.\n",
    "        self.W = self.dft_matrix(n_fft)\n",
    "\n",
    "        out_channels = n_fft // 2 + 1\n",
    "\n",
    "        self.conv_real = nn.Conv1d(in_channels=1, out_channels=out_channels,\n",
    "            kernel_size=n_fft, stride=self.hop_length, padding=0, dilation=1,\n",
    "            groups=1, bias=False)\n",
    "\n",
    "        self.conv_imag = nn.Conv1d(in_channels=1, out_channels=out_channels,\n",
    "            kernel_size=n_fft, stride=self.hop_length, padding=0, dilation=1,\n",
    "            groups=1, bias=False)\n",
    "\n",
    "        # Initialize Conv1d weights.\n",
    "        self.conv_real.weight.data = torch.Tensor(\n",
    "            np.real(self.W[:, 0 : out_channels] * fft_window[:, None]).T)[:, None, :]\n",
    "        # (n_fft // 2 + 1, 1, n_fft)\n",
    "\n",
    "        self.conv_imag.weight.data = torch.Tensor(\n",
    "            np.imag(self.W[:, 0 : out_channels] * fft_window[:, None]).T)[:, None, :]\n",
    "        # (n_fft // 2 + 1, 1, n_fft)\n",
    "\n",
    "        if freeze_parameters:\n",
    "            for param in self.parameters():\n",
    "                param.requires_grad = False\n",
    "\n",
    "    def forward(self, input):\n",
    "        r\"\"\"Calculate STFT of batch of signals.\n",
    "\n",
    "        Args: \n",
    "            input: (batch_size, data_length), input signals.\n",
    "\n",
    "        Returns:\n",
    "            real: (batch_size, 1, time_steps, n_fft // 2 + 1)\n",
    "            imag: (batch_size, 1, time_steps, n_fft // 2 + 1)\n",
    "        \"\"\"\n",
    "\n",
    "        x = input[:, None, :]   # (batch_size, channels_num, data_length)\n",
    "\n",
    "        if self.center:\n",
    "            x = F.pad(x, pad=(self.n_fft // 2, self.n_fft // 2), mode=self.pad_mode)\n",
    "\n",
    "        real = self.conv_real(x)\n",
    "        imag = self.conv_imag(x)\n",
    "        # (batch_size, n_fft // 2 + 1, time_steps)\n",
    "\n",
    "        real = real[:, None, :, :].transpose(2, 3)\n",
    "        imag = imag[:, None, :, :].transpose(2, 3)\n",
    "        # (batch_size, 1, time_steps, n_fft // 2 + 1)\n",
    "\n",
    "        return real, imag\n",
    "\n",
    "\n",
    "def magphase(real, imag):\n",
    "    r\"\"\"Calculate magnitude and phase from real and imag part of signals.\n",
    "\n",
    "    Args:\n",
    "        real: tensor, real part of signals\n",
    "        imag: tensor, imag part of signals\n",
    "\n",
    "    Returns:\n",
    "        mag: tensor, magnitude of signals\n",
    "        cos: tensor, cosine of phases of signals\n",
    "        sin: tensor, sine of phases of signals\n",
    "    \"\"\"\n",
    "    mag = (real ** 2 + imag ** 2) ** 0.5\n",
    "    cos = real / torch.clamp(mag, 1e-10, np.inf)\n",
    "    sin = imag / torch.clamp(mag, 1e-10, np.inf)\n",
    "\n",
    "    return mag, cos, sin\n",
    "\n",
    "\n",
    "class ISTFT(DFTBase):\n",
    "    def __init__(self, n_fft=2048, hop_length=None, win_length=None,\n",
    "        window='hann', center=True, pad_mode='reflect', freeze_parameters=True, \n",
    "        onnx=False, frames_num=None, device=None):\n",
    "        \"\"\"PyTorch implementation of ISTFT with Conv1d. The function has the \n",
    "        same output as librosa.istft.\n",
    "\n",
    "        Args:\n",
    "            n_fft: int, fft window size, e.g., 2048\n",
    "            hop_length: int, hop length samples, e.g., 441\n",
    "            win_length: int, window length e.g., 2048\n",
    "            window: str, window function name, e.g., 'hann'\n",
    "            center: bool\n",
    "            pad_mode: str, e.g., 'reflect'\n",
    "            freeze_parameters: bool, set to True to freeze all parameters. Set\n",
    "                to False to finetune all parameters.\n",
    "            onnx: bool, set to True when exporting trained model to ONNX. This\n",
    "                will replace several operations to operators supported by ONNX.\n",
    "            frames_num: None | int, number of frames of audio clips to be \n",
    "                inferneced. Only useable when onnx=True.\n",
    "            device: None | str, device of ONNX. Only useable when onnx=True.\n",
    "        \"\"\"\n",
    "        super(ISTFT, self).__init__()\n",
    "\n",
    "        assert pad_mode in ['constant', 'reflect']\n",
    "\n",
    "        if not onnx:\n",
    "            assert frames_num is None, \"When onnx=False, frames_num must be None!\"\n",
    "            assert device is None, \"When onnx=False, device must be None!\"\n",
    "\n",
    "        self.n_fft = n_fft\n",
    "        self.hop_length = hop_length\n",
    "        self.win_length = win_length\n",
    "        self.window = window\n",
    "        self.center = center\n",
    "        self.pad_mode = pad_mode\n",
    "        self.onnx = onnx\n",
    "\n",
    "        # By default, use the entire frame.\n",
    "        if self.win_length is None:\n",
    "            self.win_length = self.n_fft\n",
    "\n",
    "        # Set the default hop, if it's not already specified.\n",
    "        if self.hop_length is None:\n",
    "            self.hop_length = int(self.win_length // 4)\n",
    "\n",
    "        # Initialize Conv1d modules for calculating real and imag part of DFT.\n",
    "        self.init_real_imag_conv()\n",
    "\n",
    "        # Initialize overlap add window for reconstruct time domain signals.\n",
    "        self.init_overlap_add_window()\n",
    "\n",
    "        if self.onnx:\n",
    "            # Initialize ONNX modules.\n",
    "            self.init_onnx_modules(frames_num, device)\n",
    "        \n",
    "        if freeze_parameters:\n",
    "            for param in self.parameters():\n",
    "                param.requires_grad = False\n",
    "\n",
    "    def init_real_imag_conv(self):\n",
    "        r\"\"\"Initialize Conv1d for calculating real and imag part of DFT.\n",
    "        \"\"\"\n",
    "        self.W = self.idft_matrix(self.n_fft) / self.n_fft\n",
    "\n",
    "        self.conv_real = nn.Conv1d(in_channels=self.n_fft, out_channels=self.n_fft,\n",
    "            kernel_size=1, stride=1, padding=0, dilation=1,\n",
    "            groups=1, bias=False)\n",
    "\n",
    "        self.conv_imag = nn.Conv1d(in_channels=self.n_fft, out_channels=self.n_fft,\n",
    "            kernel_size=1, stride=1, padding=0, dilation=1,\n",
    "            groups=1, bias=False)\n",
    "\n",
    "        ifft_window = librosa.filters.get_window(self.window, self.win_length, fftbins=True)\n",
    "        # (win_length,)\n",
    "\n",
    "        # Pad the window to n_fft\n",
    "        ifft_window = librosa.util.pad_center(ifft_window, size=self.n_fft)\n",
    "\n",
    "        self.conv_real.weight.data = torch.Tensor(\n",
    "            np.real(self.W * ifft_window[None, :]).T)[:, :, None]\n",
    "        # (n_fft // 2 + 1, 1, n_fft)\n",
    "\n",
    "        self.conv_imag.weight.data = torch.Tensor(\n",
    "            np.imag(self.W * ifft_window[None, :]).T)[:, :, None]\n",
    "        # (n_fft // 2 + 1, 1, n_fft)\n",
    "\n",
    "    def init_overlap_add_window(self):\n",
    "        r\"\"\"Initialize overlap add window for reconstruct time domain signals.\n",
    "        \"\"\"\n",
    "        \n",
    "        ola_window = librosa.filters.get_window(self.window, self.win_length, fftbins=True)\n",
    "        # (win_length,)\n",
    "\n",
    "        ola_window = librosa.util.normalize(ola_window, norm=None) ** 2\n",
    "        ola_window = librosa.util.pad_center(ola_window, size=self.n_fft)\n",
    "        ola_window = torch.Tensor(ola_window)\n",
    "\n",
    "        self.register_buffer('ola_window', ola_window)\n",
    "        # (win_length,)\n",
    "\n",
    "    def init_onnx_modules(self, frames_num, device):\n",
    "        r\"\"\"Initialize ONNX modules.\n",
    "\n",
    "        Args:\n",
    "            frames_num: int\n",
    "            device: str | None\n",
    "        \"\"\"\n",
    "\n",
    "        # Use Conv1d to implement torch.flip(), because torch.flip() is not \n",
    "        # supported by ONNX.\n",
    "        self.reverse = nn.Conv1d(in_channels=self.n_fft // 2 + 1,\n",
    "            out_channels=self.n_fft // 2 - 1, kernel_size=1, bias=False)\n",
    "\n",
    "        tmp = np.zeros((self.n_fft // 2 - 1, self.n_fft // 2 + 1, 1))\n",
    "        tmp[:, 1 : -1, 0] = np.array(np.eye(self.n_fft // 2 - 1)[::-1])\n",
    "        self.reverse.weight.data = torch.Tensor(tmp)\n",
    "        # (n_fft // 2 - 1, n_fft // 2 + 1, 1)\n",
    "\n",
    "        # Use nn.ConvTranspose2d to implement torch.nn.functional.fold(), \n",
    "        # because torch.nn.functional.fold() is not supported by ONNX.\n",
    "        self.overlap_add = nn.ConvTranspose2d(in_channels=self.n_fft,\n",
    "            out_channels=1, kernel_size=(self.n_fft, 1), stride=(self.hop_length, 1), bias=False)\n",
    "\n",
    "        self.overlap_add.weight.data = torch.Tensor(np.eye(self.n_fft)[:, None, :, None])\n",
    "        # (n_fft, 1, n_fft, 1)\n",
    "\n",
    "        if frames_num:\n",
    "            # Pre-calculate overlap-add window sum for reconstructing signals\n",
    "            # when using ONNX.\n",
    "            self.ifft_window_sum = self._get_ifft_window_sum_onnx(frames_num, device)\n",
    "        else:\n",
    "            self.ifft_window_sum = []\n",
    "\n",
    "    def forward(self, real_stft, imag_stft, length):\n",
    "        r\"\"\"Calculate inverse STFT.\n",
    "\n",
    "        Args:\n",
    "            real_stft: (batch_size, channels=1, time_steps, n_fft // 2 + 1)\n",
    "            imag_stft: (batch_size, channels=1, time_steps, n_fft // 2 + 1)\n",
    "            length: int\n",
    "        \n",
    "        Returns:\n",
    "            real: (batch_size, data_length), output signals.\n",
    "        \"\"\"\n",
    "        assert real_stft.ndimension() == 4 and imag_stft.ndimension() == 4\n",
    "        batch_size, _, frames_num, _ = real_stft.shape\n",
    "\n",
    "        real_stft = real_stft[:, 0, :, :].transpose(1, 2)\n",
    "        imag_stft = imag_stft[:, 0, :, :].transpose(1, 2)\n",
    "        # (batch_size, n_fft // 2 + 1, time_steps)\n",
    "\n",
    "        # Get full stft representation from spectrum using symmetry attribute.\n",
    "        if self.onnx:\n",
    "            full_real_stft, full_imag_stft = self._get_full_stft_onnx(real_stft, imag_stft)\n",
    "        else:\n",
    "            full_real_stft, full_imag_stft = self._get_full_stft(real_stft, imag_stft)\n",
    "        # full_real_stft: (batch_size, n_fft, time_steps)\n",
    "        # full_imag_stft: (batch_size, n_fft, time_steps)\n",
    "\n",
    "        # Calculate IDFT frame by frame.\n",
    "        s_real = self.conv_real(full_real_stft) - self.conv_imag(full_imag_stft)\n",
    "        # (batch_size, n_fft, time_steps)\n",
    "\n",
    "        # Overlap add signals in frames to reconstruct signals.\n",
    "        if self.onnx:\n",
    "            y = self._overlap_add_divide_window_sum_onnx(s_real, frames_num)\n",
    "        else:\n",
    "            y = self._overlap_add_divide_window_sum(s_real, frames_num)\n",
    "        # y: (batch_size, audio_samples + win_length,)\n",
    "        \n",
    "        y = self._trim_edges(y, length)\n",
    "        # (batch_size, audio_samples,)\n",
    "            \n",
    "        return y\n",
    "\n",
    "    def _get_full_stft(self, real_stft, imag_stft):\n",
    "        r\"\"\"Get full stft representation from spectrum using symmetry attribute.\n",
    "\n",
    "        Args:\n",
    "            real_stft: (batch_size, n_fft // 2 + 1, time_steps)\n",
    "            imag_stft: (batch_size, n_fft // 2 + 1, time_steps)\n",
    "\n",
    "        Returns:\n",
    "            full_real_stft: (batch_size, n_fft, time_steps)\n",
    "            full_imag_stft: (batch_size, n_fft, time_steps)\n",
    "        \"\"\"\n",
    "        full_real_stft = torch.cat((real_stft, torch.flip(real_stft[:, 1 : -1, :], dims=[1])), dim=1)\n",
    "        full_imag_stft = torch.cat((imag_stft, - torch.flip(imag_stft[:, 1 : -1, :], dims=[1])), dim=1)\n",
    "\n",
    "        return full_real_stft, full_imag_stft\n",
    "\n",
    "    def _get_full_stft_onnx(self, real_stft, imag_stft):\n",
    "        r\"\"\"Get full stft representation from spectrum using symmetry attribute\n",
    "        for ONNX. Replace several pytorch operations in self._get_full_stft() \n",
    "        that are not supported by ONNX.\n",
    "\n",
    "        Args:\n",
    "            real_stft: (batch_size, n_fft // 2 + 1, time_steps)\n",
    "            imag_stft: (batch_size, n_fft // 2 + 1, time_steps)\n",
    "\n",
    "        Returns:\n",
    "            full_real_stft: (batch_size, n_fft, time_steps)\n",
    "            full_imag_stft: (batch_size, n_fft, time_steps)\n",
    "        \"\"\"\n",
    "\n",
    "        # Implement torch.flip() with Conv1d.\n",
    "        full_real_stft = torch.cat((real_stft, self.reverse(real_stft)), dim=1)\n",
    "        full_imag_stft = torch.cat((imag_stft, - self.reverse(imag_stft)), dim=1)\n",
    "\n",
    "        return full_real_stft, full_imag_stft\n",
    "\n",
    "    def _overlap_add_divide_window_sum(self, s_real, frames_num):\n",
    "        r\"\"\"Overlap add signals in frames to reconstruct signals.\n",
    "\n",
    "        Args:\n",
    "            s_real: (batch_size, n_fft, time_steps), signals in frames\n",
    "            frames_num: int\n",
    "\n",
    "        Returns:\n",
    "            y: (batch_size, audio_samples)\n",
    "        \"\"\"\n",
    "        \n",
    "        output_samples = (s_real.shape[-1] - 1) * self.hop_length + self.win_length\n",
    "        # (audio_samples,)\n",
    "\n",
    "        # Overlap-add signals in frames to signals. Ref: \n",
    "        # asteroid_filterbanks.torch_stft_fb.torch_stft_fb() from\n",
    "        # https://github.com/asteroid-team/asteroid-filterbanks\n",
    "        y = torch.nn.functional.fold(input=s_real, output_size=(1, output_samples), \n",
    "            kernel_size=(1, self.win_length), stride=(1, self.hop_length))\n",
    "        # (batch_size, 1, 1, audio_samples,)\n",
    "        \n",
    "        y = y[:, 0, 0, :]\n",
    "        # (batch_size, audio_samples)\n",
    "\n",
    "        # Get overlap-add window sum to be divided.\n",
    "        ifft_window_sum = self._get_ifft_window(frames_num)\n",
    "        # (audio_samples,)\n",
    "\n",
    "        # Following code is abandaned for divide overlap-add window, because\n",
    "        # not supported by half precision training and ONNX.\n",
    "        # min_mask = ifft_window_sum.abs() < 1e-11\n",
    "        # y[:, ~min_mask] = y[:, ~min_mask] / ifft_window_sum[None, ~min_mask]\n",
    "        # # (batch_size, audio_samples)\n",
    "\n",
    "        ifft_window_sum = torch.clamp(ifft_window_sum, 1e-11, np.inf)\n",
    "        # (audio_samples,)\n",
    "\n",
    "        y = y / ifft_window_sum[None, :]\n",
    "        # (batch_size, audio_samples,)\n",
    "\n",
    "        return y\n",
    "\n",
    "    def _get_ifft_window(self, frames_num):\n",
    "        r\"\"\"Get overlap-add window sum to be divided.\n",
    "\n",
    "        Args:\n",
    "            frames_num: int\n",
    "\n",
    "        Returns:\n",
    "            ifft_window_sum: (audio_samlpes,), overlap-add window sum to be \n",
    "            divided.\n",
    "        \"\"\"\n",
    "        \n",
    "        output_samples = (frames_num - 1) * self.hop_length + self.win_length\n",
    "        # (audio_samples,)\n",
    "\n",
    "        window_matrix = self.ola_window[None, :, None].repeat(1, 1, frames_num)\n",
    "        # (batch_size, win_length, time_steps)\n",
    "\n",
    "        ifft_window_sum = F.fold(input=window_matrix, \n",
    "            output_size=(1, output_samples), kernel_size=(1, self.win_length), \n",
    "            stride=(1, self.hop_length))\n",
    "        # (1, 1, 1, audio_samples)\n",
    "        \n",
    "        ifft_window_sum = ifft_window_sum.squeeze()\n",
    "        # (audio_samlpes,)\n",
    "\n",
    "        return ifft_window_sum\n",
    "\n",
    "    def _overlap_add_divide_window_sum_onnx(self, s_real, frames_num):\n",
    "        r\"\"\"Overlap add signals in frames to reconstruct signals for ONNX. \n",
    "        Replace several pytorch operations in \n",
    "        self._overlap_add_divide_window_sum() that are not supported by ONNX.\n",
    "\n",
    "        Args:\n",
    "            s_real: (batch_size, n_fft, time_steps), signals in frames\n",
    "            frames_num: int\n",
    "\n",
    "        Returns:\n",
    "            y: (batch_size, audio_samples)\n",
    "        \"\"\"\n",
    "\n",
    "        s_real = s_real[..., None]\n",
    "        # (batch_size, n_fft, time_steps, 1)\n",
    "\n",
    "        # Implement overlap-add with Conv1d, because torch.nn.functional.fold()\n",
    "        # is not supported by ONNX.\n",
    "        y = self.overlap_add(s_real)[:, 0, :, 0]    \n",
    "        # y: (batch_size, samples_num)\n",
    "        \n",
    "        if len(self.ifft_window_sum) != y.shape[1]:\n",
    "            device = s_real.device\n",
    "\n",
    "            self.ifft_window_sum = self._get_ifft_window_sum_onnx(frames_num, device)\n",
    "            # (audio_samples,)\n",
    "\n",
    "        # Use torch.clamp() to prevent from underflow to make sure all \n",
    "        # operations are supported by ONNX.\n",
    "        ifft_window_sum = torch.clamp(self.ifft_window_sum, 1e-11, np.inf)\n",
    "        # (audio_samples,)\n",
    "\n",
    "        y = y / ifft_window_sum[None, :]\n",
    "        # (batch_size, audio_samples,)\n",
    "        \n",
    "        return y\n",
    "\n",
    "    def _get_ifft_window_sum_onnx(self, frames_num, device):\n",
    "        r\"\"\"Pre-calculate overlap-add window sum for reconstructing signals when\n",
    "        using ONNX.\n",
    "\n",
    "        Args:\n",
    "            frames_num: int\n",
    "            device: str | None\n",
    "\n",
    "        Returns:\n",
    "            ifft_window_sum: (audio_samples,)\n",
    "        \"\"\"\n",
    "        \n",
    "        ifft_window_sum = librosa.filters.window_sumsquare(window=self.window, \n",
    "            n_frames=frames_num, win_length=self.win_length, n_fft=self.n_fft, \n",
    "            hop_length=self.hop_length)\n",
    "        # (audio_samples,)\n",
    "\n",
    "        ifft_window_sum = torch.Tensor(ifft_window_sum)\n",
    "\n",
    "        if device:\n",
    "            ifft_window_sum = ifft_window_sum.to(device)\n",
    "\n",
    "        return ifft_window_sum\n",
    "\n",
    "    def _trim_edges(self, y, length):\n",
    "        r\"\"\"Trim audio.\n",
    "\n",
    "        Args:\n",
    "            y: (audio_samples,)\n",
    "            length: int\n",
    "\n",
    "        Returns:\n",
    "            (trimmed_audio_samples,)\n",
    "        \"\"\"\n",
    "        # Trim or pad to length\n",
    "        if length is None:\n",
    "            if self.center:\n",
    "                y = y[:, self.n_fft // 2 : -self.n_fft // 2]\n",
    "        else:\n",
    "            if self.center:\n",
    "                start = self.n_fft // 2\n",
    "            else:\n",
    "                start = 0\n",
    "\n",
    "            y = y[:, start : start + length]\n",
    "\n",
    "        return y\n",
    "\n",
    "\n",
    "class Spectrogram(nn.Module):\n",
    "    def __init__(self, n_fft=2048, hop_length=None, win_length=None,\n",
    "        window='hann', center=True, pad_mode='reflect', power=2.0,\n",
    "        freeze_parameters=True):\n",
    "        r\"\"\"Calculate spectrogram using pytorch. The STFT is implemented with \n",
    "        Conv1d. The function has the same output of librosa.stft\n",
    "        \"\"\"\n",
    "        super(Spectrogram, self).__init__()\n",
    "\n",
    "        self.power = power\n",
    "\n",
    "        self.stft = STFT(n_fft=n_fft, hop_length=hop_length,\n",
    "            win_length=win_length, window=window, center=center,\n",
    "            pad_mode=pad_mode, freeze_parameters=True)\n",
    "\n",
    "    def forward(self, input):\n",
    "        r\"\"\"Calculate spectrogram of input signals.\n",
    "        Args: \n",
    "            input: (batch_size, data_length)\n",
    "\n",
    "        Returns:\n",
    "            spectrogram: (batch_size, 1, time_steps, n_fft // 2 + 1)\n",
    "        \"\"\"\n",
    "\n",
    "        (real, imag) = self.stft.forward(input)\n",
    "        # (batch_size, n_fft // 2 + 1, time_steps)\n",
    "\n",
    "        spectrogram = real ** 2 + imag ** 2\n",
    "\n",
    "        if self.power == 2.0:\n",
    "            pass\n",
    "        else:\n",
    "            spectrogram = spectrogram ** (self.power / 2.0)\n",
    "\n",
    "        return spectrogram\n",
    "\n",
    "\n",
    "class LogmelFilterBank(nn.Module):\n",
    "    def __init__(self, sr=22050, n_fft=2048, n_mels=64, fmin=0.0, fmax=None, \n",
    "        is_log=True, ref=1.0, amin=1e-10, top_db=80.0, freeze_parameters=True):\n",
    "        r\"\"\"Calculate logmel spectrogram using pytorch. The mel filter bank is \n",
    "        the pytorch implementation of as librosa.filters.mel \n",
    "        \"\"\"\n",
    "        super(LogmelFilterBank, self).__init__()\n",
    "\n",
    "        self.is_log = is_log\n",
    "        self.ref = ref\n",
    "        self.amin = amin\n",
    "        self.top_db = top_db\n",
    "        if fmax == None:\n",
    "            fmax = sr//2\n",
    "\n",
    "        self.melW = librosa.filters.mel(sr=sr, n_fft=n_fft, n_mels=n_mels,\n",
    "            fmin=fmin, fmax=fmax).T\n",
    "        # (n_fft // 2 + 1, mel_bins)\n",
    "\n",
    "        self.melW = nn.Parameter(torch.Tensor(self.melW))\n",
    "\n",
    "        if freeze_parameters:\n",
    "            for param in self.parameters():\n",
    "                param.requires_grad = False\n",
    "\n",
    "    def forward(self, input):\n",
    "        r\"\"\"Calculate (log) mel spectrogram from spectrogram.\n",
    "\n",
    "        Args:\n",
    "            input: (*, n_fft), spectrogram\n",
    "        \n",
    "        Returns: \n",
    "            output: (*, mel_bins), (log) mel spectrogram\n",
    "        \"\"\"\n",
    "\n",
    "        # Mel spectrogram\n",
    "        mel_spectrogram = torch.matmul(input, self.melW)\n",
    "        # (*, mel_bins)\n",
    "\n",
    "        # Logmel spectrogram\n",
    "        if self.is_log:\n",
    "            output = self.power_to_db(mel_spectrogram)\n",
    "        else:\n",
    "            output = mel_spectrogram\n",
    "\n",
    "        return output\n",
    "\n",
    "\n",
    "    def power_to_db(self, input):\n",
    "        r\"\"\"Power to db, this function is the pytorch implementation of \n",
    "        librosa.power_to_lb\n",
    "        \"\"\"\n",
    "        ref_value = self.ref\n",
    "        log_spec = 10.0 * torch.log10(torch.clamp(input, min=self.amin, max=np.inf))\n",
    "        log_spec -= 10.0 * np.log10(np.maximum(self.amin, ref_value))\n",
    "\n",
    "        if self.top_db is not None:\n",
    "            if self.top_db < 0:\n",
    "                raise librosa.util.exceptions.ParameterError('top_db must be non-negative')\n",
    "            log_spec = torch.clamp(log_spec, min=log_spec.max().item() - self.top_db, max=np.inf)\n",
    "\n",
    "        return log_spec\n",
    "\n",
    "\n",
    "class Enframe(nn.Module):\n",
    "    def __init__(self, frame_length=2048, hop_length=512):\n",
    "        r\"\"\"Enframe a time sequence. This function is the pytorch implementation \n",
    "        of librosa.util.frame\n",
    "        \"\"\"\n",
    "        super(Enframe, self).__init__()\n",
    "\n",
    "        self.enframe_conv = nn.Conv1d(in_channels=1, out_channels=frame_length,\n",
    "            kernel_size=frame_length, stride=hop_length,\n",
    "            padding=0, bias=False)\n",
    "\n",
    "        self.enframe_conv.weight.data = torch.Tensor(torch.eye(frame_length)[:, None, :])\n",
    "        self.enframe_conv.weight.requires_grad = False\n",
    "\n",
    "    def forward(self, input):\n",
    "        r\"\"\"Enframe signals into frames.\n",
    "        Args:\n",
    "            input: (batch_size, samples)\n",
    "        \n",
    "        Returns: \n",
    "            output: (batch_size, window_length, frames_num)\n",
    "        \"\"\"\n",
    "        output = self.enframe_conv(input[:, None, :])\n",
    "        return output\n",
    "\n",
    "\n",
    "    def power_to_db(self, input):\n",
    "        r\"\"\"Power to db, this function is the pytorch implementation of \n",
    "        librosa.power_to_lb.\n",
    "        \"\"\"\n",
    "        ref_value = self.ref\n",
    "        log_spec = 10.0 * torch.log10(torch.clamp(input, min=self.amin, max=np.inf))\n",
    "        log_spec -= 10.0 * np.log10(np.maximum(self.amin, ref_value))\n",
    "\n",
    "        if self.top_db is not None:\n",
    "            if self.top_db < 0:\n",
    "                raise librosa.util.exceptions.ParameterError('top_db must be non-negative')\n",
    "            log_spec = torch.clamp(log_spec, min=log_spec.max() - self.top_db, max=np.inf)\n",
    "\n",
    "        return log_spec\n",
    "\n",
    "\n",
    "class Scalar(nn.Module):\n",
    "    def __init__(self, scalar, freeze_parameters):\n",
    "        super(Scalar, self).__init__()\n",
    "\n",
    "        self.scalar_mean = Parameter(torch.Tensor(scalar['mean']))\n",
    "        self.scalar_std = Parameter(torch.Tensor(scalar['std']))\n",
    "\n",
    "        if freeze_parameters:\n",
    "            for param in self.parameters():\n",
    "                param.requires_grad = False\n",
    "\n",
    "    def forward(self, input):\n",
    "        return (input - self.scalar_mean) / self.scalar_std\n",
    "\n",
    "\n",
    "def debug(select, device):\n",
    "    \"\"\"Compare numpy + librosa and torchlibrosa results. For debug. \n",
    "\n",
    "    Args:\n",
    "        select: 'dft' | 'logmel'\n",
    "        device: 'cpu' | 'cuda'\n",
    "    \"\"\"\n",
    "\n",
    "    if select == 'dft':\n",
    "        n = 10\n",
    "        norm = None     # None | 'ortho'\n",
    "        np.random.seed(0)\n",
    "\n",
    "        # Data\n",
    "        np_data = np.random.uniform(-1, 1, n)\n",
    "        pt_data = torch.Tensor(np_data)\n",
    "\n",
    "        # Numpy FFT\n",
    "        np_fft = np.fft.fft(np_data, norm=norm)\n",
    "        np_ifft = np.fft.ifft(np_fft, norm=norm)\n",
    "        np_rfft = np.fft.rfft(np_data, norm=norm)\n",
    "        np_irfft = np.fft.ifft(np_rfft, norm=norm)\n",
    "\n",
    "        # Pytorch FFT\n",
    "        obj = DFT(n, norm)\n",
    "        pt_dft = obj.dft(pt_data, torch.zeros_like(pt_data))\n",
    "        pt_idft = obj.idft(pt_dft[0], pt_dft[1])\n",
    "        pt_rdft = obj.rdft(pt_data)\n",
    "        pt_irdft = obj.irdft(pt_rdft[0], pt_rdft[1])\n",
    "\n",
    "        print('Comparing librosa and pytorch implementation of DFT. All numbers '\n",
    "            'below should be close to 0.')\n",
    "        print(np.mean((np.abs(np.real(np_fft) - pt_dft[0].cpu().numpy()))))\n",
    "        print(np.mean((np.abs(np.imag(np_fft) - pt_dft[1].cpu().numpy()))))\n",
    "\n",
    "        print(np.mean((np.abs(np.real(np_ifft) - pt_idft[0].cpu().numpy()))))\n",
    "        print(np.mean((np.abs(np.imag(np_ifft) - pt_idft[1].cpu().numpy()))))\n",
    "\n",
    "        print(np.mean((np.abs(np.real(np_rfft) - pt_rdft[0].cpu().numpy()))))\n",
    "        print(np.mean((np.abs(np.imag(np_rfft) - pt_rdft[1].cpu().numpy()))))\n",
    "\n",
    "        print(np.mean(np.abs(np_data - pt_irdft.cpu().numpy())))\n",
    "\n",
    "    elif select == 'stft':\n",
    "        device = torch.device(device)\n",
    "        np.random.seed(0)\n",
    "\n",
    "        # Spectrogram parameters (the same as librosa.stft)\n",
    "        sample_rate = 22050\n",
    "        data_length = sample_rate * 1\n",
    "        n_fft = 2048\n",
    "        hop_length = 512\n",
    "        win_length = 2048\n",
    "        window = 'hann'\n",
    "        center = True\n",
    "        pad_mode = 'reflect'\n",
    "\n",
    "        # Data\n",
    "        np_data = np.random.uniform(-1, 1, data_length)\n",
    "        pt_data = torch.Tensor(np_data).to(device)\n",
    "\n",
    "        # Numpy stft matrix\n",
    "        np_stft_matrix = librosa.stft(y=np_data, n_fft=n_fft,\n",
    "            hop_length=hop_length, window=window, center=center).T\n",
    "\n",
    "        # Pytorch stft matrix\n",
    "        pt_stft_extractor = STFT(n_fft=n_fft, hop_length=hop_length,\n",
    "            win_length=win_length, window=window, center=center, pad_mode=pad_mode,\n",
    "            freeze_parameters=True)\n",
    "\n",
    "        pt_stft_extractor.to(device)\n",
    "\n",
    "        (pt_stft_real, pt_stft_imag) = pt_stft_extractor.forward(pt_data[None, :])\n",
    "\n",
    "        print('Comparing librosa and pytorch implementation of STFT & ISTFT. \\\n",
    "            All numbers below should be close to 0.')\n",
    "        print(np.mean(np.abs(np.real(np_stft_matrix) - pt_stft_real.data.cpu().numpy()[0, 0])))\n",
    "        print(np.mean(np.abs(np.imag(np_stft_matrix) - pt_stft_imag.data.cpu().numpy()[0, 0])))\n",
    "\n",
    "        # Numpy istft\n",
    "        np_istft_s = librosa.istft(stft_matrix=np_stft_matrix.T,\n",
    "            hop_length=hop_length, window=window, center=center, length=data_length)\n",
    "\n",
    "        # Pytorch istft\n",
    "        pt_istft_extractor = ISTFT(n_fft=n_fft, hop_length=hop_length,\n",
    "            win_length=win_length, window=window, center=center, pad_mode=pad_mode,\n",
    "            freeze_parameters=True)\n",
    "        pt_istft_extractor.to(device)\n",
    "\n",
    "        # Recover from real and imag part\n",
    "        pt_istft_s = pt_istft_extractor.forward(pt_stft_real, pt_stft_imag, data_length)[0, :]\n",
    "\n",
    "        # Recover from magnitude and phase\n",
    "        (pt_stft_mag, cos, sin) = magphase(pt_stft_real, pt_stft_imag)\n",
    "        pt_istft_s2 = pt_istft_extractor.forward(pt_stft_mag * cos, pt_stft_mag * sin, data_length)[0, :]\n",
    "\n",
    "        print(np.mean(np.abs(np_istft_s - pt_istft_s.data.cpu().numpy())))\n",
    "        print(np.mean(np.abs(np_data - pt_istft_s.data.cpu().numpy())))\n",
    "        print(np.mean(np.abs(np_data - pt_istft_s2.data.cpu().numpy())))\n",
    "\n",
    "    elif select == 'logmel':\n",
    "        dtype = np.complex64\n",
    "        device = torch.device(device)\n",
    "        np.random.seed(0)\n",
    "\n",
    "        # Spectrogram parameters (the same as librosa.stft)\n",
    "        sample_rate = 22050\n",
    "        data_length = sample_rate * 1\n",
    "        n_fft = 2048\n",
    "        hop_length = 512\n",
    "        win_length = 2048\n",
    "        window = 'hann'\n",
    "        center = True\n",
    "        pad_mode = 'reflect'\n",
    "\n",
    "        # Mel parameters (the same as librosa.feature.melspectrogram)\n",
    "        n_mels = 128\n",
    "        fmin = 0.\n",
    "        fmax = sample_rate / 2.0\n",
    "\n",
    "        # Power to db parameters (the same as default settings of librosa.power_to_db\n",
    "        ref = 1.0\n",
    "        amin = 1e-10\n",
    "        top_db = 80.0\n",
    "\n",
    "        # Data\n",
    "        np_data = np.random.uniform(-1, 1, data_length)\n",
    "        pt_data = torch.Tensor(np_data).to(device)\n",
    "\n",
    "        print('Comparing librosa and pytorch implementation of logmel '\n",
    "            'spectrogram. All numbers below should be close to 0.')\n",
    "\n",
    "        # Numpy librosa\n",
    "        np_stft_matrix = librosa.stft(y=np_data, n_fft=n_fft, hop_length=hop_length,\n",
    "            win_length=win_length, window=window, center=center, dtype=dtype,\n",
    "            pad_mode=pad_mode)\n",
    "\n",
    "        np_pad = np.pad(np_data, int(n_fft // 2), mode=pad_mode)\n",
    "\n",
    "        np_melW = librosa.filters.mel(sr=sample_rate, n_fft=n_fft, n_mels=n_mels,\n",
    "            fmin=fmin, fmax=fmax).T\n",
    "\n",
    "        np_mel_spectrogram = np.dot(np.abs(np_stft_matrix.T) ** 2, np_melW)\n",
    "\n",
    "        np_logmel_spectrogram = librosa.power_to_db(\n",
    "            np_mel_spectrogram, ref=ref, amin=amin, top_db=top_db)\n",
    "\n",
    "        # Pytorch\n",
    "        stft_extractor = STFT(n_fft=n_fft, hop_length=hop_length,\n",
    "            win_length=win_length, window=window, center=center, pad_mode=pad_mode,\n",
    "            freeze_parameters=True)\n",
    "\n",
    "        logmel_extractor = LogmelFilterBank(sr=sample_rate, n_fft=n_fft,\n",
    "            n_mels=n_mels, fmin=fmin, fmax=fmax, ref=ref, amin=amin,\n",
    "            top_db=top_db, freeze_parameters=True)\n",
    "\n",
    "        stft_extractor.to(device)\n",
    "        logmel_extractor.to(device)\n",
    "\n",
    "        pt_pad = F.pad(pt_data[None, None, :], pad=(n_fft // 2, n_fft // 2), mode=pad_mode)[0, 0]\n",
    "        print(np.mean(np.abs(np_pad - pt_pad.cpu().numpy())))\n",
    "\n",
    "        pt_stft_matrix_real = stft_extractor.conv_real(pt_pad[None, None, :])[0]\n",
    "        pt_stft_matrix_imag = stft_extractor.conv_imag(pt_pad[None, None, :])[0]\n",
    "        print(np.mean(np.abs(np.real(np_stft_matrix) - pt_stft_matrix_real.data.cpu().numpy())))\n",
    "        print(np.mean(np.abs(np.imag(np_stft_matrix) - pt_stft_matrix_imag.data.cpu().numpy())))\n",
    "\n",
    "        # Spectrogram\n",
    "        spectrogram_extractor = Spectrogram(n_fft=n_fft, hop_length=hop_length,\n",
    "            win_length=win_length, window=window, center=center, pad_mode=pad_mode,\n",
    "            freeze_parameters=True)\n",
    "\n",
    "        spectrogram_extractor.to(device)\n",
    "\n",
    "        pt_spectrogram = spectrogram_extractor.forward(pt_data[None, :])\n",
    "        pt_mel_spectrogram = torch.matmul(pt_spectrogram, logmel_extractor.melW)\n",
    "        print(np.mean(np.abs(np_mel_spectrogram - pt_mel_spectrogram.data.cpu().numpy()[0, 0])))\n",
    "\n",
    "        # Log mel spectrogram\n",
    "        pt_logmel_spectrogram = logmel_extractor.forward(pt_spectrogram)\n",
    "        print(np.mean(np.abs(np_logmel_spectrogram - pt_logmel_spectrogram[0, 0].data.cpu().numpy())))\n",
    "\n",
    "    elif select == 'enframe':\n",
    "        device = torch.device(device)\n",
    "        np.random.seed(0)\n",
    "\n",
    "        # Spectrogram parameters (the same as librosa.stft)\n",
    "        sample_rate = 22050\n",
    "        data_length = sample_rate * 1\n",
    "        hop_length = 512\n",
    "        win_length = 2048\n",
    "\n",
    "        # Data\n",
    "        np_data = np.random.uniform(-1, 1, data_length)\n",
    "        pt_data = torch.Tensor(np_data).to(device)\n",
    "\n",
    "        print('Comparing librosa and pytorch implementation of '\n",
    "            'librosa.util.frame. All numbers below should be close to 0.')\n",
    "\n",
    "        # Numpy librosa\n",
    "        np_frames = librosa.util.frame(np_data, frame_length=win_length,\n",
    "            hop_length=hop_length)\n",
    "\n",
    "        # Pytorch\n",
    "        pt_frame_extractor = Enframe(frame_length=win_length, hop_length=hop_length)\n",
    "        pt_frame_extractor.to(device)\n",
    "\n",
    "        pt_frames = pt_frame_extractor(pt_data[None, :])\n",
    "        print(np.mean(np.abs(np_frames - pt_frames.data.cpu().numpy())))\n",
    "\n",
    "    elif select == 'default':\n",
    "        device = torch.device(device)\n",
    "        np.random.seed(0)\n",
    "\n",
    "        # Spectrogram parameters (the same as librosa.stft)\n",
    "        sample_rate = 22050\n",
    "        data_length = sample_rate * 1\n",
    "        hop_length = 512\n",
    "        win_length = 2048\n",
    "\n",
    "        # Mel parameters (the same as librosa.feature.melspectrogram)\n",
    "        n_mels = 128\n",
    "\n",
    "        # Data\n",
    "        np_data = np.random.uniform(-1, 1, data_length)\n",
    "        pt_data = torch.Tensor(np_data).to(device)\n",
    "\n",
    "        feature_extractor = nn.Sequential(\n",
    "            Spectrogram(\n",
    "                hop_length=hop_length,\n",
    "                win_length=win_length,\n",
    "            ), LogmelFilterBank(\n",
    "                sr=sample_rate,\n",
    "                n_mels=n_mels,\n",
    "                is_log=False, #Default is true\n",
    "            ))\n",
    "\n",
    "        feature_extractor.to(device)\n",
    "\n",
    "        print(\n",
    "            'Comparing default mel spectrogram from librosa to the pytorch implementation.'\n",
    "        )\n",
    "\n",
    "        # Numpy librosa\n",
    "        np_melspect = librosa.feature.melspectrogram(np_data,\n",
    "                                                     hop_length=hop_length,\n",
    "                                                     sr=sample_rate,\n",
    "                                                     win_length=win_length,\n",
    "                                                     n_mels=n_mels).T\n",
    "        #Pytorch\n",
    "        pt_melspect = feature_extractor(pt_data[None, :]).squeeze()\n",
    "        passed = np.allclose(pt_melspect.data.to('cpu').numpy(), np_melspect)\n",
    "        print(f\"Passed? {passed}\")\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "class DropStripes(nn.Module):\n",
    "    def __init__(self, dim, drop_width, stripes_num):\n",
    "        \"\"\"Drop stripes. \n",
    "\n",
    "        Args:\n",
    "          dim: int, dimension along which to drop\n",
    "          drop_width: int, maximum width of stripes to drop\n",
    "          stripes_num: int, how many stripes to drop\n",
    "        \"\"\"\n",
    "        super(DropStripes, self).__init__()\n",
    "\n",
    "        assert dim in [2, 3]    # dim 2: time; dim 3: frequency\n",
    "\n",
    "        self.dim = dim\n",
    "        self.drop_width = drop_width\n",
    "        self.stripes_num = stripes_num\n",
    "\n",
    "    def forward(self, input):\n",
    "        \"\"\"input: (batch_size, channels, time_steps, freq_bins)\"\"\"\n",
    "\n",
    "        assert input.ndimension() == 4\n",
    "\n",
    "        if self.training is False:\n",
    "            return input\n",
    "\n",
    "        else:\n",
    "            batch_size = input.shape[0]\n",
    "            total_width = input.shape[self.dim]\n",
    "\n",
    "            for n in range(batch_size):\n",
    "                self.transform_slice(input[n], total_width)\n",
    "\n",
    "            return input\n",
    "\n",
    "\n",
    "    def transform_slice(self, e, total_width):\n",
    "        \"\"\"e: (channels, time_steps, freq_bins)\"\"\"\n",
    "\n",
    "        for _ in range(self.stripes_num):\n",
    "            distance = torch.randint(low=0, high=self.drop_width, size=(1,))[0]\n",
    "            bgn = torch.randint(low=0, high=total_width - distance, size=(1,))[0]\n",
    "\n",
    "            if self.dim == 2:\n",
    "                e[:, bgn : bgn + distance, :] = 0\n",
    "            elif self.dim == 3:\n",
    "                e[:, :, bgn : bgn + distance] = 0\n",
    "\n",
    "\n",
    "class SpecAugmentation(nn.Module):\n",
    "    def __init__(self, time_drop_width, time_stripes_num, freq_drop_width, \n",
    "        freq_stripes_num):\n",
    "        \"\"\"Spec augmetation. \n",
    "        [ref] Park, D.S., Chan, W., Zhang, Y., Chiu, C.C., Zoph, B., Cubuk, E.D. \n",
    "        and Le, Q.V., 2019. Specaugment: A simple data augmentation method \n",
    "        for automatic speech recognition. arXiv preprint arXiv:1904.08779.\n",
    "\n",
    "        Args:\n",
    "          time_drop_width: int\n",
    "          time_stripes_num: int\n",
    "          freq_drop_width: int\n",
    "          freq_stripes_num: int\n",
    "        \"\"\"\n",
    "\n",
    "        super(SpecAugmentation, self).__init__()\n",
    "\n",
    "        self.time_dropper = DropStripes(dim=2, drop_width=time_drop_width, \n",
    "            stripes_num=time_stripes_num)\n",
    "\n",
    "        self.freq_dropper = DropStripes(dim=3, drop_width=freq_drop_width, \n",
    "            stripes_num=freq_stripes_num)\n",
    "\n",
    "    def forward(self, input):\n",
    "        x = self.time_dropper(input)\n",
    "        x = self.freq_dropper(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "23e7b30f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-24T20:56:11.607201Z",
     "iopub.status.busy": "2023-05-24T20:56:11.606797Z",
     "iopub.status.idle": "2023-05-24T20:56:11.829775Z",
     "shell.execute_reply": "2023-05-24T20:56:11.828453Z"
    },
    "papermill": {
     "duration": 0.234514,
     "end_time": "2023-05-24T20:56:11.833510",
     "exception": false,
     "start_time": "2023-05-24T20:56:11.598996",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 5)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>filename</th>\n",
       "      <th>name</th>\n",
       "      <th>date</th>\n",
       "      <th>id</th>\n",
       "      <th>path</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>H02_20230421_233500</td>\n",
       "      <td>H02</td>\n",
       "      <td>20230421</td>\n",
       "      <td>233500</td>\n",
       "      <td>/root/projects/BirdClef2025/data/test_soundsca...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              filename name      date      id  \\\n",
       "0  H02_20230421_233500  H02  20230421  233500   \n",
       "\n",
       "                                                path  \n",
       "0  /root/projects/BirdClef2025/data/test_soundsca...  "
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train = pd.read_csv(CFG.train_path)\n",
    "CFG.num_classes = len(df_train.primary_label.unique())\n",
    "df_test = pd.DataFrame(\n",
    "     [(path.stem, *path.stem.split(\"_\"), path) for path in Path(CFG.test_path).glob(\"*.ogg\")],\n",
    "    columns = [\"filename\", \"name\" ,\"date\",\"id\", \"path\"]\n",
    ")\n",
    "print(df_test.shape)\n",
    "df_test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "443fb76d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-24T20:56:11.849408Z",
     "iopub.status.busy": "2023-05-24T20:56:11.848675Z",
     "iopub.status.idle": "2023-05-24T20:56:11.861738Z",
     "shell.execute_reply": "2023-05-24T20:56:11.860926Z"
    },
    "papermill": {
     "duration": 0.023366,
     "end_time": "2023-05-24T20:56:11.863861",
     "exception": false,
     "start_time": "2023-05-24T20:56:11.840495",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# preprocess\n",
    "def compute_melspec(y, sr, n_mels, fmin, fmax):\n",
    "    \"\"\"\n",
    "    Computes a mel-spectrogram and puts it at decibel scale\n",
    "    Arguments:\n",
    "        y {np array} -- signal\n",
    "        params {AudioParams} -- Parameters to use for the spectrogram. Expected to have the attributes sr, n_mels, f_min, f_max\n",
    "    Returns:\n",
    "        np array -- Mel-spectrogram\n",
    "    \"\"\"\n",
    "    melspec = lb.feature.melspectrogram(\n",
    "        y=y, sr=sr, n_mels=n_mels, fmin=fmin, fmax=fmax,\n",
    "        win_length=CFG.window_size,hop_length=CFG.hop_size,center=True,\n",
    "        n_fft=CFG.window_size,pad_mode='reflect',window='hann'\n",
    "    )\n",
    "\n",
    "    melspec = lb.power_to_db(melspec,amin=1e-10,ref=1.0,top_db=None).astype(np.float32)\n",
    "    return melspec\n",
    "\n",
    "def mono_to_color(X, eps=1e-6, mean=None, std=None):\n",
    "    mean = mean or X.mean()\n",
    "    std = std or X.std()\n",
    "    X = (X - mean) / (std + eps)\n",
    "    \n",
    "    _min, _max = X.min(), X.max()\n",
    "\n",
    "    if (_max - _min) > eps:\n",
    "        V = np.clip(X, _min, _max)\n",
    "        V = 255 * (V - _min) / (_max - _min)\n",
    "        V = V.astype(np.uint8)\n",
    "    else:\n",
    "        V = np.zeros_like(X, dtype=np.uint8)\n",
    "\n",
    "    return V\n",
    "\n",
    "def crop_or_pad(y, length, is_train=True, start=None):\n",
    "    if len(y) < length:\n",
    "        y = np.concatenate([y, np.zeros(length - len(y))])\n",
    "        \n",
    "        n_repeats = length // len(y)\n",
    "        epsilon = length % len(y)\n",
    "        \n",
    "        y = np.concatenate([y]*n_repeats + [y[:epsilon]])\n",
    "        \n",
    "    elif len(y) > length:\n",
    "        if not is_train:\n",
    "            start = start or 0\n",
    "        else:\n",
    "            start = start or np.random.randint(len(y) - length)\n",
    "\n",
    "        y = y[start:start + length]\n",
    "\n",
    "    return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "da688196",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-24T20:56:11.879530Z",
     "iopub.status.busy": "2023-05-24T20:56:11.879142Z",
     "iopub.status.idle": "2023-05-24T20:56:11.888393Z",
     "shell.execute_reply": "2023-05-24T20:56:11.887637Z"
    },
    "papermill": {
     "duration": 0.019368,
     "end_time": "2023-05-24T20:56:11.890427",
     "exception": false,
     "start_time": "2023-05-24T20:56:11.871059",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# dataset\n",
    "import torch\n",
    "import numpy as np\n",
    "from torch.utils.data import Dataset,DataLoader\n",
    "    \n",
    "\n",
    "\n",
    "class TestDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, \n",
    "                 df: pd.DataFrame, \n",
    "                 clip: np.ndarray,\n",
    "                 config=None,\n",
    "                 model=None\n",
    "                ):\n",
    "        \n",
    "        self.df = df\n",
    "        self.clip = clip\n",
    "        self.config = config\n",
    "        self.model = model\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx: int):\n",
    "        sample = self.df.loc[idx, :]\n",
    "        row_id = sample.row_id\n",
    "        \n",
    "        end_seconds = int(sample.seconds)\n",
    "        start_seconds = int(end_seconds - 5)\n",
    "        \n",
    "        if start_seconds == 0:\n",
    "            end_seconds = end_seconds+2.5\n",
    "            y = self.clip[int(self.config.sample_rate * start_seconds) : int(self.config.sample_rate * end_seconds)].astype(np.float32)\n",
    "            y_25 = self.clip[0 : int(self.config.sample_rate * 2.5)].astype(np.float32)\n",
    "            y = np.concatenate((y_25,y))\n",
    "        elif start_seconds == 55:\n",
    "            start_seconds = start_seconds - 2.5\n",
    "            y = self.clip[int(self.config.sample_rate * start_seconds) : int(self.config.sample_rate * end_seconds)].astype(np.float32)\n",
    "            y_25 = self.clip[int(self.config.sample_rate * (start_seconds+5)) : int(self.config.sample_rate * end_seconds)].astype(np.float32)\n",
    "            y = np.concatenate((y,y_25))\n",
    "        else:\n",
    "            start_seconds = start_seconds - 2.5\n",
    "            end_seconds = end_seconds + 2.5\n",
    "            y = self.clip[int(self.config.sample_rate * start_seconds) : int(self.config.sample_rate * end_seconds)].astype(np.float32)\n",
    "        image = self.model.get_mel_gram(torch.from_numpy(y).unsqueeze(0))\n",
    "        \n",
    "        return {\n",
    "            \"image\": image,\n",
    "            \"row_id\": row_id,\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "fbec8c69",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-24T20:56:11.906201Z",
     "iopub.status.busy": "2023-05-24T20:56:11.905821Z",
     "iopub.status.idle": "2023-05-24T20:56:12.030345Z",
     "shell.execute_reply": "2023-05-24T20:56:12.029335Z"
    },
    "papermill": {
     "duration": 0.13566,
     "end_time": "2023-05-24T20:56:12.032956",
     "exception": false,
     "start_time": "2023-05-24T20:56:11.897296",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 5)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>filename</th>\n",
       "      <th>name</th>\n",
       "      <th>date</th>\n",
       "      <th>id</th>\n",
       "      <th>path</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>H02_20230421_233500</td>\n",
       "      <td>H02</td>\n",
       "      <td>20230421</td>\n",
       "      <td>233500</td>\n",
       "      <td>/root/projects/BirdClef2025/data/test_soundsca...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              filename name      date      id  \\\n",
       "0  H02_20230421_233500  H02  20230421  233500   \n",
       "\n",
       "                                                path  \n",
       "0  /root/projects/BirdClef2025/data/test_soundsca...  "
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train = pd.read_csv(CFG.train_path)\n",
    "CFG.num_classes = len(df_train.primary_label.unique())\n",
    "df_test = pd.DataFrame(\n",
    "     [(path.stem, *path.stem.split(\"_\"), path) for path in Path(CFG.test_path).glob(\"*.ogg\")],\n",
    "    columns = [\"filename\", \"name\" ,\"date\",\"id\", \"path\"]\n",
    ")\n",
    "print(df_test.shape)\n",
    "df_test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "3395abb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "low_numclasses_birds_index = [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 16, 17, 18, 20, 23, 24, 25, 26, 27, 28, 32, 33, 35, 37, 39, 40, 44, 46, 48, 49, 50, 51, 52, 53, 54, 56, 57, 58, 59, 127, 137, 156, 182]\n",
    "high_numclasses_birds_index = [12, 13, 14, 15, 19, 21, 22, 29, 30, 31, 34, 36, 38, 41, 42, 43, 45, 47, 55, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 128, 129, 130, 131, 132, 133, 134, 135, 136, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "dfaf6859",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Optional\n",
    "\n",
    "from torch import Tensor\n",
    "from torch.nn.modules.transformer import _get_activation_fn\n",
    "\n",
    "\n",
    "class TransformerDecoderLayerOptimal(nn.Module):\n",
    "    def __init__(self, d_model, nhead=8, dim_feedforward=2048, dropout=0.1, activation=\"relu\",\n",
    "                 layer_norm_eps=1e-5) -> None:\n",
    "        super(TransformerDecoderLayerOptimal, self).__init__()\n",
    "        self.norm1 = nn.LayerNorm(d_model, eps=layer_norm_eps)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.dropout1 = nn.Dropout(dropout)\n",
    "        self.dropout2 = nn.Dropout(dropout)\n",
    "        self.dropout3 = nn.Dropout(dropout)\n",
    "\n",
    "        self.self_attn = nn.MultiheadAttention(d_model, nhead, dropout=dropout)\n",
    "\n",
    "        # Implementation of Feedforward model\n",
    "        self.linear1 = nn.Linear(d_model, dim_feedforward)\n",
    "        self.linear2 = nn.Linear(dim_feedforward, d_model)\n",
    "\n",
    "        self.norm2 = nn.LayerNorm(d_model, eps=layer_norm_eps)\n",
    "        self.norm3 = nn.LayerNorm(d_model, eps=layer_norm_eps)\n",
    "\n",
    "        self.activation = _get_activation_fn(activation)\n",
    "\n",
    "    def __setstate__(self, state):\n",
    "        if 'activation' not in state:\n",
    "            state['activation'] = torch.nn.functional.relu\n",
    "        super(TransformerDecoderLayerOptimal, self).__setstate__(state)\n",
    "\n",
    "    def forward(self, tgt: Tensor, memory: Tensor, tgt_mask: Optional[Tensor] = None,\n",
    "                memory_mask: Optional[Tensor] = None, tgt_key_padding_mask: Optional[Tensor] = None,\n",
    "                memory_key_padding_mask: Optional[Tensor] = None, tgt_is_causal: Optional[bool] = None,\n",
    "                memory_is_causal: bool = False):\n",
    "        tgt = tgt + self.dropout1(tgt)\n",
    "        tgt = self.norm1(tgt)\n",
    "        tgt2 = self.self_attn(tgt, memory, memory)[0]\n",
    "        tgt = tgt + self.dropout2(tgt2)\n",
    "        tgt = self.norm2(tgt)\n",
    "        tgt2 = self.linear2(self.dropout(self.activation(self.linear1(tgt))))\n",
    "        tgt = tgt + self.dropout3(tgt2)\n",
    "        tgt = self.norm3(tgt)\n",
    "        return tgt\n",
    "\n",
    "\n",
    "# @torch.jit.script\n",
    "class GroupFC(object):\n",
    "    def __init__(self, embed_len_decoder: int):\n",
    "        self.embed_len_decoder = embed_len_decoder\n",
    "\n",
    "    def __call__(self, h: torch.Tensor, duplicate_pooling: torch.Tensor, out_extrap: torch.Tensor):\n",
    "        for i in range(h.shape[1]):\n",
    "            h_i = h[:, i, :]\n",
    "            if len(duplicate_pooling.shape) == 3:\n",
    "                w_i = duplicate_pooling[i, :, :]\n",
    "            else:\n",
    "                w_i = duplicate_pooling\n",
    "            out_extrap[:, i, :] = torch.matmul(h_i, w_i)\n",
    "\n",
    "\n",
    "class MLDecoder(nn.Module):\n",
    "    def __init__(self, num_classes, num_of_groups=-1, decoder_embedding=768,\n",
    "                 initial_num_features=2048, zsl=0):\n",
    "        super(MLDecoder, self).__init__()\n",
    "        embed_len_decoder = 206 if num_of_groups < 0 else num_of_groups\n",
    "        if embed_len_decoder > num_classes:\n",
    "            embed_len_decoder = num_classes\n",
    "\n",
    "        # switching to 768 initial embeddings\n",
    "        decoder_embedding = 768 if decoder_embedding < 0 else decoder_embedding\n",
    "        embed_standart = nn.Linear(initial_num_features, decoder_embedding)\n",
    "\n",
    "        # non-learnable queries\n",
    "        if not zsl:\n",
    "            query_embed = nn.Embedding(embed_len_decoder, decoder_embedding)\n",
    "            query_embed.requires_grad_(False)\n",
    "        else:\n",
    "            query_embed = None\n",
    "\n",
    "        # decoder\n",
    "        decoder_dropout = 0.1\n",
    "        num_layers_decoder = 1\n",
    "        dim_feedforward = 2048\n",
    "        layer_decode = TransformerDecoderLayerOptimal(d_model=decoder_embedding,\n",
    "                                                      dim_feedforward=dim_feedforward, dropout=decoder_dropout)\n",
    "        self.decoder = nn.TransformerDecoder(layer_decode, num_layers=num_layers_decoder)\n",
    "        self.decoder.embed_standart = embed_standart\n",
    "        self.decoder.query_embed = query_embed\n",
    "        self.zsl = zsl\n",
    "\n",
    "        if self.zsl:\n",
    "            if decoder_embedding != 300:\n",
    "                self.wordvec_proj = nn.Linear(300, decoder_embedding)\n",
    "            else:\n",
    "                self.wordvec_proj = nn.Identity()\n",
    "            self.decoder.duplicate_pooling = torch.nn.Parameter(torch.Tensor(decoder_embedding, 1))\n",
    "            self.decoder.duplicate_pooling_bias = torch.nn.Parameter(torch.Tensor(1))\n",
    "            self.decoder.duplicate_factor = 1\n",
    "        else:\n",
    "            # group fully-connected\n",
    "            self.decoder.num_classes = num_classes\n",
    "            self.decoder.duplicate_factor = int(num_classes / embed_len_decoder + 0.999)\n",
    "            self.decoder.duplicate_pooling = torch.nn.Parameter(\n",
    "                torch.Tensor(embed_len_decoder, decoder_embedding, self.decoder.duplicate_factor))\n",
    "            self.decoder.duplicate_pooling_bias = torch.nn.Parameter(torch.Tensor(num_classes))\n",
    "        torch.nn.init.xavier_normal_(self.decoder.duplicate_pooling)\n",
    "        torch.nn.init.constant_(self.decoder.duplicate_pooling_bias, 0)\n",
    "        self.decoder.group_fc = GroupFC(embed_len_decoder)\n",
    "        self.train_wordvecs = None\n",
    "        self.test_wordvecs = None\n",
    "\n",
    "    def forward(self, x, le=False):  # label embedding\n",
    "        if len(x.shape) == 4:  # [bs,2048,7,7]\n",
    "            embedding_spatial = x.flatten(2).transpose(1, 2)\n",
    "        else:  # [bs,2048,49]\n",
    "            embedding_spatial = x.transpose(1, 2)\n",
    "        embedding_spatial_786 = self.decoder.embed_standart(embedding_spatial)\n",
    "        embedding_spatial_786 = torch.nn.functional.relu(embedding_spatial_786, inplace=True)\n",
    "\n",
    "        bs = embedding_spatial_786.shape[0]\n",
    "        if self.zsl:\n",
    "            query_embed = torch.nn.functional.relu(self.wordvec_proj(self.decoder.query_embed))\n",
    "        else:\n",
    "            query_embed = self.decoder.query_embed.weight\n",
    "        # tgt = query_embed.unsqueeze(1).repeat(1, bs, 1)\n",
    "        tgt = query_embed.unsqueeze(1).expand(-1, bs, -1)  # no allocation of memory with expand\n",
    "        h = self.decoder(tgt, embedding_spatial_786.transpose(0, 1))  # [embed_len_decoder, batch, 768]\n",
    "        h = h.transpose(0, 1)\n",
    "\n",
    "        out_extrap = torch.zeros(h.shape[0], h.shape[1], self.decoder.duplicate_factor, device=h.device, dtype=h.dtype)\n",
    "        self.decoder.group_fc(h, self.decoder.duplicate_pooling, out_extrap)\n",
    "        if not self.zsl:\n",
    "            h_out = out_extrap.flatten(1)[:, :self.decoder.num_classes]\n",
    "        else:\n",
    "            h_out = out_extrap.flatten(1)\n",
    "        h_out += self.decoder.duplicate_pooling_bias\n",
    "        logits = h_out\n",
    "\n",
    "        if not le:\n",
    "            return logits\n",
    "        else:\n",
    "            return h, logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "d63854ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "from functools import reduce\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import timm\n",
    "\n",
    "\n",
    "def get_module_by_name(module: nn.Module, access_string: str) -> nn.Module:\n",
    "    names = access_string.split(sep='.')\n",
    "    return reduce(getattr, names, module)\n",
    "\n",
    "\n",
    "def convert_scaled_std_conv2d_to_conv2d(model: nn.Module) -> nn.Module:\n",
    "    converted_model = copy.deepcopy(model)\n",
    "    module_table = dict(converted_model.named_modules())\n",
    "    for name, m in converted_model.named_modules():\n",
    "        if isinstance(m, timm.layers.std_conv.ScaledStdConv2d):\n",
    "            scaled_weight = F.batch_norm(\n",
    "                m.weight.reshape(1, m.out_channels, -1), None, None,\n",
    "                weight=(m.gain * m.scale).view(-1),\n",
    "                training=True, momentum=0., eps=m.eps).reshape_as(m.weight).detach()\n",
    "\n",
    "            bias = m.bias is not None\n",
    "            conv = nn.Conv2d(m.in_channels, m.out_channels, m.kernel_size,\n",
    "                             stride=m.stride, padding=m.padding, dilation=m.dilation,\n",
    "                             groups=m.groups, bias=bias, padding_mode=m.padding_mode)\n",
    "            conv.weight.data = scaled_weight\n",
    "            if bias:\n",
    "                conv.bias.data = m.bias\n",
    "\n",
    "            # replace ScaledStdConv2d to nn.Conv2d\n",
    "            parent_name, child_name = name.rsplit('.', 1)\n",
    "            parent_module = module_table[parent_name]\n",
    "            setattr(parent_module, child_name, conv)\n",
    "\n",
    "    return converted_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "affd84d6",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-24T20:56:12.051778Z",
     "iopub.status.busy": "2023-05-24T20:56:12.051399Z",
     "iopub.status.idle": "2023-05-24T20:56:17.939889Z",
     "shell.execute_reply": "2023-05-24T20:56:17.939025Z"
    },
    "papermill": {
     "duration": 5.90208,
     "end_time": "2023-05-24T20:56:17.942465",
     "exception": false,
     "start_time": "2023-05-24T20:56:12.040385",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# model\n",
    "import timm\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "from torchaudio.transforms import MelSpectrogram,AmplitudeToDB\n",
    "\n",
    "class AttBlockV2(nn.Module):\n",
    "    def __init__(self,\n",
    "                 in_features: int,\n",
    "                 out_features: int,\n",
    "                 activation=\"linear\"):\n",
    "        super().__init__()\n",
    "\n",
    "        self.activation = activation\n",
    "        self.att = nn.Conv1d(\n",
    "            in_channels=in_features,\n",
    "            out_channels=out_features,\n",
    "            kernel_size=1,\n",
    "            stride=1,\n",
    "            padding=0,\n",
    "            bias=True)\n",
    "        self.cla = nn.Conv1d(\n",
    "            in_channels=in_features,\n",
    "            out_channels=out_features,\n",
    "            kernel_size=1,\n",
    "            stride=1,\n",
    "            padding=0,\n",
    "            bias=True)\n",
    "\n",
    "        self.init_weights()\n",
    "\n",
    "    def init_weights(self):\n",
    "        init_layer(self.att)\n",
    "        init_layer(self.cla)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x: (n_samples, n_in, n_time)\n",
    "        norm_att = torch.softmax(torch.tanh(self.att(x)), dim=-1)\n",
    "        cla = self.nonlinear_transform(self.cla(x))\n",
    "        x = torch.sum(norm_att * cla, dim=2)\n",
    "        return x, norm_att, cla\n",
    "\n",
    "    def nonlinear_transform(self, x):\n",
    "        if self.activation == 'linear':\n",
    "            return x\n",
    "        elif self.activation == 'sigmoid':\n",
    "            return torch.sigmoid(x)\n",
    "\n",
    "class NormalizeMelSpec(nn.Module):\n",
    "    def __init__(self, eps=1e-6):\n",
    "        super().__init__()\n",
    "        self.eps = eps\n",
    "\n",
    "    def forward(self, X):\n",
    "        mean = X.mean((1, 2), keepdim=True)\n",
    "        std = X.std((1, 2), keepdim=True)\n",
    "        Xstd = (X - mean) / (std + self.eps)\n",
    "        norm_min, norm_max = Xstd.min(-1)[0].min(-1)[0], Xstd.max(-1)[0].max(-1)[0]\n",
    "        fix_ind = (norm_max - norm_min) > self.eps * torch.ones_like(\n",
    "            (norm_max - norm_min)\n",
    "        )\n",
    "        V = torch.zeros_like(Xstd)\n",
    "        if fix_ind.sum():\n",
    "            V_fix = Xstd[fix_ind]\n",
    "            norm_max_fix = norm_max[fix_ind, None, None]\n",
    "            norm_min_fix = norm_min[fix_ind, None, None]\n",
    "            V_fix = torch.max(\n",
    "                torch.min(V_fix, norm_max_fix),\n",
    "                norm_min_fix,\n",
    "            )\n",
    "            # print(V_fix.shape, norm_min_fix.shape, norm_max_fix.shape)\n",
    "            V_fix = (V_fix - norm_min_fix) / (norm_max_fix - norm_min_fix)\n",
    "            V[fix_ind] = V_fix\n",
    "        return V\n",
    "\n",
    "def gem_freq(x, p=3, eps=1e-6):\n",
    "    return F.avg_pool2d(x.clamp(min=eps).pow(p), (x.size(-2), 1)).pow(1.0 / p)\n",
    "\n",
    "\n",
    "class GeMFreq(nn.Module):\n",
    "    def __init__(self, p=3, eps=1e-6):\n",
    "        super().__init__()\n",
    "        self.p = torch.nn.Parameter(torch.ones(1) * p)\n",
    "        self.eps = eps\n",
    "\n",
    "    def forward(self, x):\n",
    "        return gem_freq(x, p=self.p, eps=self.eps)\n",
    "\n",
    "def init_layer(layer):\n",
    "    \"\"\"Initialize a Linear or Convolutional layer. \"\"\"\n",
    "    nn.init.xavier_uniform_(layer.weight)\n",
    " \n",
    "    if hasattr(layer, 'bias'):\n",
    "        if layer.bias is not None:\n",
    "            layer.bias.data.fill_(0.)\n",
    "            \n",
    "    \n",
    "def init_bn(bn):\n",
    "    \"\"\"Initialize a Batchnorm layer. \"\"\"\n",
    "    bn.bias.data.fill_(0.)\n",
    "    bn.weight.data.fill_(1.)\n",
    "\n",
    "\n",
    "def gem(x, p=3, eps=1e-6):\n",
    "    return F.avg_pool2d(x.clamp(min=eps).pow(p), (x.size(-2), x.size(-1))).pow(1./p)\n",
    "\n",
    "class GeM(nn.Module):\n",
    "    def __init__(self, p=3, eps=1e-6, p_trainable=False):\n",
    "        super(GeM,self).__init__()\n",
    "        if p_trainable:\n",
    "            self.p = nn.Parameter(torch.ones(1)*p)\n",
    "        else:\n",
    "            self.p = p\n",
    "        self.eps = eps\n",
    "\n",
    "    def forward(self, x):\n",
    "        ret = gem(x, p=self.p, eps=self.eps)   \n",
    "        ret = torch.flatten(ret,start_dim=1)\n",
    "        return ret\n",
    "    def __repr__(self):\n",
    "        return self.__class__.__name__ + '(' + 'p=' + '{:.4f}'.format(self.p.data.tolist()[0]) + ', ' + 'eps=' + str(self.eps) + ')'\n",
    "\n",
    "\n",
    "class BirdClefSEDModel(nn.Module):\n",
    "    def __init__(self, model_name=CFG.model, num_classes = CFG.num_classes, pretrained = CFG.pretrained,p=0.5):\n",
    "        super().__init__()\n",
    "        self.num_classes = num_classes\n",
    "        self.backbone = timm.create_model(model_name, pretrained=pretrained)\n",
    "        if 'effi' in CFG.model:\n",
    "            self.backbone.global_pool = nn.Identity()\n",
    "            self.backbone.classifier = nn.Identity()\n",
    "        elif 'eca' in CFG.model:\n",
    "            self.backbone.head.fc = nn.Identity()\n",
    "        if CFG.use_fsr:\n",
    "            self.backbone.conv_stem.stride = (1,1)\n",
    "        self.fc_audioset = nn.Linear(self.backbone.num_features, num_classes, bias=True)\n",
    "        self.pooling = GeM()\n",
    "        self.SpecAug = SpecAugmentation(time_drop_width=64, time_stripes_num=2,freq_drop_width=8, freq_stripes_num=2)\n",
    "        self.use_spec_aug = CFG.use_spec_aug\n",
    "        self.bn0 = nn.BatchNorm2d(CFG.mel_bins)\n",
    "    \n",
    "        # Spectrogram extractor\n",
    "        self.spectrogram_extractor = MelSpectrogram(\n",
    "            sample_rate=CFG.sample_rate,\n",
    "            n_fft=2048,\n",
    "            win_length=CFG.window_size,\n",
    "            hop_length=CFG.hop_size,\n",
    "            f_min=CFG.fmin,\n",
    "            f_max=CFG.fmax,\n",
    "            pad=0,\n",
    "            n_mels=CFG.mel_bins,\n",
    "            power=2,\n",
    "            normalized=False,\n",
    "        )\n",
    "        # Logmel feature extractor\n",
    "        self.logmel_extractor = AmplitudeToDB(top_db=None)\n",
    "        self.normlize = NormalizeMelSpec()\n",
    "        self.infer_period = CFG.infer_duration\n",
    "        self.train_period = CFG.duration\n",
    "        self.init_weight()\n",
    "\n",
    "    def init_weight(self):\n",
    "        init_bn(self.bn0)\n",
    "\n",
    "        \n",
    "    def get_mel_gram(self,audios):\n",
    "        \"\"\"\n",
    "        Input: (batch_size, data_length)\"\"\"\n",
    "        x = self.spectrogram_extractor(audios) # (batch_size,freq_bins time_steps)\n",
    "        x = self.logmel_extractor(x) \n",
    "        x = self.normlize(x)\n",
    "        x = x.permute(0,2,1)# (batch_size,time_steps, mel_bins)\n",
    "        return x\n",
    "    \n",
    "    def forward(self,images):\n",
    "        # b c f t\n",
    "        if CFG.use_spec_aug and self.training:\n",
    "            if np.random.uniform(0,1)>CFG.p_spec_aug:\n",
    "                images = self.SpecAug(images)\n",
    "        x = self.backbone.forward_features(images) #  bs,1,t,f\n",
    "        x = torch.mean(x,dim=3) # pooling freq bs,c,t\n",
    "        \n",
    "        (x1,_) = torch.max(x,dim=2) # bs,c\n",
    "        x2 = torch.mean(x,dim=2) # bs,c\n",
    "        x = x1+x2\n",
    "        x = F.dropout(x,p=0.5,training=self.training)\n",
    "        x = self.fc_audioset(x)\n",
    "\n",
    "        return x\n",
    "    \n",
    "class BirdClefModel(nn.Module):\n",
    "    def __init__(self, model_name=CFG.model, num_classes = CFG.num_classes, pretrained = CFG.pretrained,p=0.5):\n",
    "        super().__init__()\n",
    "        self.num_classes = num_classes\n",
    "        self.backbone = timm.create_model(model_name, pretrained=pretrained)\n",
    "        if 'effi' in CFG.model:\n",
    "            self.backbone.global_pool = nn.Identity()\n",
    "            self.backbone.classifier = nn.Identity()\n",
    "        elif 'eca' in CFG.model:\n",
    "            self.backbone.head.fc = nn.Identity()\n",
    "        if CFG.use_fsr:\n",
    "            self.backbone.conv_stem.stride = (1,1)\n",
    "        self.fc_audioset = nn.Linear(self.backbone.num_features, num_classes, bias=True)\n",
    "        self.pooling = GeM()\n",
    "        self.SpecAug = SpecAugmentation(time_drop_width=64, time_stripes_num=2,freq_drop_width=8, freq_stripes_num=2)\n",
    "        self.use_spec_aug = CFG.use_spec_aug\n",
    "        self.bn0 = nn.BatchNorm2d(CFG.mel_bins)\n",
    "        \n",
    "        # Spectrogram extractor\n",
    "        self.spectrogram_extractor = MelSpectrogram(\n",
    "            sample_rate=CFG.sample_rate,\n",
    "            n_fft=2048,\n",
    "            win_length=CFG.window_size,\n",
    "            hop_length=CFG.hop_size,\n",
    "            f_min=CFG.fmin,\n",
    "            f_max=CFG.fmax,\n",
    "            pad=0,\n",
    "            n_mels=CFG.mel_bins,\n",
    "            power=2,\n",
    "            normalized=False,\n",
    "        )\n",
    "        # Logmel feature extractor\n",
    "        self.logmel_extractor = AmplitudeToDB(top_db=None)\n",
    "        self.normlize = NormalizeMelSpec()\n",
    "        self.infer_period = CFG.infer_duration\n",
    "        self.train_period = CFG.duration\n",
    "        self.init_weight()\n",
    "\n",
    "    def init_weight(self):\n",
    "        init_bn(self.bn0)\n",
    "\n",
    "        \n",
    "    def get_mel_gram(self,audios):\n",
    "        \"\"\"\n",
    "        Input: (batch_size, data_length)\"\"\"\n",
    "        x = self.spectrogram_extractor(audios) # (batch_size,freq_bins time_steps)\n",
    "        x = self.logmel_extractor(x) \n",
    "        x = self.normlize(x)\n",
    "        x = x.permute(0,2,1)# (batch_size,time_steps, mel_bins)\n",
    "        return x\n",
    "    \n",
    "    def forward(self,images):\n",
    "        # b c f t\n",
    "        if CFG.use_spec_aug and self.training:\n",
    "            if np.random.uniform(0,1)>CFG.p_spec_aug:\n",
    "                images = self.SpecAug(images)\n",
    "        if self.training:\n",
    "            if images.shape[2]%4!=0:\n",
    "                images = F.pad(images,(0,0,4-images.shape[2]%4,0))\n",
    "            images = torch.cat(torch.chunk(images,chunks=4,dim=2),dim=0) #  4*bs,1,t,f\n",
    "        x = self.backbone.forward_features(images) #  4*bs,1,t,f\n",
    "\n",
    "        if self.training:\n",
    "            x = torch.cat(torch.chunk(x,chunks=4,dim=0),dim=2)\n",
    "        x = self.pooling(x)\n",
    "        x = self.fc_audioset(x)\n",
    "        return x\n",
    "    \n",
    "class BirdClefSEDAttModel(nn.Module):\n",
    "    def __init__(self, model_name=CFG.model, num_classes = CFG.num_classes, pretrained = CFG.pretrained,p=0.5,window_size=CFG.window_size,hop_size=CFG.hop_size):\n",
    "        super().__init__()\n",
    "        self.num_classes = num_classes\n",
    "        self.backbone = timm.create_model(model_name, pretrained=pretrained)\n",
    "        if 'effi' in model_name:\n",
    "            self.backbone.global_pool = nn.Identity()\n",
    "            self.backbone.classifier = nn.Identity()\n",
    "        elif 'eca' in model_name:\n",
    "            self.backbone.head.fc = nn.Identity()\n",
    "        if CFG.use_fsr:\n",
    "            self.backbone.conv_stem.stride = (1,1)\n",
    "        self.pooling = GeM()\n",
    "        self.SpecAug = SpecAugmentation(time_drop_width=64, time_stripes_num=2,freq_drop_width=8, freq_stripes_num=2)\n",
    "        self.use_spec_aug = CFG.use_spec_aug\n",
    "        self.bn0 = nn.BatchNorm2d(CFG.img_size[1])\n",
    "    \n",
    "        # Spectrogram extractor\n",
    "        self.spectrogram_extractor = MelSpectrogram(\n",
    "            sample_rate=CFG.sample_rate,\n",
    "            n_fft=CFG.n_fft,\n",
    "            win_length=window_size,\n",
    "            hop_length=hop_size,\n",
    "            f_min=CFG.fmin,\n",
    "            f_max=CFG.fmax,\n",
    "            pad=0,\n",
    "            n_mels=CFG.mel_bins,\n",
    "            power=2,\n",
    "            normalized=False,\n",
    "        )\n",
    "        # Logmel feature extractor\n",
    "        self.logmel_extractor = AmplitudeToDB(top_db=None)\n",
    "        self.normlize = NormalizeMelSpec()\n",
    "        self.fc1 = nn.Linear(self.backbone.num_features, self.backbone.num_features, bias=True)\n",
    "        self.att_block = AttBlockV2(self.backbone.num_features, num_classes, activation=\"sigmoid\")\n",
    "        self.init_weight()\n",
    "\n",
    "    def init_weight(self):\n",
    "        init_bn(self.bn0)\n",
    "        init_layer(self.fc1)\n",
    "    \n",
    "    def get_mel_gram(self,audios):\n",
    "        \"\"\"\n",
    "        Input: (batch_size, data_length)\"\"\"\n",
    "        x = self.spectrogram_extractor(audios) # (batch_size,freq_bins time_steps)\n",
    "        x = self.logmel_extractor(x) \n",
    "        x = self.normlize(x)\n",
    "        x = x.permute(0,2,1)# (batch_size,time_steps, mel_bins)\n",
    "        return x\n",
    "\n",
    "    def forward(self, images):\n",
    "        # b c f t\n",
    "        if CFG.use_spec_aug and self.training:\n",
    "            if np.random.uniform(0,1)>CFG.p_spec_aug:\n",
    "                images = self.SpecAug(images)\n",
    "\n",
    "        x = images.transpose(1, 3)\n",
    "        x = self.bn0(x)\n",
    "        x = x.transpose(1, 3)\n",
    "\n",
    "        x = x.transpose(2, 3)\n",
    "\n",
    "        x = self.backbone.forward_features(x)\n",
    "        \n",
    "        # Aggregate in frequency axis\n",
    "        x = torch.mean(x, dim=2)\n",
    "\n",
    "        x1 = F.max_pool1d(x, kernel_size=3, stride=1, padding=1)\n",
    "        x2 = F.avg_pool1d(x, kernel_size=3, stride=1, padding=1)\n",
    "        x = x1 + x2\n",
    "\n",
    "        x = F.dropout(x, p=0.3, training=self.training)\n",
    "        x = x.transpose(1, 2)\n",
    "        x = F.relu(self.fc1(x),inplace=False)\n",
    "        x = x.transpose(1, 2)\n",
    "        x = F.dropout(x, p=0.3, training=self.training)\n",
    "\n",
    "        (clipwise_output, norm_att, segmentwise_output) = self.att_block(x)\n",
    "\n",
    "        maxframewise_output = nn.AdaptiveMaxPool1d(1)(segmentwise_output).squeeze(2)\n",
    "        output_dict = {\n",
    "            \"clipwise_output\": clipwise_output,\n",
    "            \"framewise_output\":segmentwise_output,\n",
    "            \"maxframewise_output\":maxframewise_output\n",
    "        }\n",
    "\n",
    "        return output_dict\n",
    "\n",
    "\n",
    "# model_efb3 = BirdClefSEDAttModel(model_name=\"tf_efficientnet_b3_ns\",num_classes=CFG.num_classes,pretrained=CFG.pretrained)\n",
    "# model_efv2b2 = BirdClefSEDAttModel(model_name=\"tf_efficientnetv2_b2\",num_classes=CFG.num_classes,pretrained=CFG.pretrained)\n",
    "# model_seresnext = BirdClefSEDAttModel(model_name=\"seresnext26d_32x4d\",num_classes=CFG.num_classes,pretrained=CFG.pretrained)\n",
    "# model_seresnext.load_state_dict(torch.load('/kaggle/input/birdclefv3/seresnext_seclabel.pt',map_location=torch.device('cpu')))\n",
    "# model_cnn = BirdClefSEDAttModel(model_name=\"eca_nfnet_l0\",num_classes=CFG.num_classes,pretrained=CFG.pretrained)\n",
    "# model_cnn.load_state_dict(torch.load('/root/projects/BirdClef2025/BirdCLEF2023-30th-place-solution-master/logs/2025-04-06T02:04-cnn/saved_model.pt',map_location=torch.device('cpu')))\n",
    "# model_cnn = BirdClefCNNModel(model_name=\"tf_efficientnetv2_b3\",num_classes=CFG.num_classes,pretrained=CFG.pretrained)\n",
    "# model_cnn.load_state_dict(torch.load('/root/projects/BirdClef2025/BirdCLEF2023-30th-place-solution-master/logs/2025-04-25T15:17-efv2b3-cnnmldecoder-kd-0.859/saved_model_lastepoch.pt',map_location=torch.device('cpu')))\n",
    "# model_efv2b2 = BirdClefSEDAttModel(model_name=\"tf_efficientnetv2_b2\",num_classes=CFG.num_classes,pretrained=CFG.pretrained)\n",
    "# model_efv2b2.load_state_dict(torch.load('/kaggle/input/birdclefv3/efv2_finetune_alldata.pt',map_location=torch.device('cpu')))\n",
    "\n",
    "# model_efv2b3 = BirdClefSEDAttModel(model_name=\"tf_efficientnetv2_b3\",num_classes=CFG.num_classes,pretrained=CFG.pretrained,window_size=640,hop_size=320)\n",
    "# model_efv2b3.load_state_dict(torch.load('/root/projects/BirdClef2025/BirdCLEF2023-30th-place-solution-master/logs/2025-05-25T14:01/saved_model_lastepoch.pt',map_location=torch.device('cpu')))\n",
    "\n",
    "# model_seresnext = BirdClefSEDAttModel(model_name=\"seresnext26d_32x4d\",num_classes=CFG.num_classes,pretrained=CFG.pretrained)\n",
    "# model_seresnext.load_state_dict(torch.load('/root/projects/BirdClef2025/BirdCLEF2023-30th-place-solution-master/logs/2025-04-25T14:20-seresnext-kd-0.870/saved_model_lastepoch.pt',map_location=torch.device('cpu')))\n",
    "\n",
    "model_rexnet150 = BirdClefCNNFCModel(model_name=\"tf_efficientnetv2_b2\",num_classes=CFG.num_classes,pretrained=CFG.pretrained)\n",
    "model_rexnet150.load_state_dict(torch.load('/root/projects/BirdClef2025/BirdCLEF2023-30th-place-solution-master/logs/2025-06-04T07:36-rexnet150-cnn/saved_model_lastepoch.pt',map_location=torch.device('cpu')))\n",
    "# model_cnn_nfnet0 = convert_scaled_std_conv2d_to_conv2d(model_cnn_nfnet0)\n",
    "# model_resnest = BirdClefSEDAttModel(model_name=\"resnest26d\",num_classes=CFG.num_classes,pretrained=CFG.pretrained)\n",
    "# model_resnest.load_state_dict(torch.load('/kaggle/input/birdclefv3/resnest26d_alldata_mel128.pt',map_location=torch.device('cpu')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "87c2dca6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ INFO ] MO command line tool is considered as the legacy conversion API as of OpenVINO 2023.2 release.\n",
      "In 2025.0 MO command line tool and openvino.tools.mo.convert_model() will be removed. Please use OpenVINO Model Converter (OVC) or openvino.convert_model(). OVC represents a lightweight alternative of MO and provides simplified model conversion API. \n",
      "Find more information about transition from MO to OVC at https://docs.openvino.ai/2023.2/openvino_docs_OV_Converter_UG_prepare_model_convert_model_MO_OVC_transition.html\n",
      "[ INFO ] Generated IR will be compressed to FP16. If you get lower accuracy, please consider disabling compression explicitly by adding argument --compress_to_fp16=False.\n",
      "Find more information about compression to FP16 at https://docs.openvino.ai/2023.0/openvino_docs_MO_DG_FP16_Compression.html\n",
      "Check for a new version of Intel(R) Distribution of OpenVINO(TM) toolkit here https://software.intel.com/content/www/us/en/develop/tools/openvino-toolkit/download.html?cid=other&source=prod&campid=ww_2023_bu_IOTG_OpenVINO-2023-1&content=upg_all&medium=organic or on https://github.com/openvinotoolkit/openvino\n",
      "[ SUCCESS ] Generated IR version 11 model.\n",
      "[ SUCCESS ] XML file: /root/projects/BirdClef2025/BirdCLEF2023-30th-place-solution-master/usefulFunc/model_rexnet150.xml\n",
      "[ SUCCESS ] BIN file: /root/projects/BirdClef2025/BirdCLEF2023-30th-place-solution-master/usefulFunc/model_rexnet150.bin\n"
     ]
    }
   ],
   "source": [
    "dummy_input = torch.randn(24, 3, 313, 192)\n",
    "\n",
    "output_names = [\"output\"]\n",
    "model_rexnet150.eval()\n",
    "torch.onnx.export(model_rexnet150,dummy_input,\"model_rexnet150.onnx\",input_names=[\"input\"],output_names=output_names)\n",
    "!mo --input_model=model_rexnet150.onnx\n",
    "# output_names = [ \"clipwise_output\",\"framewise_output\",\"maxframewise_output\"]\n",
    "# model_efv2b3.eval()\n",
    "# torch.onnx.export(model_efv2b3,dummy_input,\"model_efv2b3.onnx\",input_names=[\"input\"],output_names=output_names)\n",
    "# !mo --input_model=model_efv2b3.onnx\n",
    "\n",
    "# output_names = [ \"clipwise_output\",\"framewise_output\",\"maxframewise_output\"]\n",
    "# model_seresnext.eval()\n",
    "# torch.onnx.export(model_seresnext,dummy_input,\"model_seresnext.onnx\",input_names=[\"input\"],output_names=output_names)\n",
    "# !mo --input_model=model_seresnext.onnx\n",
    "\n",
    "# output_names = [ \"output\"]\n",
    "# model_cnn.eval()\n",
    "# torch.onnx.export(model_cnn,dummy_input,\"model_cnn.onnx\",input_names=[\"input\"],output_names=output_names)\n",
    "# !mo --input_model=model_cnn.onnx\n",
    "\n",
    "# output_names = [ \"output\"]\n",
    "# model_cnn_nfnet0.eval()\n",
    "# torch.onnx.export(model_cnn_nfnet0,dummy_input,\"model_cnn_nfnet0.onnx\",input_names=[\"input\"],output_names=output_names)\n",
    "# !mo --input_model=model_cnn_nfnet0.onnx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7e125d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from openvino.runtime import Core\n",
    "import numpy as np\n",
    "import cv2  # \n",
    "\n",
    "# \n",
    "core = Core()\n",
    "model = core.read_model(model=\"/root/projects/BirdClef2025/BirdCLEF2023-30th-place-solution-master/usefulFunc/model_efv2b3.xml\")\n",
    "compiled_model_efv2b3 = core.compile_model(model=model, device_name=\"CPU\")\n",
    "infer_request_efv2b3 = compiled_model_efv2b3.create_infer_request()\n",
    "res = infer_request_efv2b3.infer(inputs=[dummy_input])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62541169",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-24T20:56:17.959173Z",
     "iopub.status.busy": "2023-05-24T20:56:17.958782Z",
     "iopub.status.idle": "2023-05-24T20:56:17.993370Z",
     "shell.execute_reply": "2023-05-24T20:56:17.992418Z"
    },
    "papermill": {
     "duration": 0.046138,
     "end_time": "2023-05-24T20:56:17.996019",
     "exception": false,
     "start_time": "2023-05-24T20:56:17.949881",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception in thread QueueFeederThread:\n",
      "Traceback (most recent call last):\n",
      "  File \"/root/anaconda3/envs/cibmtr/lib/python3.10/multiprocessing/queues.py\", line 239, in _feed\n",
      "Exception in thread QueueFeederThread:\n",
      "Traceback (most recent call last):\n",
      "  File \"/root/anaconda3/envs/cibmtr/lib/python3.10/multiprocessing/queues.py\", line 239, in _feed\n",
      "Exception ignored in: <function _ConnectionBase.__del__ at 0x7fbf391acb80>\n",
      "Traceback (most recent call last):\n",
      "  File \"/root/anaconda3/envs/cibmtr/lib/python3.10/multiprocessing/connection.py\", line 132, in __del__\n",
      "    reader_close()\n",
      "  File \"/root/anaconda3/envs/cibmtr/lib/python3.10/multiprocessing/connection.py\", line 177, in close\n",
      "    reader_close()\n",
      "  File \"/root/anaconda3/envs/cibmtr/lib/python3.10/multiprocessing/connection.py\", line 177, in close\n",
      "    self._close()\n",
      "  File \"/root/anaconda3/envs/cibmtr/lib/python3.10/multiprocessing/connection.py\", line 361, in _close\n",
      "    self._close()\n",
      "  File \"/root/anaconda3/envs/cibmtr/lib/python3.10/multiprocessing/connection.py\", line 361, in _close\n",
      "    self._close()\n",
      "  File \"/root/anaconda3/envs/cibmtr/lib/python3.10/multiprocessing/connection.py\", line 361, in _close\n",
      "    _close(self._handle)\n",
      "OSError: [Errno 9] Bad file descriptor\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/root/anaconda3/envs/cibmtr/lib/python3.10/threading.py\", line 1016, in _bootstrap_inner\n",
      "    _close(self._handle)\n",
      "OSError: [Errno 9] Bad file descriptor\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/root/anaconda3/envs/cibmtr/lib/python3.10/threading.py\", line 1016, in _bootstrap_inner\n",
      "    self.run()\n",
      "  File \"/root/anaconda3/envs/cibmtr/lib/python3.10/site-packages/ipykernel/ipkernel.py\", line 766, in run_closure\n",
      "    _close(self._handle)\n",
      "OSError: [Errno 9] Bad file descriptor\n",
      "    _threading_Thread_run(self)\n",
      "  File \"/root/anaconda3/envs/cibmtr/lib/python3.10/threading.py\", line 953, in run\n",
      "    self.run()\n",
      "  File \"/root/anaconda3/envs/cibmtr/lib/python3.10/site-packages/ipykernel/ipkernel.py\", line 766, in run_closure\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/root/anaconda3/envs/cibmtr/lib/python3.10/multiprocessing/queues.py\", line 271, in _feed\n",
      "    _threading_Thread_run(self)\n",
      "  File \"/root/anaconda3/envs/cibmtr/lib/python3.10/threading.py\", line 953, in run\n",
      "    queue_sem.release()\n",
      "ValueError: semaphore or lock released too many times\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/root/anaconda3/envs/cibmtr/lib/python3.10/multiprocessing/queues.py\", line 271, in _feed\n",
      "    queue_sem.release()\n",
      "ValueError: semaphore or lock released too many times\n"
     ]
    }
   ],
   "source": [
    "seconds = [i for i in range(5, 65, 5)]\n",
    "# model_seresnext.to('cpu')\n",
    "# model_efv2b3.to('cpu')\n",
    "# model_efv2b3_split1.to('cpu')\n",
    "# model_efv2b2.to('cpu')\n",
    "# model_efv2b3.to('cpu')\n",
    "# model_nfnetl0.to('cpu')\n",
    "# model_resnest.to('cpu')\n",
    "# model_seresnext.eval()\n",
    "# model_efv2b3.eval()\n",
    "# model_efv2b3_split1.eval()\n",
    "# model_efv2b2.eval()\n",
    "# model_nfnetl0.eval()\n",
    "# model_resnest.eval()\n",
    "models = [compiled_model_efv2b3]\n",
    "# models_split1 = [model_efv2b3_split1]\n",
    "# models = [model_seresnext]\n",
    "\n",
    "def smooth_array_general(array, w=[0.1, 0.2, 0.4, 0.2, 0.1]):\n",
    "    smoothed_array = np.zeros_like(array)\n",
    "    timesteps = array.shape[0]\n",
    "    radius = len(w) // 2 # 2\n",
    "\n",
    "    for t in range(timesteps):\n",
    "        for i, weight in enumerate(w):\n",
    "            index = t - radius + i\n",
    "            if index < 0: \n",
    "                smoothed_array[t] += array[0] * weight\n",
    "            elif index >= timesteps: \n",
    "                smoothed_array[t] += array[-1] * weight\n",
    "            else:\n",
    "                smoothed_array[t] += array[index] * weight\n",
    "    for c in range(array.shape[1]):\n",
    "        smoothed_array[:, c] = smoothed_array[:, c] * 0.8 + smoothed_array[:, c].mean(keepdims=True) * 0.2\n",
    "    return smoothed_array\n",
    "\n",
    "def prediction_for_clip(\n",
    "    audio_path\n",
    "):\n",
    "    predictions = []\n",
    "    device = torch.device(\"cpu\")\n",
    "    global models\n",
    "    # inference\n",
    "    prediction_dict = {}\n",
    "\n",
    "    clip, _ = librosa.load(audio_path, sr=32000)\n",
    "    name_ = \"_\".join(audio_path.name.split(\".\")[:-1])\n",
    "    row_ids = [name_+f\"_{second}\" for second in seconds]\n",
    "\n",
    "    test_df = pd.DataFrame({\n",
    "        \"row_id\": row_ids,\n",
    "        \"seconds\": seconds\n",
    "    })\n",
    "    \n",
    "    dataset = TestDataset(\n",
    "        df=test_df, \n",
    "        clip=clip,\n",
    "        config=CFG,\n",
    "        model=model_efv2b3\n",
    "    )\n",
    "\n",
    "    loader = torch.utils.data.DataLoader(\n",
    "        dataset,\n",
    "        batch_size=CFG.batch_size, \n",
    "        num_workers=4,\n",
    "        drop_last=False,\n",
    "        shuffle=False,\n",
    "        pin_memory=True\n",
    "    )\n",
    "\n",
    "    infer_request_models = []\n",
    "    for infer in models:\n",
    "        infer_request_models.append(infer.create_infer_request())\n",
    "\n",
    "    for data in loader:\n",
    "\n",
    "        row_ids = data['row_id']\n",
    "\n",
    "        for row_id in row_ids:\n",
    "            if row_id not in prediction_dict:\n",
    "                prediction_dict[str(row_id)] = []\n",
    "\n",
    "        image = data['image']#.to(device)\n",
    "        image = torch.repeat_interleave(image,repeats=3,dim=1)\n",
    "        probas = []\n",
    "        # print(image.shape)\n",
    "        with torch.no_grad():\n",
    "            output = np.zeros((image.shape[0],206))\n",
    "            for model in infer_request_models:\n",
    "                output_temp = model.infer(inputs=[image])\n",
    "                output += (output_temp['clipwise_output'] + output_temp['maxframewise_output'])/2\n",
    "                print(output_temp['framewise_output'].shape)\n",
    "                output_framewise =  output_temp['framewise_output']\n",
    "            output = output/len(models)\n",
    "\n",
    "        output = smooth_array_general(output)\n",
    "        \n",
    "        predictions.append(output)\n",
    "\n",
    "    return predictions,output_framewise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "19637bea",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-24T20:56:18.012859Z",
     "iopub.status.busy": "2023-05-24T20:56:18.012458Z",
     "iopub.status.idle": "2023-05-24T20:57:18.377424Z",
     "shell.execute_reply": "2023-05-24T20:57:18.375886Z"
    },
    "papermill": {
     "duration": 60.376716,
     "end_time": "2023-05-24T20:57:18.380316",
     "exception": false,
     "start_time": "2023-05-24T20:56:18.003600",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(320000,)\n",
      "(320000,)\n",
      "(320000,)\n",
      "(320000,)\n",
      "(320000,)\n",
      "(320000,)\n",
      "(320000,)\n",
      "(320000,)\n",
      "(320000,)\n",
      "(320000,)\n",
      "(320000,)\n",
      "(320000,)\n",
      "(12, 206, 20)\n"
     ]
    }
   ],
   "source": [
    "from functools import partial\n",
    "import time\n",
    "all_audios = list(Path(CFG.test_path).glob(\"*.ogg\"))\n",
    "\n",
    "# prediction_for_clip_with_models = partial(prediction_for_clip, models=models,num_classes=206)\n",
    "# start_time = time.time()\n",
    "# with concurrent.futures.ThreadPoolExecutor(max_workers=4) as executor:\n",
    "#     predictions = list(executor.map(prediction_for_clip,all_audios))\n",
    "# print(time.time() - start_time)\n",
    "# prediction_for_clip_with_models = partial(prediction_for_clip, models=models_split1,num_classes=len(split_bird))\n",
    "# with concurrent.futures.ThreadPoolExecutor(max_workers=4) as executor:\n",
    "#     predictions_split1 = list(executor.map(prediction_for_clip_with_models,all_audios))\n",
    "\n",
    "predictions = prediction_for_clip(all_audios[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "bc1f6c01",
   "metadata": {},
   "outputs": [],
   "source": [
    "import soundfile as sf\n",
    "# audio,_ = sf.read('/root/projects/BirdClef2025/data/train_soundscapes_10s/H09_20230513_115000_45s.ogg')\n",
    "# audio = torch.from_numpy(audio).unsqueeze(0).to(torch.float32)\n",
    "audio = torch.randn((1,int(32000*7.5)))\n",
    "images = model_efv2b3.get_mel_gram(audio)\n",
    "images = torch.repeat_interleave(images.unsqueeze(1),repeats=3,dim=1)\n",
    "# pred = model_efv2b3.forward(images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "a924cd2a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 3, 501, 192])"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "images[:,:,250:,:].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "id": "2fcb8b19",
   "metadata": {},
   "outputs": [],
   "source": [
    "bird_cols = pd.read_csv('/root/projects/BirdClef2025/BirdCLEF2023-30th-place-solution-master/usefulFunc/pesudo_labelv16_0.4thred.csv').columns[1:].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f34cb3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pred_pesudo = pd.read_csv('/root/projects/BirdClef2025/BirdCLEF2023-30th-place-solution-master/usefulFunc/pesudo_labelv16_0.2thred.csv')\n",
    "# pred_pesudo['pred_sum'] = pred_pesudo[bird_cols].sum(axis=1)\n",
    "# pred_pesudo = pred_pesudo.sort_values(by='pred_sum',ascending=False)\n",
    "# pred_pesudo.to_csv('/root/projects/BirdClef2025/BirdCLEF2023-30th-place-solution-master/usefulFunc/pesudo_labelv16_0.2thred_predsumsort.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 281,
   "id": "0fe7c7d6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'amekes'"
      ]
     },
     "execution_count": 281,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bird_cols[61]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 278,
   "id": "02c0284a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 206, 42])"
      ]
     },
     "execution_count": 278,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred['framewise_output'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 279,
   "id": "5ddc8bdd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0]),\n",
       " array([43, 43, 43, 43, 43, 43, 43, 43, 43, 43, 43, 43, 43, 43, 43, 43, 43,\n",
       "        43, 43, 43, 43, 43, 43, 43, 43, 43, 43, 43, 43, 61, 61, 61]),\n",
       " array([ 2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17, 18,\n",
       "        19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 37, 38, 39]))"
      ]
     },
     "execution_count": 279,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.where(pred['framewise_output']>0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 274,
   "id": "00c009b7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([], dtype=int64), array([], dtype=int64))"
      ]
     },
     "execution_count": 274,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.where(pred['clipwise_output']>0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "id": "e5c3aa77",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7f842d1b6f20>]"
      ]
     },
     "execution_count": 269,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(np.arange(pred['framewise_output'].shape[-1])*0.5,pred['framewise_output'][0,80,:].detach().cpu().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "48d99627",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 4740, 192])"
      ]
     },
     "execution_count": 142,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "images.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "ff0d45c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8.067443132400513\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "with concurrent.futures.ThreadPoolExecutor(max_workers=4) as executor:\n",
    "    predictions = list(executor.map(prediction_for_clip,all_audios))\n",
    "print(time.time() - start_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "c56183c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_temp = []\n",
    "for pred in predictions:\n",
    "    pred_temp = pred_temp + pred\n",
    "predictions = pred_temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "ac80db4d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-24T20:57:18.424645Z",
     "iopub.status.busy": "2023-05-24T20:57:18.424272Z",
     "iopub.status.idle": "2023-05-24T20:57:18.647564Z",
     "shell.execute_reply": "2023-05-24T20:57:18.646598Z"
    },
    "papermill": {
     "duration": 0.235021,
     "end_time": "2023-05-24T20:57:18.650298",
     "exception": false,
     "start_time": "2023-05-24T20:57:18.415277",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_2996408/628451567.py:16: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  sub_df = pd.concat([sub_df,df]).reset_index(drop=True)\n"
     ]
    }
   ],
   "source": [
    "# generate submission\n",
    "filenames = df_test.filename.values.tolist()\n",
    "bird_cols = list(pd.get_dummies(df_train['primary_label']).columns)\n",
    "sub_df = pd.DataFrame(columns=['row_id']+bird_cols)\n",
    "\n",
    "for i, file in enumerate(filenames):\n",
    "    pred = predictions[i]\n",
    "    pred_split1 = predictions[i]\n",
    "    num_rows = len(pred)\n",
    "    row_ids = [f'{file}_{(i+1)*5}' for i in range(num_rows)]\n",
    "    df = pd.DataFrame(columns=['row_id']+bird_cols)\n",
    "    \n",
    "    df['row_id'] = row_ids\n",
    "    df[bird_cols] = pred\n",
    "    \n",
    "    sub_df = pd.concat([sub_df,df]).reset_index(drop=True)\n",
    "    \n",
    "sub_df.to_csv('submission.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dccd61d7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = BirdClefSEDAttModel(model_name=\"tf_efficientnetv2_b3\",num_classes=CFG.num_classes,pretrained=CFG.pretrained)\n",
    "model.load_state_dict(torch.load('/root/projects/BirdClef2025/BirdCLEF2023-30th-place-solution-master/logs/efv2b3_pretrainednew_ft_bce-LB0.842.pt',map_location='cpu'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "4cc6f892",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = pd.read_csv('/root/projects/BirdClef2025/BirdCLEF2023-30th-place-solution-master/usefulFunc/train.csv')\n",
    "external_df = pd.read_csv('/root/projects/BirdClef2025/data/external_trainv2.csv')\n",
    "df_train = pd.concat((df_train,external_df))\n",
    "df_train = pd.concat([df_train, pd.get_dummies(df_train['primary_label'])], axis=1)\n",
    "low_numclasses_birds = list(df_train['primary_label'].value_counts()[(df_train['primary_label'].value_counts()<30)].index)\n",
    "low_numclasses_birds_index = []\n",
    "high_numclasses_birds_index = []\n",
    "birds_col = list(df_train.columns[14:])\n",
    "for idx,item in enumerate(birds_col):\n",
    "    if item in low_numclasses_birds:\n",
    "        low_numclasses_birds_index.append(idx)\n",
    "    else:\n",
    "        high_numclasses_birds_index.append(idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "835bf131",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 17, 20, 21, 23, 24, 25, 26, 27, 28, 29, 32, 33, 35, 37, 39, 40, 41, 44, 46, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 127, 137, 154, 159]\n"
     ]
    }
   ],
   "source": [
    "print(low_numclasses_birds_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "7dd449d3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_split1 = BirdClefSEDAttModel(model_name=\"tf_efficientnetv2_b3\",num_classes=len(low_numclasses_birds),pretrained=CFG.pretrained)\n",
    "model_split1.load_state_dict(torch.load('/root/projects/BirdClef2025/BirdCLEF2023-30th-place-solution-master/logs/2025-04-01T12:29/saved_model_lastepoch.pt',map_location='cpu'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "ef7ff0c4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_dict = torch.load('/root/projects/BirdClef2025/BirdCLEF2023-30th-place-solution-master/logs/efv2b3_pretrainednew_ft_bce-LB0.842.pt',map_location='cpu')\n",
    "model_dict_split = torch.load('/root/projects/BirdClef2025/BirdCLEF2023-30th-place-solution-master/logs/2025-04-01T12:29/saved_model_lastepoch.pt')\n",
    "model_dict_split = {k.replace('att_block','att_block_split'):v for k,v in model_dict_split.items() if 'att' in k}\n",
    "model_dict.update(model_dict_split)\n",
    "\n",
    "torch.save(model_dict,'/root/projects/BirdClef2025/BirdCLEF2023-30th-place-solution-master/logs/efv2b3_bce_focal.pt')\n",
    "model_split = SplitModel(low_numclasses_birds_index,model_name='tf_efficientnetv2_b3')\n",
    "model_split.load_state_dict(torch.load('/root/projects/BirdClef2025/BirdCLEF2023-30th-place-solution-master/logs/efv2b3_bce_focal.pt',map_location='cpu'))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cibmtr",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 86.723715,
   "end_time": "2023-05-24T20:57:21.146356",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2023-05-24T20:55:54.422641",
   "version": "2.4.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
