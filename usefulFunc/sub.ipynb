{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "525d5ad2",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2023-05-24T20:56:06.096182Z",
     "iopub.status.busy": "2023-05-24T20:56:06.095491Z",
     "iopub.status.idle": "2023-05-24T20:56:11.382492Z",
     "shell.execute_reply": "2023-05-24T20:56:11.381532Z"
    },
    "papermill": {
     "duration": 5.298607,
     "end_time": "2023-05-24T20:56:11.385283",
     "exception": false,
     "start_time": "2023-05-24T20:56:06.086676",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import timm\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "import soundfile as sf\n",
    "from  soundfile import SoundFile\n",
    "import librosa as lb\n",
    "import librosa.display as lbd\n",
    "import concurrent.futures\n",
    "from torchaudio.transforms import MelSpectrogram,AmplitudeToDB\n",
    "from torchvision import transforms\n",
    "from transformers import EfficientNetForImageClassification\n",
    "import torchaudio\n",
    "from transformers import AutoConfig\n",
    "from torch.optim.swa_utils import AveragedModel, SWALR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "3529a959",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-24T20:56:11.400513Z",
     "iopub.status.busy": "2023-05-24T20:56:11.400136Z",
     "iopub.status.idle": "2023-05-24T20:56:11.411221Z",
     "shell.execute_reply": "2023-05-24T20:56:11.410152Z"
    },
    "papermill": {
     "duration": 0.021075,
     "end_time": "2023-05-24T20:56:11.413425",
     "exception": false,
     "start_time": "2023-05-24T20:56:11.392350",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# config\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class CFG:\n",
    "    seed = 42\n",
    "    num_classes = 206\n",
    "    batch_size = 128\n",
    "    epochs = 30\n",
    "    PRECISION = 16    \n",
    "    PATIENCE = 8    \n",
    "    img_size = [224,224]\n",
    "    model = \"eca_nfnet_l0\"\n",
    "    pretrained = False            \n",
    "    weight_decay = 1e-3\n",
    "    use_mixup = True\n",
    "    mixup_alpha = 0.2   \n",
    "    use_spec_aug = False\n",
    "    p_spec_aug = 0.5\n",
    "    time_shift_prob = 0.5\n",
    "    gn_prob = 0.5\n",
    "    infer_duration = 5\n",
    "    fmin = 40\n",
    "    fmax = 14000\n",
    "    mel_bins = 192\n",
    "    window_size = 1024\n",
    "    n_fft = 2048\n",
    "    hop_size = 512\n",
    "    use_fsr = False\n",
    "\n",
    "    device = torch.device('cuda')  \n",
    "\n",
    "    train_path = \"/root/projects/BirdClef2025/data/train.csv\"\n",
    "    valid_path = \"/root/projects/BirdClef2025/data/valid.csv\"\n",
    "    test_path = '/root/projects/BirdClef2025/data/test_soundscapes/'\n",
    "    sample_rate = 32000\n",
    "    duration = 5\n",
    "    max_read_samples = 10\n",
    "    lr = 5e-5\n",
    "\n",
    "    scheduler     = 'CosineAnnealingLR'\n",
    "    min_lr        = 5e-7\n",
    "    T_max         = int(30000/batch_size*epochs)+50\n",
    "    T_0           = 25\n",
    "\n",
    "    n_accumulate = 4\n",
    "    \n",
    "    target_columns = \"abethr1 abhori1 abythr1 afbfly1 afdfly1 afecuc1 affeag1 afgfly1 afghor1 afmdov1 afpfly1 afpkin1 afpwag1 afrgos1 afrgrp1 afrjac1 afrthr1 amesun2 augbuz1 bagwea1 barswa bawhor2 bawman1 bcbeat1 beasun2 bkctch1 bkfruw1 blacra1 blacuc1 blakit1 blaplo1 blbpuf2 blcapa2 blfbus1 blhgon1 blhher1 blksaw1 blnmou1 blnwea1 bltapa1 bltbar1 bltori1 blwlap1 brcale1 brcsta1 brctch1 brcwea1 brican1 brobab1 broman1 brosun1 brrwhe3 brtcha1 brubru1 brwwar1 bswdov1 btweye2 bubwar2 butapa1 cabgre1 carcha1 carwoo1 categr ccbeat1 chespa1 chewea1 chibat1 chtapa3 chucis1 cibwar1 cohmar1 colsun2 combul2 combuz1 comsan crefra2 crheag1 crohor1 darbar1 darter3 didcuc1 dotbar1 dutdov1 easmog1 eaywag1 edcsun3 egygoo equaka1 eswdov1 eubeat1 fatrav1 fatwid1 fislov1 fotdro5 gabgos2 gargan gbesta1 gnbcam2 gnhsun1 gobbun1 gobsta5 gobwea1 golher1 grbcam1 grccra1 grecor greegr grewoo2 grwpyt1 gryapa1 grywrw1 gybfis1 gycwar3 gyhbus1 gyhkin1 gyhneg1 gyhspa1 gytbar1 hadibi1 hamerk1 hartur1 helgui hipbab1 hoopoe huncis1 hunsun2 joygre1 kerspa2 klacuc1 kvbsun1 laudov1 lawgol lesmaw1 lessts1 libeat1 litegr litswi1 litwea1 loceag1 lotcor1 lotlap1 luebus1 mabeat1 macshr1 malkin1 marsto1 marsun2 mcptit1 meypar1 moccha1 mouwag1 ndcsun2 nobfly1 norbro1 norcro1 norfis1 norpuf1 nubwoo1 pabspa1 palfly2 palpri1 piecro1 piekin1 pitwhy purgre2 pygbat1 quailf1 ratcis1 raybar1 rbsrob1 rebfir2 rebhor1 reboxp1 reccor reccuc1 reedov1 refbar2 refcro1 reftin1 refwar2 rehblu1 rehwea1 reisee2 rerswa1 rewsta1 rindov rocmar2 rostur1 ruegls1 rufcha2 sacibi2 sccsun2 scrcha1 scthon1 shesta1 sichor1 sincis1 slbgre1 slcbou1 sltnig1 sobfly1 somgre1 somtit4 soucit1 soufis1 spemou2 spepig1 spewea1 spfbar1 spfwea1 spmthr1 spwlap1 squher1 strher strsee1 stusta1 subbus1 supsta1 tacsun1 tafpri1 tamdov1 thrnig1 trobou1 varsun2 vibsta2 vilwea1 vimwea1 walsta1 wbgbir1 wbrcha2 wbswea1 wfbeat1 whbcan1 whbcou1 whbcro2 whbtit5 whbwea1 whbwhe3 whcpri2 whctur2 wheslf1 whhsaw1 whihel1 whrshr1 witswa1 wlwwar wookin1 woosan wtbeat1 yebapa1 yebbar1 yebduc1 yebere1 yebgre1 yebsto1 yeccan1 yefcan yelbis1 yenspu1 yertin1 yesbar1 yespet1 yetgre1 yewgre1\".split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "d5c76daa",
   "metadata": {
    "_kg_hide-input": false,
    "execution": {
     "iopub.execute_input": "2023-05-24T20:56:11.429158Z",
     "iopub.status.busy": "2023-05-24T20:56:11.428754Z",
     "iopub.status.idle": "2023-05-24T20:56:11.589773Z",
     "shell.execute_reply": "2023-05-24T20:56:11.588404Z"
    },
    "papermill": {
     "duration": 0.172374,
     "end_time": "2023-05-24T20:56:11.592399",
     "exception": false,
     "start_time": "2023-05-24T20:56:11.420025",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import math\n",
    "import argparse\n",
    "\n",
    "import librosa\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.nn.parameter import Parameter\n",
    "\n",
    "\n",
    "class DFTBase(nn.Module):\n",
    "    def __init__(self):\n",
    "        r\"\"\"Base class for DFT and IDFT matrix.\n",
    "        \"\"\"\n",
    "        super(DFTBase, self).__init__()\n",
    "\n",
    "    def dft_matrix(self, n):\n",
    "        (x, y) = np.meshgrid(np.arange(n), np.arange(n))\n",
    "        omega = np.exp(-2 * np.pi * 1j / n)\n",
    "        W = np.power(omega, x * y)  # shape: (n, n)\n",
    "        return W\n",
    "\n",
    "    def idft_matrix(self, n):\n",
    "        (x, y) = np.meshgrid(np.arange(n), np.arange(n))\n",
    "        omega = np.exp(2 * np.pi * 1j / n)\n",
    "        W = np.power(omega, x * y)  # shape: (n, n)\n",
    "        return W\n",
    "\n",
    "\n",
    "class DFT(DFTBase):\n",
    "    def __init__(self, n, norm):\n",
    "        r\"\"\"Calculate discrete Fourier transform (DFT), inverse DFT (IDFT, \n",
    "        right DFT (RDFT) RDFT, and inverse RDFT (IRDFT.) \n",
    "\n",
    "        Args:\n",
    "          n: fft window size\n",
    "          norm: None | 'ortho'\n",
    "        \"\"\"\n",
    "        super(DFT, self).__init__()\n",
    "\n",
    "        self.W = self.dft_matrix(n)\n",
    "        self.inv_W = self.idft_matrix(n)\n",
    "\n",
    "        self.W_real = torch.Tensor(np.real(self.W))\n",
    "        self.W_imag = torch.Tensor(np.imag(self.W))\n",
    "        self.inv_W_real = torch.Tensor(np.real(self.inv_W))\n",
    "        self.inv_W_imag = torch.Tensor(np.imag(self.inv_W))\n",
    "\n",
    "        self.n = n\n",
    "        self.norm = norm\n",
    "\n",
    "    def dft(self, x_real, x_imag):\n",
    "        r\"\"\"Calculate DFT of a signal.\n",
    "\n",
    "        Args:\n",
    "            x_real: (n,), real part of a signal\n",
    "            x_imag: (n,), imag part of a signal\n",
    "\n",
    "        Returns:\n",
    "            z_real: (n,), real part of output\n",
    "            z_imag: (n,), imag part of output\n",
    "        \"\"\"\n",
    "        z_real = torch.matmul(x_real, self.W_real) - torch.matmul(x_imag, self.W_imag)\n",
    "        z_imag = torch.matmul(x_imag, self.W_real) + torch.matmul(x_real, self.W_imag)\n",
    "        # shape: (n,)\n",
    "\n",
    "        if self.norm is None:\n",
    "            pass\n",
    "        elif self.norm == 'ortho':\n",
    "            z_real /= math.sqrt(self.n)\n",
    "            z_imag /= math.sqrt(self.n)\n",
    "\n",
    "        return z_real, z_imag\n",
    "\n",
    "    def idft(self, x_real, x_imag):\n",
    "        r\"\"\"Calculate IDFT of a signal.\n",
    "\n",
    "        Args:\n",
    "            x_real: (n,), real part of a signal\n",
    "            x_imag: (n,), imag part of a signal\n",
    "        Returns:\n",
    "            z_real: (n,), real part of output\n",
    "            z_imag: (n,), imag part of output\n",
    "        \"\"\"\n",
    "        z_real = torch.matmul(x_real, self.inv_W_real) - torch.matmul(x_imag, self.inv_W_imag)\n",
    "        z_imag = torch.matmul(x_imag, self.inv_W_real) + torch.matmul(x_real, self.inv_W_imag)\n",
    "        # shape: (n,)\n",
    "\n",
    "        if self.norm is None:\n",
    "            z_real /= self.n\n",
    "        elif self.norm == 'ortho':\n",
    "            z_real /= math.sqrt(n)\n",
    "            z_imag /= math.sqrt(n)\n",
    "\n",
    "        return z_real, z_imag\n",
    "\n",
    "    def rdft(self, x_real):\n",
    "        r\"\"\"Calculate right RDFT of signal.\n",
    "\n",
    "        Args:\n",
    "            x_real: (n,), real part of a signal\n",
    "            x_imag: (n,), imag part of a signal\n",
    "\n",
    "        Returns:\n",
    "            z_real: (n // 2 + 1,), real part of output\n",
    "            z_imag: (n // 2 + 1,), imag part of output\n",
    "        \"\"\"\n",
    "        n_rfft = self.n // 2 + 1\n",
    "        z_real = torch.matmul(x_real, self.W_real[..., 0 : n_rfft])\n",
    "        z_imag = torch.matmul(x_real, self.W_imag[..., 0 : n_rfft])\n",
    "        # shape: (n // 2 + 1,)\n",
    "\n",
    "        if self.norm is None:\n",
    "            pass\n",
    "        elif self.norm == 'ortho':\n",
    "            z_real /= math.sqrt(self.n)\n",
    "            z_imag /= math.sqrt(self.n)\n",
    "\n",
    "        return z_real, z_imag\n",
    "\n",
    "    def irdft(self, x_real, x_imag):\n",
    "        r\"\"\"Calculate IRDFT of signal.\n",
    "        \n",
    "        Args:\n",
    "            x_real: (n // 2 + 1,), real part of a signal\n",
    "            x_imag: (n // 2 + 1,), imag part of a signal\n",
    "\n",
    "        Returns:\n",
    "            z_real: (n,), real part of output\n",
    "            z_imag: (n,), imag part of output\n",
    "        \"\"\"\n",
    "        n_rfft = self.n // 2 + 1\n",
    "\n",
    "        flip_x_real = torch.flip(x_real, dims=(-1,))\n",
    "        flip_x_imag = torch.flip(x_imag, dims=(-1,))\n",
    "        # shape: (n // 2 + 1,)\n",
    "\n",
    "        x_real = torch.cat((x_real, flip_x_real[..., 1 : n_rfft - 1]), dim=-1)\n",
    "        x_imag = torch.cat((x_imag, -1. * flip_x_imag[..., 1 : n_rfft - 1]), dim=-1)\n",
    "        # shape: (n,)\n",
    "\n",
    "        z_real = torch.matmul(x_real, self.inv_W_real) - torch.matmul(x_imag, self.inv_W_imag)\n",
    "        # shape: (n,)\n",
    "\n",
    "        if self.norm is None:\n",
    "            z_real /= self.n\n",
    "        elif self.norm == 'ortho':\n",
    "            z_real /= math.sqrt(n)\n",
    "\n",
    "        return z_real\n",
    "\n",
    "\n",
    "class STFT(DFTBase):\n",
    "    def __init__(self, n_fft=2048, hop_length=None, win_length=None,\n",
    "        window='hann', center=True, pad_mode='reflect', freeze_parameters=True):\n",
    "        r\"\"\"PyTorch implementation of STFT with Conv1d. The function has the \n",
    "        same output as librosa.stft.\n",
    "\n",
    "        Args:\n",
    "            n_fft: int, fft window size, e.g., 2048\n",
    "            hop_length: int, hop length samples, e.g., 441\n",
    "            win_length: int, window length e.g., 2048\n",
    "            window: str, window function name, e.g., 'hann'\n",
    "            center: bool\n",
    "            pad_mode: str, e.g., 'reflect'\n",
    "            freeze_parameters: bool, set to True to freeze all parameters. Set\n",
    "                to False to finetune all parameters.\n",
    "        \"\"\"\n",
    "        super(STFT, self).__init__()\n",
    "\n",
    "        assert pad_mode in ['constant', 'reflect']\n",
    "\n",
    "        self.n_fft = n_fft\n",
    "        self.hop_length = hop_length\n",
    "        self.win_length = win_length\n",
    "        self.window = window\n",
    "        self.center = center\n",
    "        self.pad_mode = pad_mode\n",
    "\n",
    "        # By default, use the entire frame.\n",
    "        if self.win_length is None:\n",
    "            self.win_length = n_fft\n",
    "\n",
    "        # Set the default hop, if it's not already specified.\n",
    "        if self.hop_length is None:\n",
    "            self.hop_length = int(self.win_length // 4)\n",
    "\n",
    "        fft_window = librosa.filters.get_window(window, self.win_length, fftbins=True)\n",
    "\n",
    "        # Pad the window out to n_fft size.\n",
    "        fft_window = librosa.util.pad_center(fft_window, size=n_fft)\n",
    "\n",
    "        # DFT & IDFT matrix.\n",
    "        self.W = self.dft_matrix(n_fft)\n",
    "\n",
    "        out_channels = n_fft // 2 + 1\n",
    "\n",
    "        self.conv_real = nn.Conv1d(in_channels=1, out_channels=out_channels,\n",
    "            kernel_size=n_fft, stride=self.hop_length, padding=0, dilation=1,\n",
    "            groups=1, bias=False)\n",
    "\n",
    "        self.conv_imag = nn.Conv1d(in_channels=1, out_channels=out_channels,\n",
    "            kernel_size=n_fft, stride=self.hop_length, padding=0, dilation=1,\n",
    "            groups=1, bias=False)\n",
    "\n",
    "        # Initialize Conv1d weights.\n",
    "        self.conv_real.weight.data = torch.Tensor(\n",
    "            np.real(self.W[:, 0 : out_channels] * fft_window[:, None]).T)[:, None, :]\n",
    "        # (n_fft // 2 + 1, 1, n_fft)\n",
    "\n",
    "        self.conv_imag.weight.data = torch.Tensor(\n",
    "            np.imag(self.W[:, 0 : out_channels] * fft_window[:, None]).T)[:, None, :]\n",
    "        # (n_fft // 2 + 1, 1, n_fft)\n",
    "\n",
    "        if freeze_parameters:\n",
    "            for param in self.parameters():\n",
    "                param.requires_grad = False\n",
    "\n",
    "    def forward(self, input):\n",
    "        r\"\"\"Calculate STFT of batch of signals.\n",
    "\n",
    "        Args: \n",
    "            input: (batch_size, data_length), input signals.\n",
    "\n",
    "        Returns:\n",
    "            real: (batch_size, 1, time_steps, n_fft // 2 + 1)\n",
    "            imag: (batch_size, 1, time_steps, n_fft // 2 + 1)\n",
    "        \"\"\"\n",
    "\n",
    "        x = input[:, None, :]   # (batch_size, channels_num, data_length)\n",
    "\n",
    "        if self.center:\n",
    "            x = F.pad(x, pad=(self.n_fft // 2, self.n_fft // 2), mode=self.pad_mode)\n",
    "\n",
    "        real = self.conv_real(x)\n",
    "        imag = self.conv_imag(x)\n",
    "        # (batch_size, n_fft // 2 + 1, time_steps)\n",
    "\n",
    "        real = real[:, None, :, :].transpose(2, 3)\n",
    "        imag = imag[:, None, :, :].transpose(2, 3)\n",
    "        # (batch_size, 1, time_steps, n_fft // 2 + 1)\n",
    "\n",
    "        return real, imag\n",
    "\n",
    "\n",
    "def magphase(real, imag):\n",
    "    r\"\"\"Calculate magnitude and phase from real and imag part of signals.\n",
    "\n",
    "    Args:\n",
    "        real: tensor, real part of signals\n",
    "        imag: tensor, imag part of signals\n",
    "\n",
    "    Returns:\n",
    "        mag: tensor, magnitude of signals\n",
    "        cos: tensor, cosine of phases of signals\n",
    "        sin: tensor, sine of phases of signals\n",
    "    \"\"\"\n",
    "    mag = (real ** 2 + imag ** 2) ** 0.5\n",
    "    cos = real / torch.clamp(mag, 1e-10, np.inf)\n",
    "    sin = imag / torch.clamp(mag, 1e-10, np.inf)\n",
    "\n",
    "    return mag, cos, sin\n",
    "\n",
    "\n",
    "class ISTFT(DFTBase):\n",
    "    def __init__(self, n_fft=2048, hop_length=None, win_length=None,\n",
    "        window='hann', center=True, pad_mode='reflect', freeze_parameters=True, \n",
    "        onnx=False, frames_num=None, device=None):\n",
    "        \"\"\"PyTorch implementation of ISTFT with Conv1d. The function has the \n",
    "        same output as librosa.istft.\n",
    "\n",
    "        Args:\n",
    "            n_fft: int, fft window size, e.g., 2048\n",
    "            hop_length: int, hop length samples, e.g., 441\n",
    "            win_length: int, window length e.g., 2048\n",
    "            window: str, window function name, e.g., 'hann'\n",
    "            center: bool\n",
    "            pad_mode: str, e.g., 'reflect'\n",
    "            freeze_parameters: bool, set to True to freeze all parameters. Set\n",
    "                to False to finetune all parameters.\n",
    "            onnx: bool, set to True when exporting trained model to ONNX. This\n",
    "                will replace several operations to operators supported by ONNX.\n",
    "            frames_num: None | int, number of frames of audio clips to be \n",
    "                inferneced. Only useable when onnx=True.\n",
    "            device: None | str, device of ONNX. Only useable when onnx=True.\n",
    "        \"\"\"\n",
    "        super(ISTFT, self).__init__()\n",
    "\n",
    "        assert pad_mode in ['constant', 'reflect']\n",
    "\n",
    "        if not onnx:\n",
    "            assert frames_num is None, \"When onnx=False, frames_num must be None!\"\n",
    "            assert device is None, \"When onnx=False, device must be None!\"\n",
    "\n",
    "        self.n_fft = n_fft\n",
    "        self.hop_length = hop_length\n",
    "        self.win_length = win_length\n",
    "        self.window = window\n",
    "        self.center = center\n",
    "        self.pad_mode = pad_mode\n",
    "        self.onnx = onnx\n",
    "\n",
    "        # By default, use the entire frame.\n",
    "        if self.win_length is None:\n",
    "            self.win_length = self.n_fft\n",
    "\n",
    "        # Set the default hop, if it's not already specified.\n",
    "        if self.hop_length is None:\n",
    "            self.hop_length = int(self.win_length // 4)\n",
    "\n",
    "        # Initialize Conv1d modules for calculating real and imag part of DFT.\n",
    "        self.init_real_imag_conv()\n",
    "\n",
    "        # Initialize overlap add window for reconstruct time domain signals.\n",
    "        self.init_overlap_add_window()\n",
    "\n",
    "        if self.onnx:\n",
    "            # Initialize ONNX modules.\n",
    "            self.init_onnx_modules(frames_num, device)\n",
    "        \n",
    "        if freeze_parameters:\n",
    "            for param in self.parameters():\n",
    "                param.requires_grad = False\n",
    "\n",
    "    def init_real_imag_conv(self):\n",
    "        r\"\"\"Initialize Conv1d for calculating real and imag part of DFT.\n",
    "        \"\"\"\n",
    "        self.W = self.idft_matrix(self.n_fft) / self.n_fft\n",
    "\n",
    "        self.conv_real = nn.Conv1d(in_channels=self.n_fft, out_channels=self.n_fft,\n",
    "            kernel_size=1, stride=1, padding=0, dilation=1,\n",
    "            groups=1, bias=False)\n",
    "\n",
    "        self.conv_imag = nn.Conv1d(in_channels=self.n_fft, out_channels=self.n_fft,\n",
    "            kernel_size=1, stride=1, padding=0, dilation=1,\n",
    "            groups=1, bias=False)\n",
    "\n",
    "        ifft_window = librosa.filters.get_window(self.window, self.win_length, fftbins=True)\n",
    "        # (win_length,)\n",
    "\n",
    "        # Pad the window to n_fft\n",
    "        ifft_window = librosa.util.pad_center(ifft_window, size=self.n_fft)\n",
    "\n",
    "        self.conv_real.weight.data = torch.Tensor(\n",
    "            np.real(self.W * ifft_window[None, :]).T)[:, :, None]\n",
    "        # (n_fft // 2 + 1, 1, n_fft)\n",
    "\n",
    "        self.conv_imag.weight.data = torch.Tensor(\n",
    "            np.imag(self.W * ifft_window[None, :]).T)[:, :, None]\n",
    "        # (n_fft // 2 + 1, 1, n_fft)\n",
    "\n",
    "    def init_overlap_add_window(self):\n",
    "        r\"\"\"Initialize overlap add window for reconstruct time domain signals.\n",
    "        \"\"\"\n",
    "        \n",
    "        ola_window = librosa.filters.get_window(self.window, self.win_length, fftbins=True)\n",
    "        # (win_length,)\n",
    "\n",
    "        ola_window = librosa.util.normalize(ola_window, norm=None) ** 2\n",
    "        ola_window = librosa.util.pad_center(ola_window, size=self.n_fft)\n",
    "        ola_window = torch.Tensor(ola_window)\n",
    "\n",
    "        self.register_buffer('ola_window', ola_window)\n",
    "        # (win_length,)\n",
    "\n",
    "    def init_onnx_modules(self, frames_num, device):\n",
    "        r\"\"\"Initialize ONNX modules.\n",
    "\n",
    "        Args:\n",
    "            frames_num: int\n",
    "            device: str | None\n",
    "        \"\"\"\n",
    "\n",
    "        # Use Conv1d to implement torch.flip(), because torch.flip() is not \n",
    "        # supported by ONNX.\n",
    "        self.reverse = nn.Conv1d(in_channels=self.n_fft // 2 + 1,\n",
    "            out_channels=self.n_fft // 2 - 1, kernel_size=1, bias=False)\n",
    "\n",
    "        tmp = np.zeros((self.n_fft // 2 - 1, self.n_fft // 2 + 1, 1))\n",
    "        tmp[:, 1 : -1, 0] = np.array(np.eye(self.n_fft // 2 - 1)[::-1])\n",
    "        self.reverse.weight.data = torch.Tensor(tmp)\n",
    "        # (n_fft // 2 - 1, n_fft // 2 + 1, 1)\n",
    "\n",
    "        # Use nn.ConvTranspose2d to implement torch.nn.functional.fold(), \n",
    "        # because torch.nn.functional.fold() is not supported by ONNX.\n",
    "        self.overlap_add = nn.ConvTranspose2d(in_channels=self.n_fft,\n",
    "            out_channels=1, kernel_size=(self.n_fft, 1), stride=(self.hop_length, 1), bias=False)\n",
    "\n",
    "        self.overlap_add.weight.data = torch.Tensor(np.eye(self.n_fft)[:, None, :, None])\n",
    "        # (n_fft, 1, n_fft, 1)\n",
    "\n",
    "        if frames_num:\n",
    "            # Pre-calculate overlap-add window sum for reconstructing signals\n",
    "            # when using ONNX.\n",
    "            self.ifft_window_sum = self._get_ifft_window_sum_onnx(frames_num, device)\n",
    "        else:\n",
    "            self.ifft_window_sum = []\n",
    "\n",
    "    def forward(self, real_stft, imag_stft, length):\n",
    "        r\"\"\"Calculate inverse STFT.\n",
    "\n",
    "        Args:\n",
    "            real_stft: (batch_size, channels=1, time_steps, n_fft // 2 + 1)\n",
    "            imag_stft: (batch_size, channels=1, time_steps, n_fft // 2 + 1)\n",
    "            length: int\n",
    "        \n",
    "        Returns:\n",
    "            real: (batch_size, data_length), output signals.\n",
    "        \"\"\"\n",
    "        assert real_stft.ndimension() == 4 and imag_stft.ndimension() == 4\n",
    "        batch_size, _, frames_num, _ = real_stft.shape\n",
    "\n",
    "        real_stft = real_stft[:, 0, :, :].transpose(1, 2)\n",
    "        imag_stft = imag_stft[:, 0, :, :].transpose(1, 2)\n",
    "        # (batch_size, n_fft // 2 + 1, time_steps)\n",
    "\n",
    "        # Get full stft representation from spectrum using symmetry attribute.\n",
    "        if self.onnx:\n",
    "            full_real_stft, full_imag_stft = self._get_full_stft_onnx(real_stft, imag_stft)\n",
    "        else:\n",
    "            full_real_stft, full_imag_stft = self._get_full_stft(real_stft, imag_stft)\n",
    "        # full_real_stft: (batch_size, n_fft, time_steps)\n",
    "        # full_imag_stft: (batch_size, n_fft, time_steps)\n",
    "\n",
    "        # Calculate IDFT frame by frame.\n",
    "        s_real = self.conv_real(full_real_stft) - self.conv_imag(full_imag_stft)\n",
    "        # (batch_size, n_fft, time_steps)\n",
    "\n",
    "        # Overlap add signals in frames to reconstruct signals.\n",
    "        if self.onnx:\n",
    "            y = self._overlap_add_divide_window_sum_onnx(s_real, frames_num)\n",
    "        else:\n",
    "            y = self._overlap_add_divide_window_sum(s_real, frames_num)\n",
    "        # y: (batch_size, audio_samples + win_length,)\n",
    "        \n",
    "        y = self._trim_edges(y, length)\n",
    "        # (batch_size, audio_samples,)\n",
    "            \n",
    "        return y\n",
    "\n",
    "    def _get_full_stft(self, real_stft, imag_stft):\n",
    "        r\"\"\"Get full stft representation from spectrum using symmetry attribute.\n",
    "\n",
    "        Args:\n",
    "            real_stft: (batch_size, n_fft // 2 + 1, time_steps)\n",
    "            imag_stft: (batch_size, n_fft // 2 + 1, time_steps)\n",
    "\n",
    "        Returns:\n",
    "            full_real_stft: (batch_size, n_fft, time_steps)\n",
    "            full_imag_stft: (batch_size, n_fft, time_steps)\n",
    "        \"\"\"\n",
    "        full_real_stft = torch.cat((real_stft, torch.flip(real_stft[:, 1 : -1, :], dims=[1])), dim=1)\n",
    "        full_imag_stft = torch.cat((imag_stft, - torch.flip(imag_stft[:, 1 : -1, :], dims=[1])), dim=1)\n",
    "\n",
    "        return full_real_stft, full_imag_stft\n",
    "\n",
    "    def _get_full_stft_onnx(self, real_stft, imag_stft):\n",
    "        r\"\"\"Get full stft representation from spectrum using symmetry attribute\n",
    "        for ONNX. Replace several pytorch operations in self._get_full_stft() \n",
    "        that are not supported by ONNX.\n",
    "\n",
    "        Args:\n",
    "            real_stft: (batch_size, n_fft // 2 + 1, time_steps)\n",
    "            imag_stft: (batch_size, n_fft // 2 + 1, time_steps)\n",
    "\n",
    "        Returns:\n",
    "            full_real_stft: (batch_size, n_fft, time_steps)\n",
    "            full_imag_stft: (batch_size, n_fft, time_steps)\n",
    "        \"\"\"\n",
    "\n",
    "        # Implement torch.flip() with Conv1d.\n",
    "        full_real_stft = torch.cat((real_stft, self.reverse(real_stft)), dim=1)\n",
    "        full_imag_stft = torch.cat((imag_stft, - self.reverse(imag_stft)), dim=1)\n",
    "\n",
    "        return full_real_stft, full_imag_stft\n",
    "\n",
    "    def _overlap_add_divide_window_sum(self, s_real, frames_num):\n",
    "        r\"\"\"Overlap add signals in frames to reconstruct signals.\n",
    "\n",
    "        Args:\n",
    "            s_real: (batch_size, n_fft, time_steps), signals in frames\n",
    "            frames_num: int\n",
    "\n",
    "        Returns:\n",
    "            y: (batch_size, audio_samples)\n",
    "        \"\"\"\n",
    "        \n",
    "        output_samples = (s_real.shape[-1] - 1) * self.hop_length + self.win_length\n",
    "        # (audio_samples,)\n",
    "\n",
    "        # Overlap-add signals in frames to signals. Ref: \n",
    "        # asteroid_filterbanks.torch_stft_fb.torch_stft_fb() from\n",
    "        # https://github.com/asteroid-team/asteroid-filterbanks\n",
    "        y = torch.nn.functional.fold(input=s_real, output_size=(1, output_samples), \n",
    "            kernel_size=(1, self.win_length), stride=(1, self.hop_length))\n",
    "        # (batch_size, 1, 1, audio_samples,)\n",
    "        \n",
    "        y = y[:, 0, 0, :]\n",
    "        # (batch_size, audio_samples)\n",
    "\n",
    "        # Get overlap-add window sum to be divided.\n",
    "        ifft_window_sum = self._get_ifft_window(frames_num)\n",
    "        # (audio_samples,)\n",
    "\n",
    "        # Following code is abandaned for divide overlap-add window, because\n",
    "        # not supported by half precision training and ONNX.\n",
    "        # min_mask = ifft_window_sum.abs() < 1e-11\n",
    "        # y[:, ~min_mask] = y[:, ~min_mask] / ifft_window_sum[None, ~min_mask]\n",
    "        # # (batch_size, audio_samples)\n",
    "\n",
    "        ifft_window_sum = torch.clamp(ifft_window_sum, 1e-11, np.inf)\n",
    "        # (audio_samples,)\n",
    "\n",
    "        y = y / ifft_window_sum[None, :]\n",
    "        # (batch_size, audio_samples,)\n",
    "\n",
    "        return y\n",
    "\n",
    "    def _get_ifft_window(self, frames_num):\n",
    "        r\"\"\"Get overlap-add window sum to be divided.\n",
    "\n",
    "        Args:\n",
    "            frames_num: int\n",
    "\n",
    "        Returns:\n",
    "            ifft_window_sum: (audio_samlpes,), overlap-add window sum to be \n",
    "            divided.\n",
    "        \"\"\"\n",
    "        \n",
    "        output_samples = (frames_num - 1) * self.hop_length + self.win_length\n",
    "        # (audio_samples,)\n",
    "\n",
    "        window_matrix = self.ola_window[None, :, None].repeat(1, 1, frames_num)\n",
    "        # (batch_size, win_length, time_steps)\n",
    "\n",
    "        ifft_window_sum = F.fold(input=window_matrix, \n",
    "            output_size=(1, output_samples), kernel_size=(1, self.win_length), \n",
    "            stride=(1, self.hop_length))\n",
    "        # (1, 1, 1, audio_samples)\n",
    "        \n",
    "        ifft_window_sum = ifft_window_sum.squeeze()\n",
    "        # (audio_samlpes,)\n",
    "\n",
    "        return ifft_window_sum\n",
    "\n",
    "    def _overlap_add_divide_window_sum_onnx(self, s_real, frames_num):\n",
    "        r\"\"\"Overlap add signals in frames to reconstruct signals for ONNX. \n",
    "        Replace several pytorch operations in \n",
    "        self._overlap_add_divide_window_sum() that are not supported by ONNX.\n",
    "\n",
    "        Args:\n",
    "            s_real: (batch_size, n_fft, time_steps), signals in frames\n",
    "            frames_num: int\n",
    "\n",
    "        Returns:\n",
    "            y: (batch_size, audio_samples)\n",
    "        \"\"\"\n",
    "\n",
    "        s_real = s_real[..., None]\n",
    "        # (batch_size, n_fft, time_steps, 1)\n",
    "\n",
    "        # Implement overlap-add with Conv1d, because torch.nn.functional.fold()\n",
    "        # is not supported by ONNX.\n",
    "        y = self.overlap_add(s_real)[:, 0, :, 0]    \n",
    "        # y: (batch_size, samples_num)\n",
    "        \n",
    "        if len(self.ifft_window_sum) != y.shape[1]:\n",
    "            device = s_real.device\n",
    "\n",
    "            self.ifft_window_sum = self._get_ifft_window_sum_onnx(frames_num, device)\n",
    "            # (audio_samples,)\n",
    "\n",
    "        # Use torch.clamp() to prevent from underflow to make sure all \n",
    "        # operations are supported by ONNX.\n",
    "        ifft_window_sum = torch.clamp(self.ifft_window_sum, 1e-11, np.inf)\n",
    "        # (audio_samples,)\n",
    "\n",
    "        y = y / ifft_window_sum[None, :]\n",
    "        # (batch_size, audio_samples,)\n",
    "        \n",
    "        return y\n",
    "\n",
    "    def _get_ifft_window_sum_onnx(self, frames_num, device):\n",
    "        r\"\"\"Pre-calculate overlap-add window sum for reconstructing signals when\n",
    "        using ONNX.\n",
    "\n",
    "        Args:\n",
    "            frames_num: int\n",
    "            device: str | None\n",
    "\n",
    "        Returns:\n",
    "            ifft_window_sum: (audio_samples,)\n",
    "        \"\"\"\n",
    "        \n",
    "        ifft_window_sum = librosa.filters.window_sumsquare(window=self.window, \n",
    "            n_frames=frames_num, win_length=self.win_length, n_fft=self.n_fft, \n",
    "            hop_length=self.hop_length)\n",
    "        # (audio_samples,)\n",
    "\n",
    "        ifft_window_sum = torch.Tensor(ifft_window_sum)\n",
    "\n",
    "        if device:\n",
    "            ifft_window_sum = ifft_window_sum.to(device)\n",
    "\n",
    "        return ifft_window_sum\n",
    "\n",
    "    def _trim_edges(self, y, length):\n",
    "        r\"\"\"Trim audio.\n",
    "\n",
    "        Args:\n",
    "            y: (audio_samples,)\n",
    "            length: int\n",
    "\n",
    "        Returns:\n",
    "            (trimmed_audio_samples,)\n",
    "        \"\"\"\n",
    "        # Trim or pad to length\n",
    "        if length is None:\n",
    "            if self.center:\n",
    "                y = y[:, self.n_fft // 2 : -self.n_fft // 2]\n",
    "        else:\n",
    "            if self.center:\n",
    "                start = self.n_fft // 2\n",
    "            else:\n",
    "                start = 0\n",
    "\n",
    "            y = y[:, start : start + length]\n",
    "\n",
    "        return y\n",
    "\n",
    "\n",
    "class Spectrogram(nn.Module):\n",
    "    def __init__(self, n_fft=2048, hop_length=None, win_length=None,\n",
    "        window='hann', center=True, pad_mode='reflect', power=2.0,\n",
    "        freeze_parameters=True):\n",
    "        r\"\"\"Calculate spectrogram using pytorch. The STFT is implemented with \n",
    "        Conv1d. The function has the same output of librosa.stft\n",
    "        \"\"\"\n",
    "        super(Spectrogram, self).__init__()\n",
    "\n",
    "        self.power = power\n",
    "\n",
    "        self.stft = STFT(n_fft=n_fft, hop_length=hop_length,\n",
    "            win_length=win_length, window=window, center=center,\n",
    "            pad_mode=pad_mode, freeze_parameters=True)\n",
    "\n",
    "    def forward(self, input):\n",
    "        r\"\"\"Calculate spectrogram of input signals.\n",
    "        Args: \n",
    "            input: (batch_size, data_length)\n",
    "\n",
    "        Returns:\n",
    "            spectrogram: (batch_size, 1, time_steps, n_fft // 2 + 1)\n",
    "        \"\"\"\n",
    "\n",
    "        (real, imag) = self.stft.forward(input)\n",
    "        # (batch_size, n_fft // 2 + 1, time_steps)\n",
    "\n",
    "        spectrogram = real ** 2 + imag ** 2\n",
    "\n",
    "        if self.power == 2.0:\n",
    "            pass\n",
    "        else:\n",
    "            spectrogram = spectrogram ** (self.power / 2.0)\n",
    "\n",
    "        return spectrogram\n",
    "\n",
    "\n",
    "class LogmelFilterBank(nn.Module):\n",
    "    def __init__(self, sr=22050, n_fft=2048, n_mels=64, fmin=0.0, fmax=None, \n",
    "        is_log=True, ref=1.0, amin=1e-10, top_db=80.0, freeze_parameters=True):\n",
    "        r\"\"\"Calculate logmel spectrogram using pytorch. The mel filter bank is \n",
    "        the pytorch implementation of as librosa.filters.mel \n",
    "        \"\"\"\n",
    "        super(LogmelFilterBank, self).__init__()\n",
    "\n",
    "        self.is_log = is_log\n",
    "        self.ref = ref\n",
    "        self.amin = amin\n",
    "        self.top_db = top_db\n",
    "        if fmax == None:\n",
    "            fmax = sr//2\n",
    "\n",
    "        self.melW = librosa.filters.mel(sr=sr, n_fft=n_fft, n_mels=n_mels,\n",
    "            fmin=fmin, fmax=fmax).T\n",
    "        # (n_fft // 2 + 1, mel_bins)\n",
    "\n",
    "        self.melW = nn.Parameter(torch.Tensor(self.melW))\n",
    "\n",
    "        if freeze_parameters:\n",
    "            for param in self.parameters():\n",
    "                param.requires_grad = False\n",
    "\n",
    "    def forward(self, input):\n",
    "        r\"\"\"Calculate (log) mel spectrogram from spectrogram.\n",
    "\n",
    "        Args:\n",
    "            input: (*, n_fft), spectrogram\n",
    "        \n",
    "        Returns: \n",
    "            output: (*, mel_bins), (log) mel spectrogram\n",
    "        \"\"\"\n",
    "\n",
    "        # Mel spectrogram\n",
    "        mel_spectrogram = torch.matmul(input, self.melW)\n",
    "        # (*, mel_bins)\n",
    "\n",
    "        # Logmel spectrogram\n",
    "        if self.is_log:\n",
    "            output = self.power_to_db(mel_spectrogram)\n",
    "        else:\n",
    "            output = mel_spectrogram\n",
    "\n",
    "        return output\n",
    "\n",
    "\n",
    "    def power_to_db(self, input):\n",
    "        r\"\"\"Power to db, this function is the pytorch implementation of \n",
    "        librosa.power_to_lb\n",
    "        \"\"\"\n",
    "        ref_value = self.ref\n",
    "        log_spec = 10.0 * torch.log10(torch.clamp(input, min=self.amin, max=np.inf))\n",
    "        log_spec -= 10.0 * np.log10(np.maximum(self.amin, ref_value))\n",
    "\n",
    "        if self.top_db is not None:\n",
    "            if self.top_db < 0:\n",
    "                raise librosa.util.exceptions.ParameterError('top_db must be non-negative')\n",
    "            log_spec = torch.clamp(log_spec, min=log_spec.max().item() - self.top_db, max=np.inf)\n",
    "\n",
    "        return log_spec\n",
    "\n",
    "\n",
    "class Enframe(nn.Module):\n",
    "    def __init__(self, frame_length=2048, hop_length=512):\n",
    "        r\"\"\"Enframe a time sequence. This function is the pytorch implementation \n",
    "        of librosa.util.frame\n",
    "        \"\"\"\n",
    "        super(Enframe, self).__init__()\n",
    "\n",
    "        self.enframe_conv = nn.Conv1d(in_channels=1, out_channels=frame_length,\n",
    "            kernel_size=frame_length, stride=hop_length,\n",
    "            padding=0, bias=False)\n",
    "\n",
    "        self.enframe_conv.weight.data = torch.Tensor(torch.eye(frame_length)[:, None, :])\n",
    "        self.enframe_conv.weight.requires_grad = False\n",
    "\n",
    "    def forward(self, input):\n",
    "        r\"\"\"Enframe signals into frames.\n",
    "        Args:\n",
    "            input: (batch_size, samples)\n",
    "        \n",
    "        Returns: \n",
    "            output: (batch_size, window_length, frames_num)\n",
    "        \"\"\"\n",
    "        output = self.enframe_conv(input[:, None, :])\n",
    "        return output\n",
    "\n",
    "\n",
    "    def power_to_db(self, input):\n",
    "        r\"\"\"Power to db, this function is the pytorch implementation of \n",
    "        librosa.power_to_lb.\n",
    "        \"\"\"\n",
    "        ref_value = self.ref\n",
    "        log_spec = 10.0 * torch.log10(torch.clamp(input, min=self.amin, max=np.inf))\n",
    "        log_spec -= 10.0 * np.log10(np.maximum(self.amin, ref_value))\n",
    "\n",
    "        if self.top_db is not None:\n",
    "            if self.top_db < 0:\n",
    "                raise librosa.util.exceptions.ParameterError('top_db must be non-negative')\n",
    "            log_spec = torch.clamp(log_spec, min=log_spec.max() - self.top_db, max=np.inf)\n",
    "\n",
    "        return log_spec\n",
    "\n",
    "\n",
    "class Scalar(nn.Module):\n",
    "    def __init__(self, scalar, freeze_parameters):\n",
    "        super(Scalar, self).__init__()\n",
    "\n",
    "        self.scalar_mean = Parameter(torch.Tensor(scalar['mean']))\n",
    "        self.scalar_std = Parameter(torch.Tensor(scalar['std']))\n",
    "\n",
    "        if freeze_parameters:\n",
    "            for param in self.parameters():\n",
    "                param.requires_grad = False\n",
    "\n",
    "    def forward(self, input):\n",
    "        return (input - self.scalar_mean) / self.scalar_std\n",
    "\n",
    "\n",
    "def debug(select, device):\n",
    "    \"\"\"Compare numpy + librosa and torchlibrosa results. For debug. \n",
    "\n",
    "    Args:\n",
    "        select: 'dft' | 'logmel'\n",
    "        device: 'cpu' | 'cuda'\n",
    "    \"\"\"\n",
    "\n",
    "    if select == 'dft':\n",
    "        n = 10\n",
    "        norm = None     # None | 'ortho'\n",
    "        np.random.seed(0)\n",
    "\n",
    "        # Data\n",
    "        np_data = np.random.uniform(-1, 1, n)\n",
    "        pt_data = torch.Tensor(np_data)\n",
    "\n",
    "        # Numpy FFT\n",
    "        np_fft = np.fft.fft(np_data, norm=norm)\n",
    "        np_ifft = np.fft.ifft(np_fft, norm=norm)\n",
    "        np_rfft = np.fft.rfft(np_data, norm=norm)\n",
    "        np_irfft = np.fft.ifft(np_rfft, norm=norm)\n",
    "\n",
    "        # Pytorch FFT\n",
    "        obj = DFT(n, norm)\n",
    "        pt_dft = obj.dft(pt_data, torch.zeros_like(pt_data))\n",
    "        pt_idft = obj.idft(pt_dft[0], pt_dft[1])\n",
    "        pt_rdft = obj.rdft(pt_data)\n",
    "        pt_irdft = obj.irdft(pt_rdft[0], pt_rdft[1])\n",
    "\n",
    "        print('Comparing librosa and pytorch implementation of DFT. All numbers '\n",
    "            'below should be close to 0.')\n",
    "        print(np.mean((np.abs(np.real(np_fft) - pt_dft[0].cpu().numpy()))))\n",
    "        print(np.mean((np.abs(np.imag(np_fft) - pt_dft[1].cpu().numpy()))))\n",
    "\n",
    "        print(np.mean((np.abs(np.real(np_ifft) - pt_idft[0].cpu().numpy()))))\n",
    "        print(np.mean((np.abs(np.imag(np_ifft) - pt_idft[1].cpu().numpy()))))\n",
    "\n",
    "        print(np.mean((np.abs(np.real(np_rfft) - pt_rdft[0].cpu().numpy()))))\n",
    "        print(np.mean((np.abs(np.imag(np_rfft) - pt_rdft[1].cpu().numpy()))))\n",
    "\n",
    "        print(np.mean(np.abs(np_data - pt_irdft.cpu().numpy())))\n",
    "\n",
    "    elif select == 'stft':\n",
    "        device = torch.device(device)\n",
    "        np.random.seed(0)\n",
    "\n",
    "        # Spectrogram parameters (the same as librosa.stft)\n",
    "        sample_rate = 22050\n",
    "        data_length = sample_rate * 1\n",
    "        n_fft = 2048\n",
    "        hop_length = 512\n",
    "        win_length = 2048\n",
    "        window = 'hann'\n",
    "        center = True\n",
    "        pad_mode = 'reflect'\n",
    "\n",
    "        # Data\n",
    "        np_data = np.random.uniform(-1, 1, data_length)\n",
    "        pt_data = torch.Tensor(np_data).to(device)\n",
    "\n",
    "        # Numpy stft matrix\n",
    "        np_stft_matrix = librosa.stft(y=np_data, n_fft=n_fft,\n",
    "            hop_length=hop_length, window=window, center=center).T\n",
    "\n",
    "        # Pytorch stft matrix\n",
    "        pt_stft_extractor = STFT(n_fft=n_fft, hop_length=hop_length,\n",
    "            win_length=win_length, window=window, center=center, pad_mode=pad_mode,\n",
    "            freeze_parameters=True)\n",
    "\n",
    "        pt_stft_extractor.to(device)\n",
    "\n",
    "        (pt_stft_real, pt_stft_imag) = pt_stft_extractor.forward(pt_data[None, :])\n",
    "\n",
    "        print('Comparing librosa and pytorch implementation of STFT & ISTFT. \\\n",
    "            All numbers below should be close to 0.')\n",
    "        print(np.mean(np.abs(np.real(np_stft_matrix) - pt_stft_real.data.cpu().numpy()[0, 0])))\n",
    "        print(np.mean(np.abs(np.imag(np_stft_matrix) - pt_stft_imag.data.cpu().numpy()[0, 0])))\n",
    "\n",
    "        # Numpy istft\n",
    "        np_istft_s = librosa.istft(stft_matrix=np_stft_matrix.T,\n",
    "            hop_length=hop_length, window=window, center=center, length=data_length)\n",
    "\n",
    "        # Pytorch istft\n",
    "        pt_istft_extractor = ISTFT(n_fft=n_fft, hop_length=hop_length,\n",
    "            win_length=win_length, window=window, center=center, pad_mode=pad_mode,\n",
    "            freeze_parameters=True)\n",
    "        pt_istft_extractor.to(device)\n",
    "\n",
    "        # Recover from real and imag part\n",
    "        pt_istft_s = pt_istft_extractor.forward(pt_stft_real, pt_stft_imag, data_length)[0, :]\n",
    "\n",
    "        # Recover from magnitude and phase\n",
    "        (pt_stft_mag, cos, sin) = magphase(pt_stft_real, pt_stft_imag)\n",
    "        pt_istft_s2 = pt_istft_extractor.forward(pt_stft_mag * cos, pt_stft_mag * sin, data_length)[0, :]\n",
    "\n",
    "        print(np.mean(np.abs(np_istft_s - pt_istft_s.data.cpu().numpy())))\n",
    "        print(np.mean(np.abs(np_data - pt_istft_s.data.cpu().numpy())))\n",
    "        print(np.mean(np.abs(np_data - pt_istft_s2.data.cpu().numpy())))\n",
    "\n",
    "    elif select == 'logmel':\n",
    "        dtype = np.complex64\n",
    "        device = torch.device(device)\n",
    "        np.random.seed(0)\n",
    "\n",
    "        # Spectrogram parameters (the same as librosa.stft)\n",
    "        sample_rate = 22050\n",
    "        data_length = sample_rate * 1\n",
    "        n_fft = 2048\n",
    "        hop_length = 512\n",
    "        win_length = 2048\n",
    "        window = 'hann'\n",
    "        center = True\n",
    "        pad_mode = 'reflect'\n",
    "\n",
    "        # Mel parameters (the same as librosa.feature.melspectrogram)\n",
    "        n_mels = 128\n",
    "        fmin = 0.\n",
    "        fmax = sample_rate / 2.0\n",
    "\n",
    "        # Power to db parameters (the same as default settings of librosa.power_to_db\n",
    "        ref = 1.0\n",
    "        amin = 1e-10\n",
    "        top_db = 80.0\n",
    "\n",
    "        # Data\n",
    "        np_data = np.random.uniform(-1, 1, data_length)\n",
    "        pt_data = torch.Tensor(np_data).to(device)\n",
    "\n",
    "        print('Comparing librosa and pytorch implementation of logmel '\n",
    "            'spectrogram. All numbers below should be close to 0.')\n",
    "\n",
    "        # Numpy librosa\n",
    "        np_stft_matrix = librosa.stft(y=np_data, n_fft=n_fft, hop_length=hop_length,\n",
    "            win_length=win_length, window=window, center=center, dtype=dtype,\n",
    "            pad_mode=pad_mode)\n",
    "\n",
    "        np_pad = np.pad(np_data, int(n_fft // 2), mode=pad_mode)\n",
    "\n",
    "        np_melW = librosa.filters.mel(sr=sample_rate, n_fft=n_fft, n_mels=n_mels,\n",
    "            fmin=fmin, fmax=fmax).T\n",
    "\n",
    "        np_mel_spectrogram = np.dot(np.abs(np_stft_matrix.T) ** 2, np_melW)\n",
    "\n",
    "        np_logmel_spectrogram = librosa.power_to_db(\n",
    "            np_mel_spectrogram, ref=ref, amin=amin, top_db=top_db)\n",
    "\n",
    "        # Pytorch\n",
    "        stft_extractor = STFT(n_fft=n_fft, hop_length=hop_length,\n",
    "            win_length=win_length, window=window, center=center, pad_mode=pad_mode,\n",
    "            freeze_parameters=True)\n",
    "\n",
    "        logmel_extractor = LogmelFilterBank(sr=sample_rate, n_fft=n_fft,\n",
    "            n_mels=n_mels, fmin=fmin, fmax=fmax, ref=ref, amin=amin,\n",
    "            top_db=top_db, freeze_parameters=True)\n",
    "\n",
    "        stft_extractor.to(device)\n",
    "        logmel_extractor.to(device)\n",
    "\n",
    "        pt_pad = F.pad(pt_data[None, None, :], pad=(n_fft // 2, n_fft // 2), mode=pad_mode)[0, 0]\n",
    "        print(np.mean(np.abs(np_pad - pt_pad.cpu().numpy())))\n",
    "\n",
    "        pt_stft_matrix_real = stft_extractor.conv_real(pt_pad[None, None, :])[0]\n",
    "        pt_stft_matrix_imag = stft_extractor.conv_imag(pt_pad[None, None, :])[0]\n",
    "        print(np.mean(np.abs(np.real(np_stft_matrix) - pt_stft_matrix_real.data.cpu().numpy())))\n",
    "        print(np.mean(np.abs(np.imag(np_stft_matrix) - pt_stft_matrix_imag.data.cpu().numpy())))\n",
    "\n",
    "        # Spectrogram\n",
    "        spectrogram_extractor = Spectrogram(n_fft=n_fft, hop_length=hop_length,\n",
    "            win_length=win_length, window=window, center=center, pad_mode=pad_mode,\n",
    "            freeze_parameters=True)\n",
    "\n",
    "        spectrogram_extractor.to(device)\n",
    "\n",
    "        pt_spectrogram = spectrogram_extractor.forward(pt_data[None, :])\n",
    "        pt_mel_spectrogram = torch.matmul(pt_spectrogram, logmel_extractor.melW)\n",
    "        print(np.mean(np.abs(np_mel_spectrogram - pt_mel_spectrogram.data.cpu().numpy()[0, 0])))\n",
    "\n",
    "        # Log mel spectrogram\n",
    "        pt_logmel_spectrogram = logmel_extractor.forward(pt_spectrogram)\n",
    "        print(np.mean(np.abs(np_logmel_spectrogram - pt_logmel_spectrogram[0, 0].data.cpu().numpy())))\n",
    "\n",
    "    elif select == 'enframe':\n",
    "        device = torch.device(device)\n",
    "        np.random.seed(0)\n",
    "\n",
    "        # Spectrogram parameters (the same as librosa.stft)\n",
    "        sample_rate = 22050\n",
    "        data_length = sample_rate * 1\n",
    "        hop_length = 512\n",
    "        win_length = 2048\n",
    "\n",
    "        # Data\n",
    "        np_data = np.random.uniform(-1, 1, data_length)\n",
    "        pt_data = torch.Tensor(np_data).to(device)\n",
    "\n",
    "        print('Comparing librosa and pytorch implementation of '\n",
    "            'librosa.util.frame. All numbers below should be close to 0.')\n",
    "\n",
    "        # Numpy librosa\n",
    "        np_frames = librosa.util.frame(np_data, frame_length=win_length,\n",
    "            hop_length=hop_length)\n",
    "\n",
    "        # Pytorch\n",
    "        pt_frame_extractor = Enframe(frame_length=win_length, hop_length=hop_length)\n",
    "        pt_frame_extractor.to(device)\n",
    "\n",
    "        pt_frames = pt_frame_extractor(pt_data[None, :])\n",
    "        print(np.mean(np.abs(np_frames - pt_frames.data.cpu().numpy())))\n",
    "\n",
    "    elif select == 'default':\n",
    "        device = torch.device(device)\n",
    "        np.random.seed(0)\n",
    "\n",
    "        # Spectrogram parameters (the same as librosa.stft)\n",
    "        sample_rate = 22050\n",
    "        data_length = sample_rate * 1\n",
    "        hop_length = 512\n",
    "        win_length = 2048\n",
    "\n",
    "        # Mel parameters (the same as librosa.feature.melspectrogram)\n",
    "        n_mels = 128\n",
    "\n",
    "        # Data\n",
    "        np_data = np.random.uniform(-1, 1, data_length)\n",
    "        pt_data = torch.Tensor(np_data).to(device)\n",
    "\n",
    "        feature_extractor = nn.Sequential(\n",
    "            Spectrogram(\n",
    "                hop_length=hop_length,\n",
    "                win_length=win_length,\n",
    "            ), LogmelFilterBank(\n",
    "                sr=sample_rate,\n",
    "                n_mels=n_mels,\n",
    "                is_log=False, #Default is true\n",
    "            ))\n",
    "\n",
    "        feature_extractor.to(device)\n",
    "\n",
    "        print(\n",
    "            'Comparing default mel spectrogram from librosa to the pytorch implementation.'\n",
    "        )\n",
    "\n",
    "        # Numpy librosa\n",
    "        np_melspect = librosa.feature.melspectrogram(np_data,\n",
    "                                                     hop_length=hop_length,\n",
    "                                                     sr=sample_rate,\n",
    "                                                     win_length=win_length,\n",
    "                                                     n_mels=n_mels).T\n",
    "        #Pytorch\n",
    "        pt_melspect = feature_extractor(pt_data[None, :]).squeeze()\n",
    "        passed = np.allclose(pt_melspect.data.to('cpu').numpy(), np_melspect)\n",
    "        print(f\"Passed? {passed}\")\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "class DropStripes(nn.Module):\n",
    "    def __init__(self, dim, drop_width, stripes_num):\n",
    "        \"\"\"Drop stripes. \n",
    "\n",
    "        Args:\n",
    "          dim: int, dimension along which to drop\n",
    "          drop_width: int, maximum width of stripes to drop\n",
    "          stripes_num: int, how many stripes to drop\n",
    "        \"\"\"\n",
    "        super(DropStripes, self).__init__()\n",
    "\n",
    "        assert dim in [2, 3]    # dim 2: time; dim 3: frequency\n",
    "\n",
    "        self.dim = dim\n",
    "        self.drop_width = drop_width\n",
    "        self.stripes_num = stripes_num\n",
    "\n",
    "    def forward(self, input):\n",
    "        \"\"\"input: (batch_size, channels, time_steps, freq_bins)\"\"\"\n",
    "\n",
    "        assert input.ndimension() == 4\n",
    "\n",
    "        if self.training is False:\n",
    "            return input\n",
    "\n",
    "        else:\n",
    "            batch_size = input.shape[0]\n",
    "            total_width = input.shape[self.dim]\n",
    "\n",
    "            for n in range(batch_size):\n",
    "                self.transform_slice(input[n], total_width)\n",
    "\n",
    "            return input\n",
    "\n",
    "\n",
    "    def transform_slice(self, e, total_width):\n",
    "        \"\"\"e: (channels, time_steps, freq_bins)\"\"\"\n",
    "\n",
    "        for _ in range(self.stripes_num):\n",
    "            distance = torch.randint(low=0, high=self.drop_width, size=(1,))[0]\n",
    "            bgn = torch.randint(low=0, high=total_width - distance, size=(1,))[0]\n",
    "\n",
    "            if self.dim == 2:\n",
    "                e[:, bgn : bgn + distance, :] = 0\n",
    "            elif self.dim == 3:\n",
    "                e[:, :, bgn : bgn + distance] = 0\n",
    "\n",
    "\n",
    "class SpecAugmentation(nn.Module):\n",
    "    def __init__(self, time_drop_width, time_stripes_num, freq_drop_width, \n",
    "        freq_stripes_num):\n",
    "        \"\"\"Spec augmetation. \n",
    "        [ref] Park, D.S., Chan, W., Zhang, Y., Chiu, C.C., Zoph, B., Cubuk, E.D. \n",
    "        and Le, Q.V., 2019. Specaugment: A simple data augmentation method \n",
    "        for automatic speech recognition. arXiv preprint arXiv:1904.08779.\n",
    "\n",
    "        Args:\n",
    "          time_drop_width: int\n",
    "          time_stripes_num: int\n",
    "          freq_drop_width: int\n",
    "          freq_stripes_num: int\n",
    "        \"\"\"\n",
    "\n",
    "        super(SpecAugmentation, self).__init__()\n",
    "\n",
    "        self.time_dropper = DropStripes(dim=2, drop_width=time_drop_width, \n",
    "            stripes_num=time_stripes_num)\n",
    "\n",
    "        self.freq_dropper = DropStripes(dim=3, drop_width=freq_drop_width, \n",
    "            stripes_num=freq_stripes_num)\n",
    "\n",
    "    def forward(self, input):\n",
    "        x = self.time_dropper(input)\n",
    "        x = self.freq_dropper(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "23e7b30f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-24T20:56:11.607201Z",
     "iopub.status.busy": "2023-05-24T20:56:11.606797Z",
     "iopub.status.idle": "2023-05-24T20:56:11.829775Z",
     "shell.execute_reply": "2023-05-24T20:56:11.828453Z"
    },
    "papermill": {
     "duration": 0.234514,
     "end_time": "2023-05-24T20:56:11.833510",
     "exception": false,
     "start_time": "2023-05-24T20:56:11.598996",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(9, 5)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>filename</th>\n",
       "      <th>name</th>\n",
       "      <th>date</th>\n",
       "      <th>id</th>\n",
       "      <th>path</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>H02_20230429_112500</td>\n",
       "      <td>H02</td>\n",
       "      <td>20230429</td>\n",
       "      <td>112500</td>\n",
       "      <td>/root/projects/BirdClef2025/data/test_soundsca...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>H02_20230429_085000</td>\n",
       "      <td>H02</td>\n",
       "      <td>20230429</td>\n",
       "      <td>085000</td>\n",
       "      <td>/root/projects/BirdClef2025/data/test_soundsca...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>H02_20230429_092000</td>\n",
       "      <td>H02</td>\n",
       "      <td>20230429</td>\n",
       "      <td>092000</td>\n",
       "      <td>/root/projects/BirdClef2025/data/test_soundsca...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>H02_20230429_125500</td>\n",
       "      <td>H02</td>\n",
       "      <td>20230429</td>\n",
       "      <td>125500</td>\n",
       "      <td>/root/projects/BirdClef2025/data/test_soundsca...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>H02_20230429_031000</td>\n",
       "      <td>H02</td>\n",
       "      <td>20230429</td>\n",
       "      <td>031000</td>\n",
       "      <td>/root/projects/BirdClef2025/data/test_soundsca...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              filename name      date      id  \\\n",
       "0  H02_20230429_112500  H02  20230429  112500   \n",
       "1  H02_20230429_085000  H02  20230429  085000   \n",
       "2  H02_20230429_092000  H02  20230429  092000   \n",
       "3  H02_20230429_125500  H02  20230429  125500   \n",
       "4  H02_20230429_031000  H02  20230429  031000   \n",
       "\n",
       "                                                path  \n",
       "0  /root/projects/BirdClef2025/data/test_soundsca...  \n",
       "1  /root/projects/BirdClef2025/data/test_soundsca...  \n",
       "2  /root/projects/BirdClef2025/data/test_soundsca...  \n",
       "3  /root/projects/BirdClef2025/data/test_soundsca...  \n",
       "4  /root/projects/BirdClef2025/data/test_soundsca...  "
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train = pd.read_csv(CFG.train_path)\n",
    "CFG.num_classes = len(df_train.primary_label.unique())\n",
    "df_test = pd.DataFrame(\n",
    "     [(path.stem, *path.stem.split(\"_\"), path) for path in Path(CFG.test_path).glob(\"*.ogg\")],\n",
    "    columns = [\"filename\", \"name\" ,\"date\",\"id\", \"path\"]\n",
    ")\n",
    "print(df_test.shape)\n",
    "df_test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "443fb76d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-24T20:56:11.849408Z",
     "iopub.status.busy": "2023-05-24T20:56:11.848675Z",
     "iopub.status.idle": "2023-05-24T20:56:11.861738Z",
     "shell.execute_reply": "2023-05-24T20:56:11.860926Z"
    },
    "papermill": {
     "duration": 0.023366,
     "end_time": "2023-05-24T20:56:11.863861",
     "exception": false,
     "start_time": "2023-05-24T20:56:11.840495",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# preprocess\n",
    "def compute_melspec(y, sr, n_mels, fmin, fmax):\n",
    "    \"\"\"\n",
    "    Computes a mel-spectrogram and puts it at decibel scale\n",
    "    Arguments:\n",
    "        y {np array} -- signal\n",
    "        params {AudioParams} -- Parameters to use for the spectrogram. Expected to have the attributes sr, n_mels, f_min, f_max\n",
    "    Returns:\n",
    "        np array -- Mel-spectrogram\n",
    "    \"\"\"\n",
    "    melspec = lb.feature.melspectrogram(\n",
    "        y=y, sr=sr, n_mels=n_mels, fmin=fmin, fmax=fmax,\n",
    "        win_length=CFG.window_size,hop_length=CFG.hop_size,center=True,\n",
    "        n_fft=CFG.window_size,pad_mode='reflect',window='hann'\n",
    "    )\n",
    "\n",
    "    melspec = lb.power_to_db(melspec,amin=1e-10,ref=1.0,top_db=None).astype(np.float32)\n",
    "    return melspec\n",
    "\n",
    "def mono_to_color(X, eps=1e-6, mean=None, std=None):\n",
    "    mean = mean or X.mean()\n",
    "    std = std or X.std()\n",
    "    X = (X - mean) / (std + eps)\n",
    "    \n",
    "    _min, _max = X.min(), X.max()\n",
    "\n",
    "    if (_max - _min) > eps:\n",
    "        V = np.clip(X, _min, _max)\n",
    "        V = 255 * (V - _min) / (_max - _min)\n",
    "        V = V.astype(np.uint8)\n",
    "    else:\n",
    "        V = np.zeros_like(X, dtype=np.uint8)\n",
    "\n",
    "    return V\n",
    "\n",
    "def crop_or_pad(y, length, is_train=True, start=None):\n",
    "    if len(y) < length:\n",
    "        y = np.concatenate([y, np.zeros(length - len(y))])\n",
    "        \n",
    "        n_repeats = length // len(y)\n",
    "        epsilon = length % len(y)\n",
    "        \n",
    "        y = np.concatenate([y]*n_repeats + [y[:epsilon]])\n",
    "        \n",
    "    elif len(y) > length:\n",
    "        if not is_train:\n",
    "            start = start or 0\n",
    "        else:\n",
    "            start = start or np.random.randint(len(y) - length)\n",
    "\n",
    "        y = y[start:start + length]\n",
    "\n",
    "    return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "da688196",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-24T20:56:11.879530Z",
     "iopub.status.busy": "2023-05-24T20:56:11.879142Z",
     "iopub.status.idle": "2023-05-24T20:56:11.888393Z",
     "shell.execute_reply": "2023-05-24T20:56:11.887637Z"
    },
    "papermill": {
     "duration": 0.019368,
     "end_time": "2023-05-24T20:56:11.890427",
     "exception": false,
     "start_time": "2023-05-24T20:56:11.871059",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# dataset\n",
    "import torch\n",
    "import numpy as np\n",
    "from torch.utils.data import Dataset,DataLoader\n",
    "    \n",
    "\n",
    "\n",
    "class TestDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, \n",
    "                 df: pd.DataFrame, \n",
    "                 clip: np.ndarray,\n",
    "                 config=None,\n",
    "                 model=None\n",
    "                ):\n",
    "        \n",
    "        self.df = df\n",
    "        self.clip = clip\n",
    "        self.config = config\n",
    "        self.model = model\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx: int):\n",
    "\n",
    "        sample = self.df.loc[idx, :]\n",
    "        row_id = sample.row_id\n",
    "        \n",
    "        end_seconds = int(sample.seconds)\n",
    "        start_seconds = int(end_seconds - 5)\n",
    "        \n",
    "        if start_seconds != 0:\n",
    "            start_seconds = start_seconds - 2.5\n",
    "            end_seconds = end_seconds\n",
    "            y = self.clip[int(self.config.sample_rate * start_seconds) : int(self.config.sample_rate * end_seconds)].astype(np.float32)\n",
    "        else:\n",
    "            y = self.clip[int(self.config.sample_rate * start_seconds) : int(self.config.sample_rate * end_seconds)].astype(np.float32)\n",
    "            y = np.pad(y,(int(2.5*self.config.sample_rate),0),mode='constant',constant_values=0)\n",
    "        # y = self.clip[int(self.config.sample_rate * start_seconds) : int(self.config.sample_rate * end_seconds)].astype(np.float32)\n",
    "        image = self.model.get_mel_gram(torch.from_numpy(y).unsqueeze(0))\n",
    "        \n",
    "        return {\n",
    "            \"image\": image,\n",
    "            \"row_id\": row_id,\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "fbec8c69",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-24T20:56:11.906201Z",
     "iopub.status.busy": "2023-05-24T20:56:11.905821Z",
     "iopub.status.idle": "2023-05-24T20:56:12.030345Z",
     "shell.execute_reply": "2023-05-24T20:56:12.029335Z"
    },
    "papermill": {
     "duration": 0.13566,
     "end_time": "2023-05-24T20:56:12.032956",
     "exception": false,
     "start_time": "2023-05-24T20:56:11.897296",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(9, 5)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>filename</th>\n",
       "      <th>name</th>\n",
       "      <th>date</th>\n",
       "      <th>id</th>\n",
       "      <th>path</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>H02_20230429_112500</td>\n",
       "      <td>H02</td>\n",
       "      <td>20230429</td>\n",
       "      <td>112500</td>\n",
       "      <td>/root/projects/BirdClef2025/data/test_soundsca...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>H02_20230429_085000</td>\n",
       "      <td>H02</td>\n",
       "      <td>20230429</td>\n",
       "      <td>085000</td>\n",
       "      <td>/root/projects/BirdClef2025/data/test_soundsca...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>H02_20230429_092000</td>\n",
       "      <td>H02</td>\n",
       "      <td>20230429</td>\n",
       "      <td>092000</td>\n",
       "      <td>/root/projects/BirdClef2025/data/test_soundsca...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>H02_20230429_125500</td>\n",
       "      <td>H02</td>\n",
       "      <td>20230429</td>\n",
       "      <td>125500</td>\n",
       "      <td>/root/projects/BirdClef2025/data/test_soundsca...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>H02_20230429_031000</td>\n",
       "      <td>H02</td>\n",
       "      <td>20230429</td>\n",
       "      <td>031000</td>\n",
       "      <td>/root/projects/BirdClef2025/data/test_soundsca...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              filename name      date      id  \\\n",
       "0  H02_20230429_112500  H02  20230429  112500   \n",
       "1  H02_20230429_085000  H02  20230429  085000   \n",
       "2  H02_20230429_092000  H02  20230429  092000   \n",
       "3  H02_20230429_125500  H02  20230429  125500   \n",
       "4  H02_20230429_031000  H02  20230429  031000   \n",
       "\n",
       "                                                path  \n",
       "0  /root/projects/BirdClef2025/data/test_soundsca...  \n",
       "1  /root/projects/BirdClef2025/data/test_soundsca...  \n",
       "2  /root/projects/BirdClef2025/data/test_soundsca...  \n",
       "3  /root/projects/BirdClef2025/data/test_soundsca...  \n",
       "4  /root/projects/BirdClef2025/data/test_soundsca...  "
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train = pd.read_csv(CFG.train_path)\n",
    "CFG.num_classes = len(df_train.primary_label.unique())\n",
    "df_test = pd.DataFrame(\n",
    "     [(path.stem, *path.stem.split(\"_\"), path) for path in Path(CFG.test_path).glob(\"*.ogg\")],\n",
    "    columns = [\"filename\", \"name\" ,\"date\",\"id\", \"path\"]\n",
    ")\n",
    "print(df_test.shape)\n",
    "df_test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "3395abb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "low_numclasses_birds_index = [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 16, 17, 18, 20, 23, 24, 25, 26, 27, 28, 32, 33, 35, 37, 39, 40, 44, 46, 48, 49, 50, 51, 52, 53, 54, 56, 57, 58, 59, 127, 137, 156, 182]\n",
    "high_numclasses_birds_index = [12, 13, 14, 15, 19, 21, 22, 29, 30, 31, 34, 36, 38, 41, 42, 43, 45, 47, 55, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 128, 129, 130, 131, 132, 133, 134, 135, 136, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfaf6859",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Optional\n",
    "\n",
    "from torch import Tensor\n",
    "from torch.nn.modules.transformer import _get_activation_fn\n",
    "\n",
    "\n",
    "class TransformerDecoderLayerOptimal(nn.Module):\n",
    "    def __init__(self, d_model, nhead=8, dim_feedforward=2048, dropout=0.1, activation=\"relu\",\n",
    "                 layer_norm_eps=1e-5) -> None:\n",
    "        super(TransformerDecoderLayerOptimal, self).__init__()\n",
    "        self.norm1 = nn.LayerNorm(d_model, eps=layer_norm_eps)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.dropout1 = nn.Dropout(dropout)\n",
    "        self.dropout2 = nn.Dropout(dropout)\n",
    "        self.dropout3 = nn.Dropout(dropout)\n",
    "\n",
    "        self.self_attn = nn.MultiheadAttention(d_model, nhead, dropout=dropout)\n",
    "\n",
    "        # Implementation of Feedforward model\n",
    "        self.linear1 = nn.Linear(d_model, dim_feedforward)\n",
    "        self.linear2 = nn.Linear(dim_feedforward, d_model)\n",
    "\n",
    "        self.norm2 = nn.LayerNorm(d_model, eps=layer_norm_eps)\n",
    "        self.norm3 = nn.LayerNorm(d_model, eps=layer_norm_eps)\n",
    "\n",
    "        self.activation = _get_activation_fn(activation)\n",
    "\n",
    "    def __setstate__(self, state):\n",
    "        if 'activation' not in state:\n",
    "            state['activation'] = torch.nn.functional.relu\n",
    "        super(TransformerDecoderLayerOptimal, self).__setstate__(state)\n",
    "\n",
    "    def forward(self, tgt: Tensor, memory: Tensor, tgt_mask: Optional[Tensor] = None,\n",
    "                memory_mask: Optional[Tensor] = None, tgt_key_padding_mask: Optional[Tensor] = None,\n",
    "                memory_key_padding_mask: Optional[Tensor] = None, tgt_is_causal: Optional[bool] = None,\n",
    "                memory_is_causal: bool = False):\n",
    "        tgt = tgt + self.dropout1(tgt)\n",
    "        tgt = self.norm1(tgt)\n",
    "        tgt2 = self.self_attn(tgt, memory, memory)[0]\n",
    "        tgt = tgt + self.dropout2(tgt2)\n",
    "        tgt = self.norm2(tgt)\n",
    "        tgt2 = self.linear2(self.dropout(self.activation(self.linear1(tgt))))\n",
    "        tgt = tgt + self.dropout3(tgt2)\n",
    "        tgt = self.norm3(tgt)\n",
    "        return tgt\n",
    "\n",
    "\n",
    "# @torch.jit.script\n",
    "class GroupFC(object):\n",
    "    def __init__(self, embed_len_decoder: int):\n",
    "        self.embed_len_decoder = embed_len_decoder\n",
    "\n",
    "    def __call__(self, h: torch.Tensor, duplicate_pooling: torch.Tensor, out_extrap: torch.Tensor):\n",
    "        for i in range(h.shape[1]):\n",
    "            h_i = h[:, i, :]\n",
    "            if len(duplicate_pooling.shape) == 3:\n",
    "                w_i = duplicate_pooling[i, :, :]\n",
    "            else:\n",
    "                w_i = duplicate_pooling\n",
    "            out_extrap[:, i, :] = torch.matmul(h_i, w_i)\n",
    "\n",
    "\n",
    "class MLDecoder(nn.Module):\n",
    "    def __init__(self, num_classes, num_of_groups=-1, decoder_embedding=768,\n",
    "                 initial_num_features=2048, zsl=0):\n",
    "        super(MLDecoder, self).__init__()\n",
    "        embed_len_decoder = 206 if num_of_groups < 0 else num_of_groups\n",
    "        if embed_len_decoder > num_classes:\n",
    "            embed_len_decoder = num_classes\n",
    "\n",
    "        # switching to 768 initial embeddings\n",
    "        decoder_embedding = 768 if decoder_embedding < 0 else decoder_embedding\n",
    "        embed_standart = nn.Linear(initial_num_features, decoder_embedding)\n",
    "\n",
    "        # non-learnable queries\n",
    "        if not zsl:\n",
    "            query_embed = nn.Embedding(embed_len_decoder, decoder_embedding)\n",
    "            query_embed.requires_grad_(False)\n",
    "        else:\n",
    "            query_embed = None\n",
    "\n",
    "        # decoder\n",
    "        decoder_dropout = 0.1\n",
    "        num_layers_decoder = 1\n",
    "        dim_feedforward = 2048\n",
    "        layer_decode = TransformerDecoderLayerOptimal(d_model=decoder_embedding,\n",
    "                                                      dim_feedforward=dim_feedforward, dropout=decoder_dropout)\n",
    "        self.decoder = nn.TransformerDecoder(layer_decode, num_layers=num_layers_decoder)\n",
    "        self.decoder.embed_standart = embed_standart\n",
    "        self.decoder.query_embed = query_embed\n",
    "        self.zsl = zsl\n",
    "\n",
    "        if self.zsl:\n",
    "            if decoder_embedding != 300:\n",
    "                self.wordvec_proj = nn.Linear(300, decoder_embedding)\n",
    "            else:\n",
    "                self.wordvec_proj = nn.Identity()\n",
    "            self.decoder.duplicate_pooling = torch.nn.Parameter(torch.Tensor(decoder_embedding, 1))\n",
    "            self.decoder.duplicate_pooling_bias = torch.nn.Parameter(torch.Tensor(1))\n",
    "            self.decoder.duplicate_factor = 1\n",
    "        else:\n",
    "            # group fully-connected\n",
    "            self.decoder.num_classes = num_classes\n",
    "            self.decoder.duplicate_factor = int(num_classes / embed_len_decoder + 0.999)\n",
    "            self.decoder.duplicate_pooling = torch.nn.Parameter(\n",
    "                torch.Tensor(embed_len_decoder, decoder_embedding, self.decoder.duplicate_factor))\n",
    "            self.decoder.duplicate_pooling_bias = torch.nn.Parameter(torch.Tensor(num_classes))\n",
    "        torch.nn.init.xavier_normal_(self.decoder.duplicate_pooling)\n",
    "        torch.nn.init.constant_(self.decoder.duplicate_pooling_bias, 0)\n",
    "        self.decoder.group_fc = GroupFC(embed_len_decoder)\n",
    "        self.train_wordvecs = None\n",
    "        self.test_wordvecs = None\n",
    "\n",
    "    def forward(self, x, le=False):  # label embedding\n",
    "        if len(x.shape) == 4:  # [bs,2048,7,7]\n",
    "            embedding_spatial = x.flatten(2).transpose(1, 2)\n",
    "        else:  # [bs,2048,49]\n",
    "            embedding_spatial = x.transpose(1, 2)\n",
    "        embedding_spatial_786 = self.decoder.embed_standart(embedding_spatial)\n",
    "        embedding_spatial_786 = torch.nn.functional.relu(embedding_spatial_786, inplace=True)\n",
    "\n",
    "        bs = embedding_spatial_786.shape[0]\n",
    "        if self.zsl:\n",
    "            query_embed = torch.nn.functional.relu(self.wordvec_proj(self.decoder.query_embed))\n",
    "        else:\n",
    "            query_embed = self.decoder.query_embed.weight\n",
    "        # tgt = query_embed.unsqueeze(1).repeat(1, bs, 1)\n",
    "        tgt = query_embed.unsqueeze(1).expand(-1, bs, -1)  # no allocation of memory with expand\n",
    "        h = self.decoder(tgt, embedding_spatial_786.transpose(0, 1))  # [embed_len_decoder, batch, 768]\n",
    "        h = h.transpose(0, 1)\n",
    "\n",
    "        out_extrap = torch.zeros(h.shape[0], h.shape[1], self.decoder.duplicate_factor, device=h.device, dtype=h.dtype)\n",
    "        self.decoder.group_fc(h, self.decoder.duplicate_pooling, out_extrap)\n",
    "        if not self.zsl:\n",
    "            h_out = out_extrap.flatten(1)[:, :self.decoder.num_classes]\n",
    "        else:\n",
    "            h_out = out_extrap.flatten(1)\n",
    "        h_out += self.decoder.duplicate_pooling_bias\n",
    "        logits = h_out\n",
    "\n",
    "        if not le:\n",
    "            return logits\n",
    "        else:\n",
    "            return h, logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "affd84d6",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-24T20:56:12.051778Z",
     "iopub.status.busy": "2023-05-24T20:56:12.051399Z",
     "iopub.status.idle": "2023-05-24T20:56:17.939889Z",
     "shell.execute_reply": "2023-05-24T20:56:17.939025Z"
    },
    "papermill": {
     "duration": 5.90208,
     "end_time": "2023-05-24T20:56:17.942465",
     "exception": false,
     "start_time": "2023-05-24T20:56:12.040385",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# model\n",
    "import timm\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "from torchaudio.transforms import MelSpectrogram,AmplitudeToDB\n",
    "\n",
    "class AttBlockV2(nn.Module):\n",
    "    def __init__(self,\n",
    "                 in_features: int,\n",
    "                 out_features: int,\n",
    "                 activation=\"linear\"):\n",
    "        super().__init__()\n",
    "\n",
    "        self.activation = activation\n",
    "        self.att = nn.Conv1d(\n",
    "            in_channels=in_features,\n",
    "            out_channels=out_features,\n",
    "            kernel_size=1,\n",
    "            stride=1,\n",
    "            padding=0,\n",
    "            bias=True)\n",
    "        self.cla = nn.Conv1d(\n",
    "            in_channels=in_features,\n",
    "            out_channels=out_features,\n",
    "            kernel_size=1,\n",
    "            stride=1,\n",
    "            padding=0,\n",
    "            bias=True)\n",
    "\n",
    "        self.init_weights()\n",
    "\n",
    "    def init_weights(self):\n",
    "        init_layer(self.att)\n",
    "        init_layer(self.cla)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x: (n_samples, n_in, n_time)\n",
    "        norm_att = torch.softmax(torch.tanh(self.att(x)), dim=-1)\n",
    "        cla = self.nonlinear_transform(self.cla(x))\n",
    "        x = torch.sum(norm_att * cla, dim=2)\n",
    "        return x, norm_att, cla\n",
    "\n",
    "    def nonlinear_transform(self, x):\n",
    "        if self.activation == 'linear':\n",
    "            return x\n",
    "        elif self.activation == 'sigmoid':\n",
    "            return torch.sigmoid(x)\n",
    "\n",
    "class NormalizeMelSpec(nn.Module):\n",
    "    def __init__(self, eps=1e-6):\n",
    "        super().__init__()\n",
    "        self.eps = eps\n",
    "\n",
    "    def forward(self, X):\n",
    "        mean = X.mean((1, 2), keepdim=True)\n",
    "        std = X.std((1, 2), keepdim=True)\n",
    "        Xstd = (X - mean) / (std + self.eps)\n",
    "        norm_min, norm_max = Xstd.min(-1)[0].min(-1)[0], Xstd.max(-1)[0].max(-1)[0]\n",
    "        fix_ind = (norm_max - norm_min) > self.eps * torch.ones_like(\n",
    "            (norm_max - norm_min)\n",
    "        )\n",
    "        V = torch.zeros_like(Xstd)\n",
    "        if fix_ind.sum():\n",
    "            V_fix = Xstd[fix_ind]\n",
    "            norm_max_fix = norm_max[fix_ind, None, None]\n",
    "            norm_min_fix = norm_min[fix_ind, None, None]\n",
    "            V_fix = torch.max(\n",
    "                torch.min(V_fix, norm_max_fix),\n",
    "                norm_min_fix,\n",
    "            )\n",
    "            # print(V_fix.shape, norm_min_fix.shape, norm_max_fix.shape)\n",
    "            V_fix = (V_fix - norm_min_fix) / (norm_max_fix - norm_min_fix)\n",
    "            V[fix_ind] = V_fix\n",
    "        return V\n",
    "\n",
    "def gem_freq(x, p=3, eps=1e-6):\n",
    "    return F.avg_pool2d(x.clamp(min=eps).pow(p), (x.size(-2), 1)).pow(1.0 / p)\n",
    "\n",
    "\n",
    "class GeMFreq(nn.Module):\n",
    "    def __init__(self, p=3, eps=1e-6):\n",
    "        super().__init__()\n",
    "        self.p = torch.nn.Parameter(torch.ones(1) * p)\n",
    "        self.eps = eps\n",
    "\n",
    "    def forward(self, x):\n",
    "        return gem_freq(x, p=self.p, eps=self.eps)\n",
    "\n",
    "def init_layer(layer):\n",
    "    \"\"\"Initialize a Linear or Convolutional layer. \"\"\"\n",
    "    nn.init.xavier_uniform_(layer.weight)\n",
    " \n",
    "    if hasattr(layer, 'bias'):\n",
    "        if layer.bias is not None:\n",
    "            layer.bias.data.fill_(0.)\n",
    "            \n",
    "    \n",
    "def init_bn(bn):\n",
    "    \"\"\"Initialize a Batchnorm layer. \"\"\"\n",
    "    bn.bias.data.fill_(0.)\n",
    "    bn.weight.data.fill_(1.)\n",
    "\n",
    "\n",
    "def gem(x, p=3, eps=1e-6):\n",
    "    return nn.AdaptiveAvgPool2d(1)(x.clamp(min=eps).pow(p)).pow(1./p)\n",
    "    # return F.avg_pool2d(x.clamp(min=eps).pow(p), (x.size(-2), x.size(-1))).pow(1./p)\n",
    "\n",
    "class GeM(nn.Module):\n",
    "    def __init__(self, p=3, eps=1e-6, p_trainable=False):\n",
    "        super(GeM,self).__init__()\n",
    "        if p_trainable:\n",
    "            self.p = nn.Parameter(torch.ones(1)*p)\n",
    "        else:\n",
    "            self.p = p\n",
    "        self.eps = eps\n",
    "\n",
    "    def forward(self, x):\n",
    "        ret = gem(x, p=self.p, eps=self.eps)   \n",
    "        ret = torch.flatten(ret,start_dim=1)\n",
    "        return ret\n",
    "\n",
    "\n",
    "class BirdClefSEDModel(nn.Module):\n",
    "    def __init__(self, model_name=CFG.model, num_classes = CFG.num_classes, pretrained = CFG.pretrained,p=0.5):\n",
    "        super().__init__()\n",
    "        self.num_classes = num_classes\n",
    "        self.backbone = timm.create_model(model_name, pretrained=pretrained)\n",
    "        if 'effi' in CFG.model:\n",
    "            self.backbone.global_pool = nn.Identity()\n",
    "            self.backbone.classifier = nn.Identity()\n",
    "        elif 'eca' in CFG.model:\n",
    "            self.backbone.head.fc = nn.Identity()\n",
    "        if CFG.use_fsr:\n",
    "            self.backbone.conv_stem.stride = (1,1)\n",
    "        self.fc_audioset = nn.Linear(self.backbone.num_features, num_classes, bias=True)\n",
    "        self.pooling = GeM()\n",
    "        self.SpecAug = SpecAugmentation(time_drop_width=64, time_stripes_num=2,freq_drop_width=8, freq_stripes_num=2)\n",
    "        self.use_spec_aug = CFG.use_spec_aug\n",
    "        self.bn0 = nn.BatchNorm2d(CFG.mel_bins)\n",
    "    \n",
    "        # Spectrogram extractor\n",
    "        self.spectrogram_extractor = MelSpectrogram(\n",
    "            sample_rate=CFG.sample_rate,\n",
    "            n_fft=2048,\n",
    "            win_length=CFG.window_size,\n",
    "            hop_length=CFG.hop_size,\n",
    "            f_min=CFG.fmin,\n",
    "            f_max=CFG.fmax,\n",
    "            pad=0,\n",
    "            n_mels=CFG.mel_bins,\n",
    "            power=2,\n",
    "            normalized=False,\n",
    "        )\n",
    "        # Logmel feature extractor\n",
    "        self.logmel_extractor = AmplitudeToDB(top_db=None)\n",
    "        self.normlize = NormalizeMelSpec()\n",
    "        self.infer_period = CFG.infer_duration\n",
    "        self.train_period = CFG.duration\n",
    "        self.init_weight()\n",
    "\n",
    "    def init_weight(self):\n",
    "        init_bn(self.bn0)\n",
    "\n",
    "        \n",
    "    def get_mel_gram(self,audios):\n",
    "        \"\"\"\n",
    "        Input: (batch_size, data_length)\"\"\"\n",
    "        x = self.spectrogram_extractor(audios) # (batch_size,freq_bins time_steps)\n",
    "        x = self.logmel_extractor(x) \n",
    "        x = self.normlize(x)\n",
    "        x = x.permute(0,2,1)# (batch_size,time_steps, mel_bins)\n",
    "        return x\n",
    "    \n",
    "    def forward(self,images):\n",
    "        # b c f t\n",
    "        if CFG.use_spec_aug and self.training:\n",
    "            if np.random.uniform(0,1)>CFG.p_spec_aug:\n",
    "                images = self.SpecAug(images)\n",
    "        x = self.backbone.forward_features(images) #  bs,1,t,f\n",
    "        x = torch.mean(x,dim=3) # pooling freq bs,c,t\n",
    "        \n",
    "        (x1,_) = torch.max(x,dim=2) # bs,c\n",
    "        x2 = torch.mean(x,dim=2) # bs,c\n",
    "        x = x1+x2\n",
    "        x = F.dropout(x,p=0.5,training=self.training)\n",
    "        x = self.fc_audioset(x)\n",
    "\n",
    "        return x\n",
    "    \n",
    "class BirdClefModel(nn.Module):\n",
    "    def __init__(self, model_name=CFG.model, num_classes = CFG.num_classes, pretrained = CFG.pretrained,p=0.5):\n",
    "        super().__init__()\n",
    "        self.num_classes = num_classes\n",
    "        self.backbone = timm.create_model(model_name, pretrained=pretrained)\n",
    "        if 'effi' in CFG.model:\n",
    "            self.backbone.global_pool = nn.Identity()\n",
    "            self.backbone.classifier = nn.Identity()\n",
    "        elif 'eca' in CFG.model:\n",
    "            self.backbone.head.fc = nn.Identity()\n",
    "        if CFG.use_fsr:\n",
    "            self.backbone.conv_stem.stride = (1,1)\n",
    "        self.fc_audioset = nn.Linear(self.backbone.num_features, num_classes, bias=True)\n",
    "        self.pooling = GeM()\n",
    "        self.SpecAug = SpecAugmentation(time_drop_width=64, time_stripes_num=2,freq_drop_width=8, freq_stripes_num=2)\n",
    "        self.use_spec_aug = CFG.use_spec_aug\n",
    "        self.bn0 = nn.BatchNorm2d(CFG.mel_bins)\n",
    "        \n",
    "        # Spectrogram extractor\n",
    "        self.spectrogram_extractor = MelSpectrogram(\n",
    "            sample_rate=CFG.sample_rate,\n",
    "            n_fft=2048,\n",
    "            win_length=CFG.window_size,\n",
    "            hop_length=CFG.hop_size,\n",
    "            f_min=CFG.fmin,\n",
    "            f_max=CFG.fmax,\n",
    "            pad=0,\n",
    "            n_mels=CFG.mel_bins,\n",
    "            power=2,\n",
    "            normalized=False,\n",
    "        )\n",
    "        # Logmel feature extractor\n",
    "        self.logmel_extractor = AmplitudeToDB(top_db=None)\n",
    "        self.normlize = NormalizeMelSpec()\n",
    "        self.infer_period = CFG.infer_duration\n",
    "        self.train_period = CFG.duration\n",
    "        self.init_weight()\n",
    "\n",
    "    def init_weight(self):\n",
    "        init_bn(self.bn0)\n",
    "\n",
    "        \n",
    "    def get_mel_gram(self,audios):\n",
    "        \"\"\"\n",
    "        Input: (batch_size, data_length)\"\"\"\n",
    "        x = self.spectrogram_extractor(audios) # (batch_size,freq_bins time_steps)\n",
    "        x = self.logmel_extractor(x) \n",
    "        x = self.normlize(x)\n",
    "        x = x.permute(0,2,1)# (batch_size,time_steps, mel_bins)\n",
    "        return x\n",
    "    \n",
    "    def forward(self,images):\n",
    "        # b c f t\n",
    "        if CFG.use_spec_aug and self.training:\n",
    "            if np.random.uniform(0,1)>CFG.p_spec_aug:\n",
    "                images = self.SpecAug(images)\n",
    "        if self.training:\n",
    "            if images.shape[2]%4!=0:\n",
    "                images = F.pad(images,(0,0,4-images.shape[2]%4,0))\n",
    "            images = torch.cat(torch.chunk(images,chunks=4,dim=2),dim=0) #  4*bs,1,t,f\n",
    "        x = self.backbone.forward_features(images) #  4*bs,1,t,f\n",
    "\n",
    "        if self.training:\n",
    "            x = torch.cat(torch.chunk(x,chunks=4,dim=0),dim=2)\n",
    "        x = self.pooling(x)\n",
    "        x = self.fc_audioset(x)\n",
    "        return x\n",
    "    \n",
    "class BirdClefSEDAttModel(nn.Module):\n",
    "    def __init__(self, model_name=CFG.model, num_classes = CFG.num_classes, pretrained = CFG.pretrained,p=0.5):\n",
    "        super().__init__()\n",
    "        self.num_classes = num_classes\n",
    "        self.backbone = timm.create_model(model_name, pretrained=pretrained)\n",
    "        if 'effi' in model_name:\n",
    "            self.backbone.global_pool = nn.Identity()\n",
    "            self.backbone.classifier = nn.Identity()\n",
    "        elif 'eca' in model_name:\n",
    "            self.backbone.head.fc = nn.Identity()\n",
    "        if CFG.use_fsr:\n",
    "            self.backbone.conv_stem.stride = (1,1)\n",
    "        self.pooling = GeM()\n",
    "        self.SpecAug = SpecAugmentation(time_drop_width=64, time_stripes_num=2,freq_drop_width=8, freq_stripes_num=2)\n",
    "        self.use_spec_aug = CFG.use_spec_aug\n",
    "        self.bn0 = nn.BatchNorm2d(CFG.mel_bins)\n",
    "    \n",
    "        # Spectrogram extractor\n",
    "        self.spectrogram_extractor = MelSpectrogram(\n",
    "            sample_rate=CFG.sample_rate,\n",
    "            n_fft=CFG.n_fft,\n",
    "            win_length=CFG.window_size,\n",
    "            hop_length=CFG.hop_size,\n",
    "            f_min=CFG.fmin,\n",
    "            f_max=CFG.fmax,\n",
    "            pad=0,\n",
    "            n_mels=CFG.mel_bins,\n",
    "            power=2,\n",
    "            normalized=False,\n",
    "        )\n",
    "        # Logmel feature extractor\n",
    "        self.logmel_extractor = AmplitudeToDB(top_db=None)\n",
    "        self.normlize = NormalizeMelSpec()\n",
    "\n",
    "        self.fc1 = nn.Linear(self.backbone.num_features, self.backbone.num_features, bias=True)\n",
    "        self.att_block = AttBlockV2(\n",
    "            self.backbone.num_features, num_classes, activation=\"sigmoid\")\n",
    "\n",
    "        self.init_weight()\n",
    "\n",
    "    def init_weight(self):\n",
    "        init_bn(self.bn0)\n",
    "        init_layer(self.fc1)\n",
    "    \n",
    "    def get_mel_gram(self,audios):\n",
    "        \"\"\"\n",
    "        Input: (batch_size, data_length)\"\"\"\n",
    "        x = self.spectrogram_extractor(audios) # (batch_size,freq_bins time_steps)\n",
    "        x = self.logmel_extractor(x) \n",
    "        x = self.normlize(x)\n",
    "        x = x.permute(0,2,1)# (batch_size,time_steps, mel_bins)\n",
    "        return x\n",
    "\n",
    "    def forward(self, images):\n",
    "        # b c f t\n",
    "        if CFG.use_spec_aug and self.training:\n",
    "            if np.random.uniform(0,1)>CFG.p_spec_aug:\n",
    "                images = self.SpecAug(images)\n",
    "\n",
    "        x = images.transpose(1, 3)\n",
    "        x = self.bn0(x)\n",
    "        x = x.transpose(1, 3)\n",
    "\n",
    "        x = x.transpose(2, 3)\n",
    "\n",
    "        x = self.backbone.forward_features(x)\n",
    "        \n",
    "        # Aggregate in frequency axis\n",
    "        x = torch.mean(x, dim=2)\n",
    "\n",
    "        x1 = F.max_pool1d(x, kernel_size=3, stride=1, padding=1)\n",
    "        x2 = F.avg_pool1d(x, kernel_size=3, stride=1, padding=1)\n",
    "        x = x1 + x2\n",
    "\n",
    "        x = F.dropout(x, p=0.3, training=self.training)\n",
    "        x = x.transpose(1, 2)\n",
    "        x = F.relu(self.fc1(x),inplace=False)\n",
    "        x = x.transpose(1, 2)\n",
    "        x = F.dropout(x, p=0.3, training=self.training)\n",
    "\n",
    "        (clipwise_output, norm_att, segmentwise_output) = self.att_block(x)\n",
    "\n",
    "        maxframewise_output = nn.AdaptiveMaxPool1d(1)(segmentwise_output).squeeze(2)\n",
    "        output_dict = {\n",
    "            \"clipwise_output\": clipwise_output,\n",
    "            \"framewise_output\":segmentwise_output,\n",
    "            \"maxframewise_output\":maxframewise_output\n",
    "        }\n",
    "\n",
    "        return output_dict\n",
    "    \n",
    "class BirdClefSEDAttModelSplit(BirdClefSEDAttModel):\n",
    "    def __init__(self, split1,split2,model_name=CFG.model,  pretrained = CFG.pretrained,p=0.5,device='cuda'):\n",
    "        super().__init__(model_name=model_name, num_classes = CFG.num_classes, pretrained = CFG.pretrained,p=0.5)\n",
    "        self.att_block_split1 = AttBlockV2(self.backbone.num_features, len(split1), activation=\"sigmoid\")\n",
    "        self.att_block_split2 = AttBlockV2(self.backbone.num_features, len(split2), activation=\"sigmoid\")\n",
    "        self.split1 = torch.tensor(split1).to(device)\n",
    "        self.split2 = torch.tensor(split2).to(device)\n",
    "\n",
    "    def forward(self, x):\n",
    "        bs = x.shape[0]\n",
    "        x = x.contiguous()\n",
    "        # b c f t\n",
    "        if CFG.use_spec_aug and self.training:\n",
    "            if np.random.uniform(0,1)>CFG.p_spec_aug:\n",
    "                x = self.SpecAug(x)\n",
    "\n",
    "        x = x.transpose(1, 3)\n",
    "        x = self.bn0(x)\n",
    "        x = x.transpose(1, 3)\n",
    "\n",
    "        x = x.transpose(2, 3)\n",
    "\n",
    "        x = self.backbone.forward_features(x)\n",
    "        \n",
    "        # Aggregate in frequency axis\n",
    "        x = torch.mean(x, dim=2)\n",
    "\n",
    "        x1 = F.max_pool1d(x, kernel_size=3, stride=1, padding=1)\n",
    "        x2 = F.avg_pool1d(x, kernel_size=3, stride=1, padding=1)\n",
    "        x = x1 + x2\n",
    "\n",
    "        x = F.dropout(x, p=0.3, training=self.training)\n",
    "        x = x.transpose(1, 2)\n",
    "        x = F.relu(self.fc1(x),inplace=False)\n",
    "        x = x.transpose(1, 2)\n",
    "        x = F.dropout(x, p=0.3, training=self.training)\n",
    "\n",
    "        (clipwise_output1, norm_att1, segmentwise_output1) = self.att_block_split1(x)\n",
    "\n",
    "        maxframewise_output1 = nn.AdaptiveMaxPool1d(1)(segmentwise_output1).squeeze(2)\n",
    "\n",
    "        (clipwise_output2, norm_att2, segmentwise_output2) = self.att_block_split2(x)\n",
    "\n",
    "        maxframewise_output2 = nn.AdaptiveMaxPool1d(1)(segmentwise_output2).squeeze(2)\n",
    "        # maxframewise_output = nn.AdaptiveAvgPool1d(1)(segmentwise_output).squeeze(2)\n",
    "        output_dict1 = {\n",
    "            \"clipwise_output\": clipwise_output1,\n",
    "            \"framewise_output\":segmentwise_output1,\n",
    "            \"maxframewise_output\":maxframewise_output1\n",
    "        }\n",
    "        output_dict2 = {\n",
    "            \"clipwise_output\": clipwise_output2,\n",
    "            \"framewise_output\":segmentwise_output2,\n",
    "            \"maxframewise_output\":maxframewise_output2\n",
    "        }\n",
    "\n",
    "        return output_dict1,output_dict2\n",
    "    \n",
    "class SplitModel(BirdClefSEDAttModel):\n",
    "    def __init__(self, split_index,model_name=CFG.model, num_classes=CFG.num_classes, pretrained=CFG.pretrained, p=0.5,device='cpu'):\n",
    "        super().__init__(model_name, num_classes, pretrained, p)\n",
    "        self.split_index = torch.tensor(split_index).to(device)\n",
    "        self.att_block_split = AttBlockV2(self.backbone.num_features, len(split_index), activation=\"sigmoid\")\n",
    "    \n",
    "    def forward(self, images):\n",
    "        # b c f t\n",
    "        if CFG.use_spec_aug and self.training:\n",
    "            if np.random.uniform(0,1)>CFG.p_spec_aug:\n",
    "                images = self.SpecAug(images)\n",
    "\n",
    "        x = images.transpose(1, 3)\n",
    "        x = self.bn0(x)\n",
    "        x = x.transpose(1, 3)\n",
    "\n",
    "        x = x.transpose(2, 3)\n",
    "\n",
    "        x = self.backbone.forward_features(x)\n",
    "        \n",
    "        # Aggregate in frequency axis\n",
    "        x = torch.mean(x, dim=2)\n",
    "\n",
    "        x1 = F.max_pool1d(x, kernel_size=3, stride=1, padding=1)\n",
    "        x2 = F.avg_pool1d(x, kernel_size=3, stride=1, padding=1)\n",
    "        x = x1 + x2\n",
    "\n",
    "        x = F.dropout(x, p=0.3, training=self.training)\n",
    "        x = x.transpose(1, 2)\n",
    "        x = F.relu(self.fc1(x),inplace=False)\n",
    "        x = x.transpose(1, 2)\n",
    "        x = F.dropout(x, p=0.3, training=self.training)\n",
    "\n",
    "        (clipwise_output, norm_att, segmentwise_output) = self.att_block(x)\n",
    "        (clipwise_output_split, norm_att_split, segmentwise_output_split) = self.att_block_split(x)\n",
    "\n",
    "        maxframewise_output = nn.AdaptiveMaxPool1d(1)(segmentwise_output).squeeze(2)\n",
    "        maxframewise_output_split = nn.AdaptiveMaxPool1d(1)(segmentwise_output_split).squeeze(2)\n",
    "\n",
    "        clipwise_output[:,self.split_index] = clipwise_output_split\n",
    "        maxframewise_output[:,self.split_index] = maxframewise_output_split\n",
    "\n",
    "        output_dict = {\n",
    "            \"clipwise_output\": clipwise_output,\n",
    "            \"maxframewise_output\":maxframewise_output\n",
    "        }\n",
    "\n",
    "        return output_dict\n",
    "\n",
    "class PowerToDB(torch.nn.Module):\n",
    "    \"\"\"\n",
    "    A power spectrogram to decibel conversion layer. See birdset.datamodule.components.augmentations\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, ref=1.0, amin=1e-10, top_db=80.0):\n",
    "        super(PowerToDB, self).__init__()\n",
    "        # Initialize parameters\n",
    "        self.ref = ref\n",
    "        self.amin = amin\n",
    "        self.top_db = top_db\n",
    "\n",
    "    def forward(self, S):\n",
    "        # Convert S to a PyTorch tensor if it is not already\n",
    "        S = torch.as_tensor(S, dtype=torch.float32)\n",
    "\n",
    "        if self.amin <= 0:\n",
    "            raise ValueError(\"amin must be strictly positive\")\n",
    "\n",
    "        if torch.is_complex(S):\n",
    "            magnitude = S.abs()\n",
    "        else:\n",
    "            magnitude = S\n",
    "\n",
    "        # Check if ref is a callable function or a scalar\n",
    "        if callable(self.ref):\n",
    "            ref_value = self.ref(magnitude)\n",
    "        else:\n",
    "            ref_value = torch.abs(torch.tensor(self.ref, dtype=S.dtype))\n",
    "\n",
    "        # Compute the log spectrogram\n",
    "        log_spec = 10.0 * torch.log10(\n",
    "            torch.maximum(magnitude, torch.tensor(self.amin, device=magnitude.device))\n",
    "        )\n",
    "        log_spec -= 10.0 * torch.log10(\n",
    "            torch.maximum(ref_value, torch.tensor(self.amin, device=magnitude.device))\n",
    "        )\n",
    "\n",
    "        # Apply top_db threshold if necessary\n",
    "        if self.top_db is not None:\n",
    "            if self.top_db < 0:\n",
    "                raise ValueError(\"top_db must be non-negative\")\n",
    "            log_spec = torch.maximum(log_spec, log_spec.max() - self.top_db)\n",
    "\n",
    "        return log_spec\n",
    "\n",
    "class BirdSetSEDModel(nn.Module):\n",
    "    def __init__(self, num_classes = CFG.num_classes):\n",
    "        super().__init__()\n",
    "        config = AutoConfig.from_pretrained('/root/projects/BirdClef2025/data_birdset/models--DBD-research-group--EfficientNet-B1-BirdSet-XCL/snapshots/69ce718f7e5750dc7885d9356d6c8c470114dfdf/config.json')\n",
    "        config.num_channels = 3\n",
    "        self.backbone = EfficientNetForImageClassification(config)\n",
    "        self.powerToDB = PowerToDB()\n",
    "        self.fc1 = nn.Linear(1280, 1280, bias=True)\n",
    "        self.att_block = AttBlockV2(1280, num_classes, activation=\"sigmoid\")\n",
    "        self.bn0 = nn.BatchNorm2d(256)\n",
    "        self.aud2mel = torchaudio.transforms.Spectrogram(\n",
    "            n_fft=2048, hop_length=256, power=2.0\n",
    "        )\n",
    "        self.melscaled = torchaudio.transforms.MelScale(n_mels=256, n_stft=1025)\n",
    "        self.normlizer = transforms.Normalize((-4.268,), (4.569,))\n",
    "        self.SpecAug = SpecAugmentation(time_drop_width=64, time_stripes_num=2,freq_drop_width=8, freq_stripes_num=2)\n",
    "\n",
    "        init_bn(self.bn0)\n",
    "        # Resample to 32kHz\n",
    "\n",
    "    def get_mel_gram(self,audios):\n",
    "        spectrogram = self.aud2mel(audios)\n",
    "        melspec = self.melscaled(spectrogram)\n",
    "        dbscale = self.powerToDB(melspec)\n",
    "        normalized_dbscale = self.normlizer(dbscale)\n",
    "        return normalized_dbscale.permute(0,2,1)\n",
    "\n",
    "    def forward(self,x):\n",
    "\n",
    "        x = x.contiguous()\n",
    "        # b c f t\n",
    "        if CFG.use_spec_aug and self.training:\n",
    "            if np.random.uniform(0,1)>CFG.p_spec_aug:\n",
    "                x = self.SpecAug(x)\n",
    "\n",
    "        x = x.transpose(1, 3)\n",
    "        x = self.bn0(x)\n",
    "        x = x.transpose(1, 3)\n",
    "\n",
    "        x = x.transpose(2, 3)\n",
    "\n",
    "        x = self.backbone.efficientnet(x).last_hidden_state\n",
    "        \n",
    "        # Aggregate in frequency axis\n",
    "        x = torch.mean(x, dim=2)\n",
    "\n",
    "        x1 = F.max_pool1d(x, kernel_size=3, stride=1, padding=1)\n",
    "        x2 = F.avg_pool1d(x, kernel_size=3, stride=1, padding=1)\n",
    "        x = x1 + x2\n",
    "\n",
    "        x = F.dropout(x, p=0.3, training=self.training)\n",
    "        x = x.transpose(1, 2)\n",
    "        x = F.relu(self.fc1(x),inplace=False)\n",
    "        x = x.transpose(1, 2)\n",
    "        x = F.dropout(x, p=0.3, training=self.training)\n",
    "        \n",
    "        (clipwise_output, norm_att, segmentwise_output) = self.att_block(x)\n",
    "\n",
    "        maxframewise_output = nn.AdaptiveMaxPool1d(1)(segmentwise_output).squeeze(2)\n",
    "        # maxframewise_output = nn.AdaptiveAvgPool1d(1)(segmentwise_output).squeeze(2)\n",
    "\n",
    "        output_dict = {\n",
    "            \"clipwise_output\": clipwise_output,\n",
    "            \"framewise_output\":segmentwise_output,\n",
    "            \"maxframewise_output\":maxframewise_output\n",
    "        }\n",
    "\n",
    "        return output_dict\n",
    "\n",
    "class BirdClefCNNModel(nn.Module):\n",
    "    def __init__(self, model_name=CFG.model, num_classes = CFG.num_classes, pretrained = CFG.pretrained,p=0.5):\n",
    "        super().__init__()\n",
    "        self.num_classes = num_classes\n",
    "        self.backbone = timm.create_model(model_name, pretrained=pretrained)\n",
    "        if 'effi' in model_name:\n",
    "            self.backbone.global_pool = nn.Identity()\n",
    "            self.backbone.classifier = nn.Identity()\n",
    "        elif 'eca' in model_name:\n",
    "            self.backbone.head.fc = nn.Identity()\n",
    "        if CFG.use_fsr:\n",
    "            self.backbone.conv_stem.stride = (1,1)\n",
    "        self.pooling = GeM()\n",
    "        self.SpecAug = SpecAugmentation(time_drop_width=64, time_stripes_num=2,freq_drop_width=8, freq_stripes_num=2)\n",
    "        self.use_spec_aug = CFG.use_spec_aug\n",
    "        self.bn0 = nn.BatchNorm2d(CFG.mel_bins)\n",
    "\n",
    "        # Spectrogram extractor\n",
    "        self.spectrogram_extractor = MelSpectrogram(\n",
    "            sample_rate=CFG.sample_rate,\n",
    "            n_fft=CFG.n_fft,\n",
    "            win_length=CFG.window_size,\n",
    "            hop_length=CFG.hop_size,\n",
    "            f_min=CFG.fmin,\n",
    "            f_max=CFG.fmax,\n",
    "            pad=0,\n",
    "            n_mels=CFG.mel_bins,\n",
    "            power=2,\n",
    "            normalized=False,\n",
    "        )\n",
    "        # Logmel feature extractor\n",
    "        self.logmel_extractor = AmplitudeToDB(top_db=None)\n",
    "        self.normlize = NormalizeMelSpec()\n",
    "\n",
    "        self.ml_decoder = MLDecoder(num_classes=num_classes,initial_num_features=self.backbone.num_features)\n",
    "        self.init_weight()\n",
    "\n",
    "    def init_weight(self):\n",
    "        init_bn(self.bn0)\n",
    "\n",
    "    def get_mel_gram(self,audios):\n",
    "        \"\"\"\n",
    "        Input: (batch_size, data_length)\"\"\"\n",
    "        x = self.spectrogram_extractor(audios) # (batch_size,freq_bins time_steps)\n",
    "        x = self.logmel_extractor(x) \n",
    "        x = self.normlize(x)\n",
    "        x = x.permute(0,2,1)# (batch_size,time_steps, mel_bins)\n",
    "\n",
    "        return x\n",
    "\n",
    "    def forward(self,x:torch.Tensor,le=False):\n",
    "        x = x.contiguous()\n",
    "        # b c f t\n",
    "        if CFG.use_spec_aug and self.training:\n",
    "            if np.random.uniform(0,1)>CFG.p_spec_aug:\n",
    "                x = self.SpecAug(x)\n",
    "\n",
    "        x = x.transpose(1, 3)\n",
    "        x = self.bn0(x)\n",
    "        x = x.transpose(1, 3)\n",
    "\n",
    "        x = x.transpose(2, 3)\n",
    "\n",
    "        x = self.backbone.forward_features(x)\n",
    "\n",
    "        if le:\n",
    "            h,x = self.ml_decoder.forward(x,le=le)\n",
    "            return h,x\n",
    "        else:\n",
    "            x = self.ml_decoder.forward(x,le=le)\n",
    "            return x\n",
    "\n",
    "class BirdClefCNNFCModel(BirdClefCNNModel):\n",
    "    def __init__(self, model_name=CFG.model, num_classes=CFG.num_classes, pretrained=CFG.pretrained, p=0.5):\n",
    "        super().__init__(model_name, num_classes, pretrained, p)\n",
    "        self.fc = nn.Linear(in_features=self.backbone.num_features,out_features=num_classes)\n",
    "        self.pooling = GeM()\n",
    "    def forward(self,x:torch.Tensor,le=False):\n",
    "        x = x.contiguous()\n",
    "        # b c f t\n",
    "        if CFG.use_spec_aug and self.training:\n",
    "            if np.random.uniform(0,1)>CFG.p_spec_aug:\n",
    "                x = self.SpecAug(x)\n",
    "\n",
    "        x = x.transpose(1, 3)\n",
    "        x = self.bn0(x)\n",
    "        x = x.transpose(1, 3)\n",
    "\n",
    "        x = x.transpose(2, 3)\n",
    "\n",
    "        x = self.backbone.forward_features(x)\n",
    "\n",
    "        x = self.pooling(x)\n",
    "        x = self.fc(x)\n",
    "        return x\n",
    "\n",
    "# model_efb3 = BirdClefSEDAttModel(model_name=\"tf_efficientnet_b3_ns\",num_classes=CFG.num_classes,pretrained=CFG.pretrained)\n",
    "# model_efv2b2 = BirdClefSEDAttModel(model_name=\"tf_efficientnetv2_b2\",num_classes=CFG.num_classes,pretrained=CFG.pretrained)\n",
    "# model_seresnext = BirdClefSEDAttModel(model_name=\"seresnext26d_32x4d\",num_classes=CFG.num_classes,pretrained=CFG.pretrained)\n",
    "# model_seresnext.load_state_dict(torch.load('/kaggle/input/birdclefv3/seresnext_seclabel.pt',map_location=torch.device('cpu')))\n",
    "# model_nfnetl0 = BirdClefSEDAttModel(model_name=\"eca_nfnet_l0\",num_classes=CFG.num_classes,pretrained=CFG.pretrained)\n",
    "# model_nfnetl0.load_state_dict(torch.load('/kaggle/input/birdclefv3/ecanfnetl0_seclabel.pt',map_location=torch.device('cpu')))\n",
    "# model_efv2b2 = BirdClefSEDAttModel(model_name=\"tf_efficientnetv2_b2\",num_classes=CFG.num_classes,pretrained=CFG.pretrained)\n",
    "# model_efv2b2.load_state_dict(torch.load('/kaggle/input/birdclefv3/efv2_finetune_alldata.pt',map_location=torch.device('cpu')))\n",
    "\n",
    "model_efv2b3 = BirdClefCNNFCModel(model_name=\"tf_efficientnetv2_b3\",num_classes=CFG.num_classes,pretrained=CFG.pretrained)\n",
    "# model_dict = torch.load('/root/projects/BirdClef2025/BirdCLEF2023-30th-place-solution-master/logs/2025-04-08T01:58/saved_model.pt',map_location=torch.device('cpu'))\n",
    "# model_dict = {key.replace('module.',''):value for key, value in model_dict.items() if 'module' in key}\n",
    "model_efv2b3.load_state_dict(torch.load('/root/projects/BirdClef2025/BirdCLEF2023-30th-place-solution-master/logs/2025-04-16T01:57/saved_model_lastepoch.pt',map_location='cpu'))\n",
    "# model_resnest = BirdClefSEDAttModel(model_name=\"resnest26d\",num_classes=CFG.num_classes,pretrained=CFG.pretrained)\n",
    "# model_resnest.load_state_dict(torch.load('/kaggle/input/birdclefv3/resnest26d_alldata_mel128.pt',map_location=torch.device('cpu')))\n",
    "\n",
    "# model_efv2b3_split1 = BirdClefSEDAttModel(model_name=\"tf_efficientnetv2_b3\",num_classes=len(split_bird),pretrained=CFG.pretrained)\n",
    "# model_efv2b3_split1.load_state_dict(torch.load('/root/projects/BirdClef2025/BirdCLEF2023-30th-place-solution-master/logs/2025-03-24T02:52/saved_model_lastepoch.pt',map_location=torch.device('cpu')))\n",
    "# model_efv2b3 = BirdSetSEDModel(206)\n",
    "# model_efv2b3.load_state_dict(torch.load('/root/projects/BirdClef2025/BirdCLEF2023-30th-place-solution-master/logs/2025-04-03T13:08/saved_model.pt',map_location=torch.device('cpu')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "6a4f898b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_efv2b3.pooling.p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "87c2dca6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ INFO ] MO command line tool is considered as the legacy conversion API as of OpenVINO 2023.2 release.\n",
      "In 2025.0 MO command line tool and openvino.tools.mo.convert_model() will be removed. Please use OpenVINO Model Converter (OVC) or openvino.convert_model(). OVC represents a lightweight alternative of MO and provides simplified model conversion API. \n",
      "Find more information about transition from MO to OVC at https://docs.openvino.ai/2023.2/openvino_docs_OV_Converter_UG_prepare_model_convert_model_MO_OVC_transition.html\n",
      "[ INFO ] Generated IR will be compressed to FP16. If you get lower accuracy, please consider disabling compression explicitly by adding argument --compress_to_fp16=False.\n",
      "Find more information about compression to FP16 at https://docs.openvino.ai/2023.0/openvino_docs_MO_DG_FP16_Compression.html\n",
      "Check for a new version of Intel(R) Distribution of OpenVINO(TM) toolkit here https://software.intel.com/content/www/us/en/develop/tools/openvino-toolkit/download.html?cid=other&source=prod&campid=ww_2023_bu_IOTG_OpenVINO-2023-1&content=upg_all&medium=organic or on https://github.com/openvinotoolkit/openvino\n",
      "[ SUCCESS ] Generated IR version 11 model.\n",
      "[ SUCCESS ] XML file: /root/projects/BirdClef2025/BirdCLEF2023-30th-place-solution-master/usefulFunc/model_efv2b3.xml\n",
      "[ SUCCESS ] BIN file: /root/projects/BirdClef2025/BirdCLEF2023-30th-place-solution-master/usefulFunc/model_efv2b3.bin\n"
     ]
    }
   ],
   "source": [
    "dummy_input = torch.randn(12, 3, 469, 192)\n",
    "# output_names = [ \"clipwise_output\",\"framewise_output\",\"maxframewise_output\"]\n",
    "output_names = [ \"output\"]\n",
    "model_efv2b3.eval()\n",
    "torch.onnx.export(model_efv2b3,dummy_input,\"model_efv2b3.onnx\",input_names=[\"input\"],output_names=output_names)\n",
    "!mo --input_model=model_efv2b3.onnx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "c7e125d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from openvino.runtime import Core\n",
    "import numpy as np\n",
    "import cv2  # 用于预处理图像\n",
    "\n",
    "# 初始化推理引擎\n",
    "core = Core()\n",
    "model = core.read_model(model=\"/root/projects/BirdClef2025/BirdCLEF2023-30th-place-solution-master/usefulFunc/model_efv2b3.xml\")\n",
    "compiled_model_efv2b3 = core.compile_model(model=model, device_name=\"CPU\")\n",
    "infer_request_efv2b3 = compiled_model_efv2b3.create_infer_request()\n",
    "res = infer_request_efv2b3.infer(inputs=[dummy_input])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "62541169",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-24T20:56:17.959173Z",
     "iopub.status.busy": "2023-05-24T20:56:17.958782Z",
     "iopub.status.idle": "2023-05-24T20:56:17.993370Z",
     "shell.execute_reply": "2023-05-24T20:56:17.992418Z"
    },
    "papermill": {
     "duration": 0.046138,
     "end_time": "2023-05-24T20:56:17.996019",
     "exception": false,
     "start_time": "2023-05-24T20:56:17.949881",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "seconds = [i for i in range(5, 65, 5)]\n",
    "# model_seresnext.to('cpu')\n",
    "# model_efv2b3.to('cpu')\n",
    "# model_efv2b3_split1.to('cpu')\n",
    "# model_efv2b2.to('cpu')\n",
    "# model_efv2b3.to('cpu')\n",
    "# model_nfnetl0.to('cpu')\n",
    "# model_resnest.to('cpu')\n",
    "# model_seresnext.eval()\n",
    "# model_efv2b3.eval()\n",
    "# model_efv2b3_split1.eval()\n",
    "# model_efv2b2.eval()\n",
    "# model_nfnetl0.eval()\n",
    "# model_resnest.eval()\n",
    "models = [compiled_model_efv2b3]\n",
    "# models_split1 = [model_efv2b3_split1]\n",
    "# models = [model_seresnext]\n",
    "\n",
    "def smooth_array_general(array, w=[0.1, 0.2, 0.4, 0.2, 0.1]):\n",
    "    smoothed_array = np.zeros_like(array)\n",
    "    timesteps = array.shape[0]\n",
    "    radius = len(w) // 2 # 2\n",
    "\n",
    "    for t in range(timesteps):\n",
    "        for i, weight in enumerate(w):\n",
    "            index = t - radius + i\n",
    "            if index < 0: \n",
    "                smoothed_array[t] += array[0] * weight\n",
    "            elif index >= timesteps: \n",
    "                smoothed_array[t] += array[-1] * weight\n",
    "            else:\n",
    "                smoothed_array[t] += array[index] * weight\n",
    "    for c in range(array.shape[1]):\n",
    "        smoothed_array[:, c] = smoothed_array[:, c] * 0.8 + smoothed_array[:, c].mean(keepdims=True) * 0.2\n",
    "    return smoothed_array\n",
    "\n",
    "def prediction_for_clip(\n",
    "    audio_path\n",
    "):\n",
    "    predictions = []\n",
    "    device = torch.device(\"cpu\")\n",
    "    global models\n",
    "    # inference\n",
    "    prediction_dict = {}\n",
    "\n",
    "    clip, _ = librosa.load(audio_path, sr=32000)\n",
    "    name_ = \"_\".join(audio_path.name.split(\".\")[:-1])\n",
    "    row_ids = [name_+f\"_{second}\" for second in seconds]\n",
    "\n",
    "    test_df = pd.DataFrame({\n",
    "        \"row_id\": row_ids,\n",
    "        \"seconds\": seconds\n",
    "    })\n",
    "    \n",
    "    dataset = TestDataset(\n",
    "        df=test_df, \n",
    "        clip=clip,\n",
    "        config=CFG,\n",
    "        model=model_efv2b3\n",
    "    )\n",
    "\n",
    "    loader = torch.utils.data.DataLoader(\n",
    "        dataset,\n",
    "        batch_size=CFG.batch_size, \n",
    "        num_workers=4,\n",
    "        drop_last=False,\n",
    "        shuffle=False,\n",
    "        pin_memory=True\n",
    "    )\n",
    "\n",
    "    infer_request_models = []\n",
    "    for infer in models:\n",
    "        infer_request_models.append(infer.create_infer_request())\n",
    "\n",
    "    for data in loader:\n",
    "\n",
    "        row_ids = data['row_id']\n",
    "\n",
    "        for row_id in row_ids:\n",
    "            if row_id not in prediction_dict:\n",
    "                prediction_dict[str(row_id)] = []\n",
    "\n",
    "        image = data['image']#.to(device)\n",
    "        image = torch.repeat_interleave(image,repeats=3,dim=1)\n",
    "        probas = []\n",
    "        print(image.shape)\n",
    "        with torch.no_grad():\n",
    "            output = np.zeros((image.shape[0],206))\n",
    "            for model in infer_request_models:\n",
    "                output_temp = model.infer(inputs=[image])\n",
    "                output += output_temp['output']\n",
    "                # numpy sigmoid\n",
    "                output = 1/(1+np.exp(-output))\n",
    "                # output += (output_temp['clipwise_output'] + output_temp['maxframewise_output'])/2\n",
    "            output = output/len(models)\n",
    "\n",
    "        output = smooth_array_general(output)\n",
    "        \n",
    "        predictions.append(output)\n",
    "\n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "19637bea",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-24T20:56:18.012859Z",
     "iopub.status.busy": "2023-05-24T20:56:18.012458Z",
     "iopub.status.idle": "2023-05-24T20:57:18.377424Z",
     "shell.execute_reply": "2023-05-24T20:57:18.375886Z"
    },
    "papermill": {
     "duration": 60.376716,
     "end_time": "2023-05-24T20:57:18.380316",
     "exception": false,
     "start_time": "2023-05-24T20:56:18.003600",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([12, 3, 469, 192])\n",
      "torch.Size([12, 3, 469, 192])\n",
      "torch.Size([12, 3, 469, 192])\n",
      "torch.Size([12, 3, 469, 192])\n",
      "torch.Size([12, 3, 469, 192])\n",
      "torch.Size([12, 3, 469, 192])\n",
      "torch.Size([12, 3, 469, 192])\n",
      "torch.Size([12, 3, 469, 192])\n",
      "torch.Size([12, 3, 469, 192])\n",
      "9.793631553649902\n"
     ]
    }
   ],
   "source": [
    "from functools import partial\n",
    "import time\n",
    "all_audios = list(Path(CFG.test_path).glob(\"*.ogg\"))\n",
    "\n",
    "# prediction_for_clip_with_models = partial(prediction_for_clip, models=models,num_classes=206)\n",
    "start_time = time.time()\n",
    "with concurrent.futures.ThreadPoolExecutor(max_workers=4) as executor:\n",
    "    predictions = list(executor.map(prediction_for_clip,all_audios))\n",
    "print(time.time() - start_time)\n",
    "# prediction_for_clip_with_models = partial(prediction_for_clip, models=models_split1,num_classes=len(split_bird))\n",
    "# with concurrent.futures.ThreadPoolExecutor(max_workers=4) as executor:\n",
    "#     predictions_split1 = list(executor.map(prediction_for_clip_with_models,all_audios))\n",
    "\n",
    "# predictions = prediction_for_clip(all_audios[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c0815bb",
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[82], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mpredictions\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241m.\u001b[39mmax\n",
      "\u001b[0;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "predictions[0][0].max"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "ff0d45c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8.067443132400513\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "with concurrent.futures.ThreadPoolExecutor(max_workers=4) as executor:\n",
    "    predictions = list(executor.map(prediction_for_clip,all_audios))\n",
    "print(time.time() - start_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "c56183c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_temp = []\n",
    "for pred in predictions:\n",
    "    pred_temp = pred_temp + pred\n",
    "predictions = pred_temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "ac80db4d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-24T20:57:18.424645Z",
     "iopub.status.busy": "2023-05-24T20:57:18.424272Z",
     "iopub.status.idle": "2023-05-24T20:57:18.647564Z",
     "shell.execute_reply": "2023-05-24T20:57:18.646598Z"
    },
    "papermill": {
     "duration": 0.235021,
     "end_time": "2023-05-24T20:57:18.650298",
     "exception": false,
     "start_time": "2023-05-24T20:57:18.415277",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_2996408/628451567.py:16: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  sub_df = pd.concat([sub_df,df]).reset_index(drop=True)\n"
     ]
    }
   ],
   "source": [
    "# generate submission\n",
    "filenames = df_test.filename.values.tolist()\n",
    "bird_cols = list(pd.get_dummies(df_train['primary_label']).columns)\n",
    "sub_df = pd.DataFrame(columns=['row_id']+bird_cols)\n",
    "\n",
    "for i, file in enumerate(filenames):\n",
    "    pred = predictions[i]\n",
    "    pred_split1 = predictions[i]\n",
    "    num_rows = len(pred)\n",
    "    row_ids = [f'{file}_{(i+1)*5}' for i in range(num_rows)]\n",
    "    df = pd.DataFrame(columns=['row_id']+bird_cols)\n",
    "    \n",
    "    df['row_id'] = row_ids\n",
    "    df[bird_cols] = pred\n",
    "    \n",
    "    sub_df = pd.concat([sub_df,df]).reset_index(drop=True)\n",
    "    \n",
    "sub_df.to_csv('submission.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dccd61d7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = BirdClefSEDAttModel(model_name=\"tf_efficientnetv2_b3\",num_classes=CFG.num_classes,pretrained=CFG.pretrained)\n",
    "model.load_state_dict(torch.load('/root/projects/BirdClef2025/BirdCLEF2023-30th-place-solution-master/logs/efv2b3_pretrainednew_ft_bce-LB0.842.pt',map_location='cpu'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "4cc6f892",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = pd.read_csv('/root/projects/BirdClef2025/BirdCLEF2023-30th-place-solution-master/usefulFunc/train.csv')\n",
    "external_df = pd.read_csv('/root/projects/BirdClef2025/data/external_trainv2.csv')\n",
    "df_train = pd.concat((df_train,external_df))\n",
    "df_train = pd.concat([df_train, pd.get_dummies(df_train['primary_label'])], axis=1)\n",
    "low_numclasses_birds = list(df_train['primary_label'].value_counts()[(df_train['primary_label'].value_counts()<30)].index)\n",
    "low_numclasses_birds_index = []\n",
    "high_numclasses_birds_index = []\n",
    "birds_col = list(df_train.columns[14:])\n",
    "for idx,item in enumerate(birds_col):\n",
    "    if item in low_numclasses_birds:\n",
    "        low_numclasses_birds_index.append(idx)\n",
    "    else:\n",
    "        high_numclasses_birds_index.append(idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "835bf131",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 17, 20, 21, 23, 24, 25, 26, 27, 28, 29, 32, 33, 35, 37, 39, 40, 41, 44, 46, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 127, 137, 154, 159]\n"
     ]
    }
   ],
   "source": [
    "print(low_numclasses_birds_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "7dd449d3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_split1 = BirdClefSEDAttModel(model_name=\"tf_efficientnetv2_b3\",num_classes=len(low_numclasses_birds),pretrained=CFG.pretrained)\n",
    "model_split1.load_state_dict(torch.load('/root/projects/BirdClef2025/BirdCLEF2023-30th-place-solution-master/logs/2025-04-01T12:29/saved_model_lastepoch.pt',map_location='cpu'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "ef7ff0c4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_dict = torch.load('/root/projects/BirdClef2025/BirdCLEF2023-30th-place-solution-master/logs/efv2b3_pretrainednew_ft_bce-LB0.842.pt',map_location='cpu')\n",
    "model_dict_split = torch.load('/root/projects/BirdClef2025/BirdCLEF2023-30th-place-solution-master/logs/2025-04-01T12:29/saved_model_lastepoch.pt')\n",
    "model_dict_split = {k.replace('att_block','att_block_split'):v for k,v in model_dict_split.items() if 'att' in k}\n",
    "model_dict.update(model_dict_split)\n",
    "\n",
    "torch.save(model_dict,'/root/projects/BirdClef2025/BirdCLEF2023-30th-place-solution-master/logs/efv2b3_bce_focal.pt')\n",
    "model_split = SplitModel(low_numclasses_birds_index,model_name='tf_efficientnetv2_b3')\n",
    "model_split.load_state_dict(torch.load('/root/projects/BirdClef2025/BirdCLEF2023-30th-place-solution-master/logs/efv2b3_bce_focal.pt',map_location='cpu'))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cibmtr",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 86.723715,
   "end_time": "2023-05-24T20:57:21.146356",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2023-05-24T20:55:54.422641",
   "version": "2.4.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
