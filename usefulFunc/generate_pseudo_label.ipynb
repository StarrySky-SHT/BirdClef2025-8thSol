{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import timm\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "import soundfile as sf\n",
    "from  soundfile import SoundFile\n",
    "import librosa as lb\n",
    "import librosa.display as lbd\n",
    "import concurrent.futures\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "# config\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class CFG:\n",
    "    seed = 42\n",
    "    num_classes = 206\n",
    "    batch_size = 128\n",
    "    epochs = 30\n",
    "    PRECISION = 16    \n",
    "    PATIENCE = 8    \n",
    "    img_size = [224,224]\n",
    "    model = \"eca_nfnet_l0\"\n",
    "    pretrained = False            \n",
    "    weight_decay = 1e-3\n",
    "    use_mixup = True\n",
    "    mixup_alpha = 0.2   \n",
    "    use_spec_aug = False\n",
    "    p_spec_aug = 0.5\n",
    "    time_shift_prob = 0.5\n",
    "    gn_prob = 0.5\n",
    "    infer_duration = 5\n",
    "    fmin = 50\n",
    "    fmax = 14000\n",
    "    mel_bins = 192\n",
    "    window_size = 1024\n",
    "    hop_size = 512\n",
    "    use_fsr = False\n",
    "\n",
    "    device = torch.device('cpu')  \n",
    "\n",
    "    train_path = \"/root/projects/BirdClef2025/data/train.csv\"\n",
    "    valid_path = \"/root/projects/BirdClef2025/data/valid.csv\"\n",
    "    test_path = '/root/projects/BirdClef2025/data/train_soundscapes/'\n",
    "    sample_rate = 32000\n",
    "    duration = 5\n",
    "    max_read_samples = 10\n",
    "    lr = 5e-5\n",
    "\n",
    "    scheduler     = 'CosineAnnealingLR'\n",
    "    min_lr        = 5e-7\n",
    "    T_max         = int(30000/batch_size*epochs)+50\n",
    "    T_0           = 25\n",
    "\n",
    "    n_accumulate = 4\n",
    "    \n",
    "    target_columns = \"abethr1 abhori1 abythr1 afbfly1 afdfly1 afecuc1 affeag1 afgfly1 afghor1 afmdov1 afpfly1 afpkin1 afpwag1 afrgos1 afrgrp1 afrjac1 afrthr1 amesun2 augbuz1 bagwea1 barswa bawhor2 bawman1 bcbeat1 beasun2 bkctch1 bkfruw1 blacra1 blacuc1 blakit1 blaplo1 blbpuf2 blcapa2 blfbus1 blhgon1 blhher1 blksaw1 blnmou1 blnwea1 bltapa1 bltbar1 bltori1 blwlap1 brcale1 brcsta1 brctch1 brcwea1 brican1 brobab1 broman1 brosun1 brrwhe3 brtcha1 brubru1 brwwar1 bswdov1 btweye2 bubwar2 butapa1 cabgre1 carcha1 carwoo1 categr ccbeat1 chespa1 chewea1 chibat1 chtapa3 chucis1 cibwar1 cohmar1 colsun2 combul2 combuz1 comsan crefra2 crheag1 crohor1 darbar1 darter3 didcuc1 dotbar1 dutdov1 easmog1 eaywag1 edcsun3 egygoo equaka1 eswdov1 eubeat1 fatrav1 fatwid1 fislov1 fotdro5 gabgos2 gargan gbesta1 gnbcam2 gnhsun1 gobbun1 gobsta5 gobwea1 golher1 grbcam1 grccra1 grecor greegr grewoo2 grwpyt1 gryapa1 grywrw1 gybfis1 gycwar3 gyhbus1 gyhkin1 gyhneg1 gyhspa1 gytbar1 hadibi1 hamerk1 hartur1 helgui hipbab1 hoopoe huncis1 hunsun2 joygre1 kerspa2 klacuc1 kvbsun1 laudov1 lawgol lesmaw1 lessts1 libeat1 litegr litswi1 litwea1 loceag1 lotcor1 lotlap1 luebus1 mabeat1 macshr1 malkin1 marsto1 marsun2 mcptit1 meypar1 moccha1 mouwag1 ndcsun2 nobfly1 norbro1 norcro1 norfis1 norpuf1 nubwoo1 pabspa1 palfly2 palpri1 piecro1 piekin1 pitwhy purgre2 pygbat1 quailf1 ratcis1 raybar1 rbsrob1 rebfir2 rebhor1 reboxp1 reccor reccuc1 reedov1 refbar2 refcro1 reftin1 refwar2 rehblu1 rehwea1 reisee2 rerswa1 rewsta1 rindov rocmar2 rostur1 ruegls1 rufcha2 sacibi2 sccsun2 scrcha1 scthon1 shesta1 sichor1 sincis1 slbgre1 slcbou1 sltnig1 sobfly1 somgre1 somtit4 soucit1 soufis1 spemou2 spepig1 spewea1 spfbar1 spfwea1 spmthr1 spwlap1 squher1 strher strsee1 stusta1 subbus1 supsta1 tacsun1 tafpri1 tamdov1 thrnig1 trobou1 varsun2 vibsta2 vilwea1 vimwea1 walsta1 wbgbir1 wbrcha2 wbswea1 wfbeat1 whbcan1 whbcou1 whbcro2 whbtit5 whbwea1 whbwhe3 whcpri2 whctur2 wheslf1 whhsaw1 whihel1 whrshr1 witswa1 wlwwar wookin1 woosan wtbeat1 yebapa1 yebbar1 yebduc1 yebere1 yebgre1 yebsto1 yeccan1 yefcan yelbis1 yenspu1 yertin1 yesbar1 yespet1 yetgre1 yewgre1\".split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import argparse\n",
    "\n",
    "import librosa\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.nn.parameter import Parameter\n",
    "\n",
    "\n",
    "class DFTBase(nn.Module):\n",
    "    def __init__(self):\n",
    "        r\"\"\"Base class for DFT and IDFT matrix.\n",
    "        \"\"\"\n",
    "        super(DFTBase, self).__init__()\n",
    "\n",
    "    def dft_matrix(self, n):\n",
    "        (x, y) = np.meshgrid(np.arange(n), np.arange(n))\n",
    "        omega = np.exp(-2 * np.pi * 1j / n)\n",
    "        W = np.power(omega, x * y)  # shape: (n, n)\n",
    "        return W\n",
    "\n",
    "    def idft_matrix(self, n):\n",
    "        (x, y) = np.meshgrid(np.arange(n), np.arange(n))\n",
    "        omega = np.exp(2 * np.pi * 1j / n)\n",
    "        W = np.power(omega, x * y)  # shape: (n, n)\n",
    "        return W\n",
    "\n",
    "\n",
    "class DFT(DFTBase):\n",
    "    def __init__(self, n, norm):\n",
    "        r\"\"\"Calculate discrete Fourier transform (DFT), inverse DFT (IDFT, \n",
    "        right DFT (RDFT) RDFT, and inverse RDFT (IRDFT.) \n",
    "\n",
    "        Args:\n",
    "          n: fft window size\n",
    "          norm: None | 'ortho'\n",
    "        \"\"\"\n",
    "        super(DFT, self).__init__()\n",
    "\n",
    "        self.W = self.dft_matrix(n)\n",
    "        self.inv_W = self.idft_matrix(n)\n",
    "\n",
    "        self.W_real = torch.Tensor(np.real(self.W))\n",
    "        self.W_imag = torch.Tensor(np.imag(self.W))\n",
    "        self.inv_W_real = torch.Tensor(np.real(self.inv_W))\n",
    "        self.inv_W_imag = torch.Tensor(np.imag(self.inv_W))\n",
    "\n",
    "        self.n = n\n",
    "        self.norm = norm\n",
    "\n",
    "    def dft(self, x_real, x_imag):\n",
    "        r\"\"\"Calculate DFT of a signal.\n",
    "\n",
    "        Args:\n",
    "            x_real: (n,), real part of a signal\n",
    "            x_imag: (n,), imag part of a signal\n",
    "\n",
    "        Returns:\n",
    "            z_real: (n,), real part of output\n",
    "            z_imag: (n,), imag part of output\n",
    "        \"\"\"\n",
    "        z_real = torch.matmul(x_real, self.W_real) - torch.matmul(x_imag, self.W_imag)\n",
    "        z_imag = torch.matmul(x_imag, self.W_real) + torch.matmul(x_real, self.W_imag)\n",
    "        # shape: (n,)\n",
    "\n",
    "        if self.norm is None:\n",
    "            pass\n",
    "        elif self.norm == 'ortho':\n",
    "            z_real /= math.sqrt(self.n)\n",
    "            z_imag /= math.sqrt(self.n)\n",
    "\n",
    "        return z_real, z_imag\n",
    "\n",
    "    def idft(self, x_real, x_imag):\n",
    "        r\"\"\"Calculate IDFT of a signal.\n",
    "\n",
    "        Args:\n",
    "            x_real: (n,), real part of a signal\n",
    "            x_imag: (n,), imag part of a signal\n",
    "        Returns:\n",
    "            z_real: (n,), real part of output\n",
    "            z_imag: (n,), imag part of output\n",
    "        \"\"\"\n",
    "        z_real = torch.matmul(x_real, self.inv_W_real) - torch.matmul(x_imag, self.inv_W_imag)\n",
    "        z_imag = torch.matmul(x_imag, self.inv_W_real) + torch.matmul(x_real, self.inv_W_imag)\n",
    "        # shape: (n,)\n",
    "\n",
    "        if self.norm is None:\n",
    "            z_real /= self.n\n",
    "        elif self.norm == 'ortho':\n",
    "            z_real /= math.sqrt(n)\n",
    "            z_imag /= math.sqrt(n)\n",
    "\n",
    "        return z_real, z_imag\n",
    "\n",
    "    def rdft(self, x_real):\n",
    "        r\"\"\"Calculate right RDFT of signal.\n",
    "\n",
    "        Args:\n",
    "            x_real: (n,), real part of a signal\n",
    "            x_imag: (n,), imag part of a signal\n",
    "\n",
    "        Returns:\n",
    "            z_real: (n // 2 + 1,), real part of output\n",
    "            z_imag: (n // 2 + 1,), imag part of output\n",
    "        \"\"\"\n",
    "        n_rfft = self.n // 2 + 1\n",
    "        z_real = torch.matmul(x_real, self.W_real[..., 0 : n_rfft])\n",
    "        z_imag = torch.matmul(x_real, self.W_imag[..., 0 : n_rfft])\n",
    "        # shape: (n // 2 + 1,)\n",
    "\n",
    "        if self.norm is None:\n",
    "            pass\n",
    "        elif self.norm == 'ortho':\n",
    "            z_real /= math.sqrt(self.n)\n",
    "            z_imag /= math.sqrt(self.n)\n",
    "\n",
    "        return z_real, z_imag\n",
    "\n",
    "    def irdft(self, x_real, x_imag):\n",
    "        r\"\"\"Calculate IRDFT of signal.\n",
    "        \n",
    "        Args:\n",
    "            x_real: (n // 2 + 1,), real part of a signal\n",
    "            x_imag: (n // 2 + 1,), imag part of a signal\n",
    "\n",
    "        Returns:\n",
    "            z_real: (n,), real part of output\n",
    "            z_imag: (n,), imag part of output\n",
    "        \"\"\"\n",
    "        n_rfft = self.n // 2 + 1\n",
    "\n",
    "        flip_x_real = torch.flip(x_real, dims=(-1,))\n",
    "        flip_x_imag = torch.flip(x_imag, dims=(-1,))\n",
    "        # shape: (n // 2 + 1,)\n",
    "\n",
    "        x_real = torch.cat((x_real, flip_x_real[..., 1 : n_rfft - 1]), dim=-1)\n",
    "        x_imag = torch.cat((x_imag, -1. * flip_x_imag[..., 1 : n_rfft - 1]), dim=-1)\n",
    "        # shape: (n,)\n",
    "\n",
    "        z_real = torch.matmul(x_real, self.inv_W_real) - torch.matmul(x_imag, self.inv_W_imag)\n",
    "        # shape: (n,)\n",
    "\n",
    "        if self.norm is None:\n",
    "            z_real /= self.n\n",
    "        elif self.norm == 'ortho':\n",
    "            z_real /= math.sqrt(n)\n",
    "\n",
    "        return z_real\n",
    "\n",
    "\n",
    "class STFT(DFTBase):\n",
    "    def __init__(self, n_fft=2048, hop_length=None, win_length=None,\n",
    "        window='hann', center=True, pad_mode='reflect', freeze_parameters=True):\n",
    "        r\"\"\"PyTorch implementation of STFT with Conv1d. The function has the \n",
    "        same output as librosa.stft.\n",
    "\n",
    "        Args:\n",
    "            n_fft: int, fft window size, e.g., 2048\n",
    "            hop_length: int, hop length samples, e.g., 441\n",
    "            win_length: int, window length e.g., 2048\n",
    "            window: str, window function name, e.g., 'hann'\n",
    "            center: bool\n",
    "            pad_mode: str, e.g., 'reflect'\n",
    "            freeze_parameters: bool, set to True to freeze all parameters. Set\n",
    "                to False to finetune all parameters.\n",
    "        \"\"\"\n",
    "        super(STFT, self).__init__()\n",
    "\n",
    "        assert pad_mode in ['constant', 'reflect']\n",
    "\n",
    "        self.n_fft = n_fft\n",
    "        self.hop_length = hop_length\n",
    "        self.win_length = win_length\n",
    "        self.window = window\n",
    "        self.center = center\n",
    "        self.pad_mode = pad_mode\n",
    "\n",
    "        # By default, use the entire frame.\n",
    "        if self.win_length is None:\n",
    "            self.win_length = n_fft\n",
    "\n",
    "        # Set the default hop, if it's not already specified.\n",
    "        if self.hop_length is None:\n",
    "            self.hop_length = int(self.win_length // 4)\n",
    "\n",
    "        fft_window = librosa.filters.get_window(window, self.win_length, fftbins=True)\n",
    "\n",
    "        # Pad the window out to n_fft size.\n",
    "        fft_window = librosa.util.pad_center(fft_window, size=n_fft)\n",
    "\n",
    "        # DFT & IDFT matrix.\n",
    "        self.W = self.dft_matrix(n_fft)\n",
    "\n",
    "        out_channels = n_fft // 2 + 1\n",
    "\n",
    "        self.conv_real = nn.Conv1d(in_channels=1, out_channels=out_channels,\n",
    "            kernel_size=n_fft, stride=self.hop_length, padding=0, dilation=1,\n",
    "            groups=1, bias=False)\n",
    "\n",
    "        self.conv_imag = nn.Conv1d(in_channels=1, out_channels=out_channels,\n",
    "            kernel_size=n_fft, stride=self.hop_length, padding=0, dilation=1,\n",
    "            groups=1, bias=False)\n",
    "\n",
    "        # Initialize Conv1d weights.\n",
    "        self.conv_real.weight.data = torch.Tensor(\n",
    "            np.real(self.W[:, 0 : out_channels] * fft_window[:, None]).T)[:, None, :]\n",
    "        # (n_fft // 2 + 1, 1, n_fft)\n",
    "\n",
    "        self.conv_imag.weight.data = torch.Tensor(\n",
    "            np.imag(self.W[:, 0 : out_channels] * fft_window[:, None]).T)[:, None, :]\n",
    "        # (n_fft // 2 + 1, 1, n_fft)\n",
    "\n",
    "        if freeze_parameters:\n",
    "            for param in self.parameters():\n",
    "                param.requires_grad = False\n",
    "\n",
    "    def forward(self, input):\n",
    "        r\"\"\"Calculate STFT of batch of signals.\n",
    "\n",
    "        Args: \n",
    "            input: (batch_size, data_length), input signals.\n",
    "\n",
    "        Returns:\n",
    "            real: (batch_size, 1, time_steps, n_fft // 2 + 1)\n",
    "            imag: (batch_size, 1, time_steps, n_fft // 2 + 1)\n",
    "        \"\"\"\n",
    "\n",
    "        x = input[:, None, :]   # (batch_size, channels_num, data_length)\n",
    "\n",
    "        if self.center:\n",
    "            x = F.pad(x, pad=(self.n_fft // 2, self.n_fft // 2), mode=self.pad_mode)\n",
    "\n",
    "        real = self.conv_real(x)\n",
    "        imag = self.conv_imag(x)\n",
    "        # (batch_size, n_fft // 2 + 1, time_steps)\n",
    "\n",
    "        real = real[:, None, :, :].transpose(2, 3)\n",
    "        imag = imag[:, None, :, :].transpose(2, 3)\n",
    "        # (batch_size, 1, time_steps, n_fft // 2 + 1)\n",
    "\n",
    "        return real, imag\n",
    "\n",
    "\n",
    "def magphase(real, imag):\n",
    "    r\"\"\"Calculate magnitude and phase from real and imag part of signals.\n",
    "\n",
    "    Args:\n",
    "        real: tensor, real part of signals\n",
    "        imag: tensor, imag part of signals\n",
    "\n",
    "    Returns:\n",
    "        mag: tensor, magnitude of signals\n",
    "        cos: tensor, cosine of phases of signals\n",
    "        sin: tensor, sine of phases of signals\n",
    "    \"\"\"\n",
    "    mag = (real ** 2 + imag ** 2) ** 0.5\n",
    "    cos = real / torch.clamp(mag, 1e-10, np.inf)\n",
    "    sin = imag / torch.clamp(mag, 1e-10, np.inf)\n",
    "\n",
    "    return mag, cos, sin\n",
    "\n",
    "\n",
    "class ISTFT(DFTBase):\n",
    "    def __init__(self, n_fft=2048, hop_length=None, win_length=None,\n",
    "        window='hann', center=True, pad_mode='reflect', freeze_parameters=True, \n",
    "        onnx=False, frames_num=None, device=None):\n",
    "        \"\"\"PyTorch implementation of ISTFT with Conv1d. The function has the \n",
    "        same output as librosa.istft.\n",
    "\n",
    "        Args:\n",
    "            n_fft: int, fft window size, e.g., 2048\n",
    "            hop_length: int, hop length samples, e.g., 441\n",
    "            win_length: int, window length e.g., 2048\n",
    "            window: str, window function name, e.g., 'hann'\n",
    "            center: bool\n",
    "            pad_mode: str, e.g., 'reflect'\n",
    "            freeze_parameters: bool, set to True to freeze all parameters. Set\n",
    "                to False to finetune all parameters.\n",
    "            onnx: bool, set to True when exporting trained model to ONNX. This\n",
    "                will replace several operations to operators supported by ONNX.\n",
    "            frames_num: None | int, number of frames of audio clips to be \n",
    "                inferneced. Only useable when onnx=True.\n",
    "            device: None | str, device of ONNX. Only useable when onnx=True.\n",
    "        \"\"\"\n",
    "        super(ISTFT, self).__init__()\n",
    "\n",
    "        assert pad_mode in ['constant', 'reflect']\n",
    "\n",
    "        if not onnx:\n",
    "            assert frames_num is None, \"When onnx=False, frames_num must be None!\"\n",
    "            assert device is None, \"When onnx=False, device must be None!\"\n",
    "\n",
    "        self.n_fft = n_fft\n",
    "        self.hop_length = hop_length\n",
    "        self.win_length = win_length\n",
    "        self.window = window\n",
    "        self.center = center\n",
    "        self.pad_mode = pad_mode\n",
    "        self.onnx = onnx\n",
    "\n",
    "        # By default, use the entire frame.\n",
    "        if self.win_length is None:\n",
    "            self.win_length = self.n_fft\n",
    "\n",
    "        # Set the default hop, if it's not already specified.\n",
    "        if self.hop_length is None:\n",
    "            self.hop_length = int(self.win_length // 4)\n",
    "\n",
    "        # Initialize Conv1d modules for calculating real and imag part of DFT.\n",
    "        self.init_real_imag_conv()\n",
    "\n",
    "        # Initialize overlap add window for reconstruct time domain signals.\n",
    "        self.init_overlap_add_window()\n",
    "\n",
    "        if self.onnx:\n",
    "            # Initialize ONNX modules.\n",
    "            self.init_onnx_modules(frames_num, device)\n",
    "        \n",
    "        if freeze_parameters:\n",
    "            for param in self.parameters():\n",
    "                param.requires_grad = False\n",
    "\n",
    "    def init_real_imag_conv(self):\n",
    "        r\"\"\"Initialize Conv1d for calculating real and imag part of DFT.\n",
    "        \"\"\"\n",
    "        self.W = self.idft_matrix(self.n_fft) / self.n_fft\n",
    "\n",
    "        self.conv_real = nn.Conv1d(in_channels=self.n_fft, out_channels=self.n_fft,\n",
    "            kernel_size=1, stride=1, padding=0, dilation=1,\n",
    "            groups=1, bias=False)\n",
    "\n",
    "        self.conv_imag = nn.Conv1d(in_channels=self.n_fft, out_channels=self.n_fft,\n",
    "            kernel_size=1, stride=1, padding=0, dilation=1,\n",
    "            groups=1, bias=False)\n",
    "\n",
    "        ifft_window = librosa.filters.get_window(self.window, self.win_length, fftbins=True)\n",
    "        # (win_length,)\n",
    "\n",
    "        # Pad the window to n_fft\n",
    "        ifft_window = librosa.util.pad_center(ifft_window, size=self.n_fft)\n",
    "\n",
    "        self.conv_real.weight.data = torch.Tensor(\n",
    "            np.real(self.W * ifft_window[None, :]).T)[:, :, None]\n",
    "        # (n_fft // 2 + 1, 1, n_fft)\n",
    "\n",
    "        self.conv_imag.weight.data = torch.Tensor(\n",
    "            np.imag(self.W * ifft_window[None, :]).T)[:, :, None]\n",
    "        # (n_fft // 2 + 1, 1, n_fft)\n",
    "\n",
    "    def init_overlap_add_window(self):\n",
    "        r\"\"\"Initialize overlap add window for reconstruct time domain signals.\n",
    "        \"\"\"\n",
    "        \n",
    "        ola_window = librosa.filters.get_window(self.window, self.win_length, fftbins=True)\n",
    "        # (win_length,)\n",
    "\n",
    "        ola_window = librosa.util.normalize(ola_window, norm=None) ** 2\n",
    "        ola_window = librosa.util.pad_center(ola_window, size=self.n_fft)\n",
    "        ola_window = torch.Tensor(ola_window)\n",
    "\n",
    "        self.register_buffer('ola_window', ola_window)\n",
    "        # (win_length,)\n",
    "\n",
    "    def init_onnx_modules(self, frames_num, device):\n",
    "        r\"\"\"Initialize ONNX modules.\n",
    "\n",
    "        Args:\n",
    "            frames_num: int\n",
    "            device: str | None\n",
    "        \"\"\"\n",
    "\n",
    "        # Use Conv1d to implement torch.flip(), because torch.flip() is not \n",
    "        # supported by ONNX.\n",
    "        self.reverse = nn.Conv1d(in_channels=self.n_fft // 2 + 1,\n",
    "            out_channels=self.n_fft // 2 - 1, kernel_size=1, bias=False)\n",
    "\n",
    "        tmp = np.zeros((self.n_fft // 2 - 1, self.n_fft // 2 + 1, 1))\n",
    "        tmp[:, 1 : -1, 0] = np.array(np.eye(self.n_fft // 2 - 1)[::-1])\n",
    "        self.reverse.weight.data = torch.Tensor(tmp)\n",
    "        # (n_fft // 2 - 1, n_fft // 2 + 1, 1)\n",
    "\n",
    "        # Use nn.ConvTranspose2d to implement torch.nn.functional.fold(), \n",
    "        # because torch.nn.functional.fold() is not supported by ONNX.\n",
    "        self.overlap_add = nn.ConvTranspose2d(in_channels=self.n_fft,\n",
    "            out_channels=1, kernel_size=(self.n_fft, 1), stride=(self.hop_length, 1), bias=False)\n",
    "\n",
    "        self.overlap_add.weight.data = torch.Tensor(np.eye(self.n_fft)[:, None, :, None])\n",
    "        # (n_fft, 1, n_fft, 1)\n",
    "\n",
    "        if frames_num:\n",
    "            # Pre-calculate overlap-add window sum for reconstructing signals\n",
    "            # when using ONNX.\n",
    "            self.ifft_window_sum = self._get_ifft_window_sum_onnx(frames_num, device)\n",
    "        else:\n",
    "            self.ifft_window_sum = []\n",
    "\n",
    "    def forward(self, real_stft, imag_stft, length):\n",
    "        r\"\"\"Calculate inverse STFT.\n",
    "\n",
    "        Args:\n",
    "            real_stft: (batch_size, channels=1, time_steps, n_fft // 2 + 1)\n",
    "            imag_stft: (batch_size, channels=1, time_steps, n_fft // 2 + 1)\n",
    "            length: int\n",
    "        \n",
    "        Returns:\n",
    "            real: (batch_size, data_length), output signals.\n",
    "        \"\"\"\n",
    "        assert real_stft.ndimension() == 4 and imag_stft.ndimension() == 4\n",
    "        batch_size, _, frames_num, _ = real_stft.shape\n",
    "\n",
    "        real_stft = real_stft[:, 0, :, :].transpose(1, 2)\n",
    "        imag_stft = imag_stft[:, 0, :, :].transpose(1, 2)\n",
    "        # (batch_size, n_fft // 2 + 1, time_steps)\n",
    "\n",
    "        # Get full stft representation from spectrum using symmetry attribute.\n",
    "        if self.onnx:\n",
    "            full_real_stft, full_imag_stft = self._get_full_stft_onnx(real_stft, imag_stft)\n",
    "        else:\n",
    "            full_real_stft, full_imag_stft = self._get_full_stft(real_stft, imag_stft)\n",
    "        # full_real_stft: (batch_size, n_fft, time_steps)\n",
    "        # full_imag_stft: (batch_size, n_fft, time_steps)\n",
    "\n",
    "        # Calculate IDFT frame by frame.\n",
    "        s_real = self.conv_real(full_real_stft) - self.conv_imag(full_imag_stft)\n",
    "        # (batch_size, n_fft, time_steps)\n",
    "\n",
    "        # Overlap add signals in frames to reconstruct signals.\n",
    "        if self.onnx:\n",
    "            y = self._overlap_add_divide_window_sum_onnx(s_real, frames_num)\n",
    "        else:\n",
    "            y = self._overlap_add_divide_window_sum(s_real, frames_num)\n",
    "        # y: (batch_size, audio_samples + win_length,)\n",
    "        \n",
    "        y = self._trim_edges(y, length)\n",
    "        # (batch_size, audio_samples,)\n",
    "            \n",
    "        return y\n",
    "\n",
    "    def _get_full_stft(self, real_stft, imag_stft):\n",
    "        r\"\"\"Get full stft representation from spectrum using symmetry attribute.\n",
    "\n",
    "        Args:\n",
    "            real_stft: (batch_size, n_fft // 2 + 1, time_steps)\n",
    "            imag_stft: (batch_size, n_fft // 2 + 1, time_steps)\n",
    "\n",
    "        Returns:\n",
    "            full_real_stft: (batch_size, n_fft, time_steps)\n",
    "            full_imag_stft: (batch_size, n_fft, time_steps)\n",
    "        \"\"\"\n",
    "        full_real_stft = torch.cat((real_stft, torch.flip(real_stft[:, 1 : -1, :], dims=[1])), dim=1)\n",
    "        full_imag_stft = torch.cat((imag_stft, - torch.flip(imag_stft[:, 1 : -1, :], dims=[1])), dim=1)\n",
    "\n",
    "        return full_real_stft, full_imag_stft\n",
    "\n",
    "    def _get_full_stft_onnx(self, real_stft, imag_stft):\n",
    "        r\"\"\"Get full stft representation from spectrum using symmetry attribute\n",
    "        for ONNX. Replace several pytorch operations in self._get_full_stft() \n",
    "        that are not supported by ONNX.\n",
    "\n",
    "        Args:\n",
    "            real_stft: (batch_size, n_fft // 2 + 1, time_steps)\n",
    "            imag_stft: (batch_size, n_fft // 2 + 1, time_steps)\n",
    "\n",
    "        Returns:\n",
    "            full_real_stft: (batch_size, n_fft, time_steps)\n",
    "            full_imag_stft: (batch_size, n_fft, time_steps)\n",
    "        \"\"\"\n",
    "\n",
    "        # Implement torch.flip() with Conv1d.\n",
    "        full_real_stft = torch.cat((real_stft, self.reverse(real_stft)), dim=1)\n",
    "        full_imag_stft = torch.cat((imag_stft, - self.reverse(imag_stft)), dim=1)\n",
    "\n",
    "        return full_real_stft, full_imag_stft\n",
    "\n",
    "    def _overlap_add_divide_window_sum(self, s_real, frames_num):\n",
    "        r\"\"\"Overlap add signals in frames to reconstruct signals.\n",
    "\n",
    "        Args:\n",
    "            s_real: (batch_size, n_fft, time_steps), signals in frames\n",
    "            frames_num: int\n",
    "\n",
    "        Returns:\n",
    "            y: (batch_size, audio_samples)\n",
    "        \"\"\"\n",
    "        \n",
    "        output_samples = (s_real.shape[-1] - 1) * self.hop_length + self.win_length\n",
    "        # (audio_samples,)\n",
    "\n",
    "        # Overlap-add signals in frames to signals. Ref: \n",
    "        # asteroid_filterbanks.torch_stft_fb.torch_stft_fb() from\n",
    "        # https://github.com/asteroid-team/asteroid-filterbanks\n",
    "        y = torch.nn.functional.fold(input=s_real, output_size=(1, output_samples), \n",
    "            kernel_size=(1, self.win_length), stride=(1, self.hop_length))\n",
    "        # (batch_size, 1, 1, audio_samples,)\n",
    "        \n",
    "        y = y[:, 0, 0, :]\n",
    "        # (batch_size, audio_samples)\n",
    "\n",
    "        # Get overlap-add window sum to be divided.\n",
    "        ifft_window_sum = self._get_ifft_window(frames_num)\n",
    "        # (audio_samples,)\n",
    "\n",
    "        # Following code is abandaned for divide overlap-add window, because\n",
    "        # not supported by half precision training and ONNX.\n",
    "        # min_mask = ifft_window_sum.abs() < 1e-11\n",
    "        # y[:, ~min_mask] = y[:, ~min_mask] / ifft_window_sum[None, ~min_mask]\n",
    "        # # (batch_size, audio_samples)\n",
    "\n",
    "        ifft_window_sum = torch.clamp(ifft_window_sum, 1e-11, np.inf)\n",
    "        # (audio_samples,)\n",
    "\n",
    "        y = y / ifft_window_sum[None, :]\n",
    "        # (batch_size, audio_samples,)\n",
    "\n",
    "        return y\n",
    "\n",
    "    def _get_ifft_window(self, frames_num):\n",
    "        r\"\"\"Get overlap-add window sum to be divided.\n",
    "\n",
    "        Args:\n",
    "            frames_num: int\n",
    "\n",
    "        Returns:\n",
    "            ifft_window_sum: (audio_samlpes,), overlap-add window sum to be \n",
    "            divided.\n",
    "        \"\"\"\n",
    "        \n",
    "        output_samples = (frames_num - 1) * self.hop_length + self.win_length\n",
    "        # (audio_samples,)\n",
    "\n",
    "        window_matrix = self.ola_window[None, :, None].repeat(1, 1, frames_num)\n",
    "        # (batch_size, win_length, time_steps)\n",
    "\n",
    "        ifft_window_sum = F.fold(input=window_matrix, \n",
    "            output_size=(1, output_samples), kernel_size=(1, self.win_length), \n",
    "            stride=(1, self.hop_length))\n",
    "        # (1, 1, 1, audio_samples)\n",
    "        \n",
    "        ifft_window_sum = ifft_window_sum.squeeze()\n",
    "        # (audio_samlpes,)\n",
    "\n",
    "        return ifft_window_sum\n",
    "\n",
    "    def _overlap_add_divide_window_sum_onnx(self, s_real, frames_num):\n",
    "        r\"\"\"Overlap add signals in frames to reconstruct signals for ONNX. \n",
    "        Replace several pytorch operations in \n",
    "        self._overlap_add_divide_window_sum() that are not supported by ONNX.\n",
    "\n",
    "        Args:\n",
    "            s_real: (batch_size, n_fft, time_steps), signals in frames\n",
    "            frames_num: int\n",
    "\n",
    "        Returns:\n",
    "            y: (batch_size, audio_samples)\n",
    "        \"\"\"\n",
    "\n",
    "        s_real = s_real[..., None]\n",
    "        # (batch_size, n_fft, time_steps, 1)\n",
    "\n",
    "        # Implement overlap-add with Conv1d, because torch.nn.functional.fold()\n",
    "        # is not supported by ONNX.\n",
    "        y = self.overlap_add(s_real)[:, 0, :, 0]    \n",
    "        # y: (batch_size, samples_num)\n",
    "        \n",
    "        if len(self.ifft_window_sum) != y.shape[1]:\n",
    "            device = s_real.device\n",
    "\n",
    "            self.ifft_window_sum = self._get_ifft_window_sum_onnx(frames_num, device)\n",
    "            # (audio_samples,)\n",
    "\n",
    "        # Use torch.clamp() to prevent from underflow to make sure all \n",
    "        # operations are supported by ONNX.\n",
    "        ifft_window_sum = torch.clamp(self.ifft_window_sum, 1e-11, np.inf)\n",
    "        # (audio_samples,)\n",
    "\n",
    "        y = y / ifft_window_sum[None, :]\n",
    "        # (batch_size, audio_samples,)\n",
    "        \n",
    "        return y\n",
    "\n",
    "    def _get_ifft_window_sum_onnx(self, frames_num, device):\n",
    "        r\"\"\"Pre-calculate overlap-add window sum for reconstructing signals when\n",
    "        using ONNX.\n",
    "\n",
    "        Args:\n",
    "            frames_num: int\n",
    "            device: str | None\n",
    "\n",
    "        Returns:\n",
    "            ifft_window_sum: (audio_samples,)\n",
    "        \"\"\"\n",
    "        \n",
    "        ifft_window_sum = librosa.filters.window_sumsquare(window=self.window, \n",
    "            n_frames=frames_num, win_length=self.win_length, n_fft=self.n_fft, \n",
    "            hop_length=self.hop_length)\n",
    "        # (audio_samples,)\n",
    "\n",
    "        ifft_window_sum = torch.Tensor(ifft_window_sum)\n",
    "\n",
    "        if device:\n",
    "            ifft_window_sum = ifft_window_sum.to(device)\n",
    "\n",
    "        return ifft_window_sum\n",
    "\n",
    "    def _trim_edges(self, y, length):\n",
    "        r\"\"\"Trim audio.\n",
    "\n",
    "        Args:\n",
    "            y: (audio_samples,)\n",
    "            length: int\n",
    "\n",
    "        Returns:\n",
    "            (trimmed_audio_samples,)\n",
    "        \"\"\"\n",
    "        # Trim or pad to length\n",
    "        if length is None:\n",
    "            if self.center:\n",
    "                y = y[:, self.n_fft // 2 : -self.n_fft // 2]\n",
    "        else:\n",
    "            if self.center:\n",
    "                start = self.n_fft // 2\n",
    "            else:\n",
    "                start = 0\n",
    "\n",
    "            y = y[:, start : start + length]\n",
    "\n",
    "        return y\n",
    "\n",
    "\n",
    "class Spectrogram(nn.Module):\n",
    "    def __init__(self, n_fft=2048, hop_length=None, win_length=None,\n",
    "        window='hann', center=True, pad_mode='reflect', power=2.0,\n",
    "        freeze_parameters=True):\n",
    "        r\"\"\"Calculate spectrogram using pytorch. The STFT is implemented with \n",
    "        Conv1d. The function has the same output of librosa.stft\n",
    "        \"\"\"\n",
    "        super(Spectrogram, self).__init__()\n",
    "\n",
    "        self.power = power\n",
    "\n",
    "        self.stft = STFT(n_fft=n_fft, hop_length=hop_length,\n",
    "            win_length=win_length, window=window, center=center,\n",
    "            pad_mode=pad_mode, freeze_parameters=True)\n",
    "\n",
    "    def forward(self, input):\n",
    "        r\"\"\"Calculate spectrogram of input signals.\n",
    "        Args: \n",
    "            input: (batch_size, data_length)\n",
    "\n",
    "        Returns:\n",
    "            spectrogram: (batch_size, 1, time_steps, n_fft // 2 + 1)\n",
    "        \"\"\"\n",
    "\n",
    "        (real, imag) = self.stft.forward(input)\n",
    "        # (batch_size, n_fft // 2 + 1, time_steps)\n",
    "\n",
    "        spectrogram = real ** 2 + imag ** 2\n",
    "\n",
    "        if self.power == 2.0:\n",
    "            pass\n",
    "        else:\n",
    "            spectrogram = spectrogram ** (self.power / 2.0)\n",
    "\n",
    "        return spectrogram\n",
    "\n",
    "\n",
    "class LogmelFilterBank(nn.Module):\n",
    "    def __init__(self, sr=22050, n_fft=2048, n_mels=64, fmin=0.0, fmax=None, \n",
    "        is_log=True, ref=1.0, amin=1e-10, top_db=80.0, freeze_parameters=True):\n",
    "        r\"\"\"Calculate logmel spectrogram using pytorch. The mel filter bank is \n",
    "        the pytorch implementation of as librosa.filters.mel \n",
    "        \"\"\"\n",
    "        super(LogmelFilterBank, self).__init__()\n",
    "\n",
    "        self.is_log = is_log\n",
    "        self.ref = ref\n",
    "        self.amin = amin\n",
    "        self.top_db = top_db\n",
    "        if fmax == None:\n",
    "            fmax = sr//2\n",
    "\n",
    "        self.melW = librosa.filters.mel(sr=sr, n_fft=n_fft, n_mels=n_mels,\n",
    "            fmin=fmin, fmax=fmax).T\n",
    "        # (n_fft // 2 + 1, mel_bins)\n",
    "\n",
    "        self.melW = nn.Parameter(torch.Tensor(self.melW))\n",
    "\n",
    "        if freeze_parameters:\n",
    "            for param in self.parameters():\n",
    "                param.requires_grad = False\n",
    "\n",
    "    def forward(self, input):\n",
    "        r\"\"\"Calculate (log) mel spectrogram from spectrogram.\n",
    "\n",
    "        Args:\n",
    "            input: (*, n_fft), spectrogram\n",
    "        \n",
    "        Returns: \n",
    "            output: (*, mel_bins), (log) mel spectrogram\n",
    "        \"\"\"\n",
    "\n",
    "        # Mel spectrogram\n",
    "        mel_spectrogram = torch.matmul(input, self.melW)\n",
    "        # (*, mel_bins)\n",
    "\n",
    "        # Logmel spectrogram\n",
    "        if self.is_log:\n",
    "            output = self.power_to_db(mel_spectrogram)\n",
    "        else:\n",
    "            output = mel_spectrogram\n",
    "\n",
    "        return output\n",
    "\n",
    "\n",
    "    def power_to_db(self, input):\n",
    "        r\"\"\"Power to db, this function is the pytorch implementation of \n",
    "        librosa.power_to_lb\n",
    "        \"\"\"\n",
    "        ref_value = self.ref\n",
    "        log_spec = 10.0 * torch.log10(torch.clamp(input, min=self.amin, max=np.inf))\n",
    "        log_spec -= 10.0 * np.log10(np.maximum(self.amin, ref_value))\n",
    "\n",
    "        if self.top_db is not None:\n",
    "            if self.top_db < 0:\n",
    "                raise librosa.util.exceptions.ParameterError('top_db must be non-negative')\n",
    "            log_spec = torch.clamp(log_spec, min=log_spec.max().item() - self.top_db, max=np.inf)\n",
    "\n",
    "        return log_spec\n",
    "\n",
    "\n",
    "class Enframe(nn.Module):\n",
    "    def __init__(self, frame_length=2048, hop_length=512):\n",
    "        r\"\"\"Enframe a time sequence. This function is the pytorch implementation \n",
    "        of librosa.util.frame\n",
    "        \"\"\"\n",
    "        super(Enframe, self).__init__()\n",
    "\n",
    "        self.enframe_conv = nn.Conv1d(in_channels=1, out_channels=frame_length,\n",
    "            kernel_size=frame_length, stride=hop_length,\n",
    "            padding=0, bias=False)\n",
    "\n",
    "        self.enframe_conv.weight.data = torch.Tensor(torch.eye(frame_length)[:, None, :])\n",
    "        self.enframe_conv.weight.requires_grad = False\n",
    "\n",
    "    def forward(self, input):\n",
    "        r\"\"\"Enframe signals into frames.\n",
    "        Args:\n",
    "            input: (batch_size, samples)\n",
    "        \n",
    "        Returns: \n",
    "            output: (batch_size, window_length, frames_num)\n",
    "        \"\"\"\n",
    "        output = self.enframe_conv(input[:, None, :])\n",
    "        return output\n",
    "\n",
    "\n",
    "    def power_to_db(self, input):\n",
    "        r\"\"\"Power to db, this function is the pytorch implementation of \n",
    "        librosa.power_to_lb.\n",
    "        \"\"\"\n",
    "        ref_value = self.ref\n",
    "        log_spec = 10.0 * torch.log10(torch.clamp(input, min=self.amin, max=np.inf))\n",
    "        log_spec -= 10.0 * np.log10(np.maximum(self.amin, ref_value))\n",
    "\n",
    "        if self.top_db is not None:\n",
    "            if self.top_db < 0:\n",
    "                raise librosa.util.exceptions.ParameterError('top_db must be non-negative')\n",
    "            log_spec = torch.clamp(log_spec, min=log_spec.max() - self.top_db, max=np.inf)\n",
    "\n",
    "        return log_spec\n",
    "\n",
    "\n",
    "class Scalar(nn.Module):\n",
    "    def __init__(self, scalar, freeze_parameters):\n",
    "        super(Scalar, self).__init__()\n",
    "\n",
    "        self.scalar_mean = Parameter(torch.Tensor(scalar['mean']))\n",
    "        self.scalar_std = Parameter(torch.Tensor(scalar['std']))\n",
    "\n",
    "        if freeze_parameters:\n",
    "            for param in self.parameters():\n",
    "                param.requires_grad = False\n",
    "\n",
    "    def forward(self, input):\n",
    "        return (input - self.scalar_mean) / self.scalar_std\n",
    "\n",
    "\n",
    "def debug(select, device):\n",
    "    \"\"\"Compare numpy + librosa and torchlibrosa results. For debug. \n",
    "\n",
    "    Args:\n",
    "        select: 'dft' | 'logmel'\n",
    "        device: 'cpu' | 'cuda'\n",
    "    \"\"\"\n",
    "\n",
    "    if select == 'dft':\n",
    "        n = 10\n",
    "        norm = None     # None | 'ortho'\n",
    "        np.random.seed(0)\n",
    "\n",
    "        # Data\n",
    "        np_data = np.random.uniform(-1, 1, n)\n",
    "        pt_data = torch.Tensor(np_data)\n",
    "\n",
    "        # Numpy FFT\n",
    "        np_fft = np.fft.fft(np_data, norm=norm)\n",
    "        np_ifft = np.fft.ifft(np_fft, norm=norm)\n",
    "        np_rfft = np.fft.rfft(np_data, norm=norm)\n",
    "        np_irfft = np.fft.ifft(np_rfft, norm=norm)\n",
    "\n",
    "        # Pytorch FFT\n",
    "        obj = DFT(n, norm)\n",
    "        pt_dft = obj.dft(pt_data, torch.zeros_like(pt_data))\n",
    "        pt_idft = obj.idft(pt_dft[0], pt_dft[1])\n",
    "        pt_rdft = obj.rdft(pt_data)\n",
    "        pt_irdft = obj.irdft(pt_rdft[0], pt_rdft[1])\n",
    "\n",
    "        print('Comparing librosa and pytorch implementation of DFT. All numbers '\n",
    "            'below should be close to 0.')\n",
    "        print(np.mean((np.abs(np.real(np_fft) - pt_dft[0].cpu().numpy()))))\n",
    "        print(np.mean((np.abs(np.imag(np_fft) - pt_dft[1].cpu().numpy()))))\n",
    "\n",
    "        print(np.mean((np.abs(np.real(np_ifft) - pt_idft[0].cpu().numpy()))))\n",
    "        print(np.mean((np.abs(np.imag(np_ifft) - pt_idft[1].cpu().numpy()))))\n",
    "\n",
    "        print(np.mean((np.abs(np.real(np_rfft) - pt_rdft[0].cpu().numpy()))))\n",
    "        print(np.mean((np.abs(np.imag(np_rfft) - pt_rdft[1].cpu().numpy()))))\n",
    "\n",
    "        print(np.mean(np.abs(np_data - pt_irdft.cpu().numpy())))\n",
    "\n",
    "    elif select == 'stft':\n",
    "        device = torch.device(device)\n",
    "        np.random.seed(0)\n",
    "\n",
    "        # Spectrogram parameters (the same as librosa.stft)\n",
    "        sample_rate = 22050\n",
    "        data_length = sample_rate * 1\n",
    "        n_fft = 2048\n",
    "        hop_length = 512\n",
    "        win_length = 2048\n",
    "        window = 'hann'\n",
    "        center = True\n",
    "        pad_mode = 'reflect'\n",
    "\n",
    "        # Data\n",
    "        np_data = np.random.uniform(-1, 1, data_length)\n",
    "        pt_data = torch.Tensor(np_data).to(device)\n",
    "\n",
    "        # Numpy stft matrix\n",
    "        np_stft_matrix = librosa.stft(y=np_data, n_fft=n_fft,\n",
    "            hop_length=hop_length, window=window, center=center).T\n",
    "\n",
    "        # Pytorch stft matrix\n",
    "        pt_stft_extractor = STFT(n_fft=n_fft, hop_length=hop_length,\n",
    "            win_length=win_length, window=window, center=center, pad_mode=pad_mode,\n",
    "            freeze_parameters=True)\n",
    "\n",
    "        pt_stft_extractor.to(device)\n",
    "\n",
    "        (pt_stft_real, pt_stft_imag) = pt_stft_extractor.forward(pt_data[None, :])\n",
    "\n",
    "        print('Comparing librosa and pytorch implementation of STFT & ISTFT. \\\n",
    "            All numbers below should be close to 0.')\n",
    "        print(np.mean(np.abs(np.real(np_stft_matrix) - pt_stft_real.data.cpu().numpy()[0, 0])))\n",
    "        print(np.mean(np.abs(np.imag(np_stft_matrix) - pt_stft_imag.data.cpu().numpy()[0, 0])))\n",
    "\n",
    "        # Numpy istft\n",
    "        np_istft_s = librosa.istft(stft_matrix=np_stft_matrix.T,\n",
    "            hop_length=hop_length, window=window, center=center, length=data_length)\n",
    "\n",
    "        # Pytorch istft\n",
    "        pt_istft_extractor = ISTFT(n_fft=n_fft, hop_length=hop_length,\n",
    "            win_length=win_length, window=window, center=center, pad_mode=pad_mode,\n",
    "            freeze_parameters=True)\n",
    "        pt_istft_extractor.to(device)\n",
    "\n",
    "        # Recover from real and imag part\n",
    "        pt_istft_s = pt_istft_extractor.forward(pt_stft_real, pt_stft_imag, data_length)[0, :]\n",
    "\n",
    "        # Recover from magnitude and phase\n",
    "        (pt_stft_mag, cos, sin) = magphase(pt_stft_real, pt_stft_imag)\n",
    "        pt_istft_s2 = pt_istft_extractor.forward(pt_stft_mag * cos, pt_stft_mag * sin, data_length)[0, :]\n",
    "\n",
    "        print(np.mean(np.abs(np_istft_s - pt_istft_s.data.cpu().numpy())))\n",
    "        print(np.mean(np.abs(np_data - pt_istft_s.data.cpu().numpy())))\n",
    "        print(np.mean(np.abs(np_data - pt_istft_s2.data.cpu().numpy())))\n",
    "\n",
    "    elif select == 'logmel':\n",
    "        dtype = np.complex64\n",
    "        device = torch.device(device)\n",
    "        np.random.seed(0)\n",
    "\n",
    "        # Spectrogram parameters (the same as librosa.stft)\n",
    "        sample_rate = 22050\n",
    "        data_length = sample_rate * 1\n",
    "        n_fft = 2048\n",
    "        hop_length = 512\n",
    "        win_length = 2048\n",
    "        window = 'hann'\n",
    "        center = True\n",
    "        pad_mode = 'reflect'\n",
    "\n",
    "        # Mel parameters (the same as librosa.feature.melspectrogram)\n",
    "        n_mels = 128\n",
    "        fmin = 0.\n",
    "        fmax = sample_rate / 2.0\n",
    "\n",
    "        # Power to db parameters (the same as default settings of librosa.power_to_db\n",
    "        ref = 1.0\n",
    "        amin = 1e-10\n",
    "        top_db = 80.0\n",
    "\n",
    "        # Data\n",
    "        np_data = np.random.uniform(-1, 1, data_length)\n",
    "        pt_data = torch.Tensor(np_data).to(device)\n",
    "\n",
    "        print('Comparing librosa and pytorch implementation of logmel '\n",
    "            'spectrogram. All numbers below should be close to 0.')\n",
    "\n",
    "        # Numpy librosa\n",
    "        np_stft_matrix = librosa.stft(y=np_data, n_fft=n_fft, hop_length=hop_length,\n",
    "            win_length=win_length, window=window, center=center, dtype=dtype,\n",
    "            pad_mode=pad_mode)\n",
    "\n",
    "        np_pad = np.pad(np_data, int(n_fft // 2), mode=pad_mode)\n",
    "\n",
    "        np_melW = librosa.filters.mel(sr=sample_rate, n_fft=n_fft, n_mels=n_mels,\n",
    "            fmin=fmin, fmax=fmax).T\n",
    "\n",
    "        np_mel_spectrogram = np.dot(np.abs(np_stft_matrix.T) ** 2, np_melW)\n",
    "\n",
    "        np_logmel_spectrogram = librosa.power_to_db(\n",
    "            np_mel_spectrogram, ref=ref, amin=amin, top_db=top_db)\n",
    "\n",
    "        # Pytorch\n",
    "        stft_extractor = STFT(n_fft=n_fft, hop_length=hop_length,\n",
    "            win_length=win_length, window=window, center=center, pad_mode=pad_mode,\n",
    "            freeze_parameters=True)\n",
    "\n",
    "        logmel_extractor = LogmelFilterBank(sr=sample_rate, n_fft=n_fft,\n",
    "            n_mels=n_mels, fmin=fmin, fmax=fmax, ref=ref, amin=amin,\n",
    "            top_db=top_db, freeze_parameters=True)\n",
    "\n",
    "        stft_extractor.to(device)\n",
    "        logmel_extractor.to(device)\n",
    "\n",
    "        pt_pad = F.pad(pt_data[None, None, :], pad=(n_fft // 2, n_fft // 2), mode=pad_mode)[0, 0]\n",
    "        print(np.mean(np.abs(np_pad - pt_pad.cpu().numpy())))\n",
    "\n",
    "        pt_stft_matrix_real = stft_extractor.conv_real(pt_pad[None, None, :])[0]\n",
    "        pt_stft_matrix_imag = stft_extractor.conv_imag(pt_pad[None, None, :])[0]\n",
    "        print(np.mean(np.abs(np.real(np_stft_matrix) - pt_stft_matrix_real.data.cpu().numpy())))\n",
    "        print(np.mean(np.abs(np.imag(np_stft_matrix) - pt_stft_matrix_imag.data.cpu().numpy())))\n",
    "\n",
    "        # Spectrogram\n",
    "        spectrogram_extractor = Spectrogram(n_fft=n_fft, hop_length=hop_length,\n",
    "            win_length=win_length, window=window, center=center, pad_mode=pad_mode,\n",
    "            freeze_parameters=True)\n",
    "\n",
    "        spectrogram_extractor.to(device)\n",
    "\n",
    "        pt_spectrogram = spectrogram_extractor.forward(pt_data[None, :])\n",
    "        pt_mel_spectrogram = torch.matmul(pt_spectrogram, logmel_extractor.melW)\n",
    "        print(np.mean(np.abs(np_mel_spectrogram - pt_mel_spectrogram.data.cpu().numpy()[0, 0])))\n",
    "\n",
    "        # Log mel spectrogram\n",
    "        pt_logmel_spectrogram = logmel_extractor.forward(pt_spectrogram)\n",
    "        print(np.mean(np.abs(np_logmel_spectrogram - pt_logmel_spectrogram[0, 0].data.cpu().numpy())))\n",
    "\n",
    "    elif select == 'enframe':\n",
    "        device = torch.device(device)\n",
    "        np.random.seed(0)\n",
    "\n",
    "        # Spectrogram parameters (the same as librosa.stft)\n",
    "        sample_rate = 22050\n",
    "        data_length = sample_rate * 1\n",
    "        hop_length = 512\n",
    "        win_length = 2048\n",
    "\n",
    "        # Data\n",
    "        np_data = np.random.uniform(-1, 1, data_length)\n",
    "        pt_data = torch.Tensor(np_data).to(device)\n",
    "\n",
    "        print('Comparing librosa and pytorch implementation of '\n",
    "            'librosa.util.frame. All numbers below should be close to 0.')\n",
    "\n",
    "        # Numpy librosa\n",
    "        np_frames = librosa.util.frame(np_data, frame_length=win_length,\n",
    "            hop_length=hop_length)\n",
    "\n",
    "        # Pytorch\n",
    "        pt_frame_extractor = Enframe(frame_length=win_length, hop_length=hop_length)\n",
    "        pt_frame_extractor.to(device)\n",
    "\n",
    "        pt_frames = pt_frame_extractor(pt_data[None, :])\n",
    "        print(np.mean(np.abs(np_frames - pt_frames.data.cpu().numpy())))\n",
    "\n",
    "    elif select == 'default':\n",
    "        device = torch.device(device)\n",
    "        np.random.seed(0)\n",
    "\n",
    "        # Spectrogram parameters (the same as librosa.stft)\n",
    "        sample_rate = 22050\n",
    "        data_length = sample_rate * 1\n",
    "        hop_length = 512\n",
    "        win_length = 2048\n",
    "\n",
    "        # Mel parameters (the same as librosa.feature.melspectrogram)\n",
    "        n_mels = 128\n",
    "\n",
    "        # Data\n",
    "        np_data = np.random.uniform(-1, 1, data_length)\n",
    "        pt_data = torch.Tensor(np_data).to(device)\n",
    "\n",
    "        feature_extractor = nn.Sequential(\n",
    "            Spectrogram(\n",
    "                hop_length=hop_length,\n",
    "                win_length=win_length,\n",
    "            ), LogmelFilterBank(\n",
    "                sr=sample_rate,\n",
    "                n_mels=n_mels,\n",
    "                is_log=False, #Default is true\n",
    "            ))\n",
    "\n",
    "        feature_extractor.to(device)\n",
    "\n",
    "        print(\n",
    "            'Comparing default mel spectrogram from librosa to the pytorch implementation.'\n",
    "        )\n",
    "\n",
    "        # Numpy librosa\n",
    "        np_melspect = librosa.feature.melspectrogram(np_data,\n",
    "                                                     hop_length=hop_length,\n",
    "                                                     sr=sample_rate,\n",
    "                                                     win_length=win_length,\n",
    "                                                     n_mels=n_mels).T\n",
    "        #Pytorch\n",
    "        pt_melspect = feature_extractor(pt_data[None, :]).squeeze()\n",
    "        passed = np.allclose(pt_melspect.data.to('cpu').numpy(), np_melspect)\n",
    "        print(f\"Passed? {passed}\")\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "class DropStripes(nn.Module):\n",
    "    def __init__(self, dim, drop_width, stripes_num):\n",
    "        \"\"\"Drop stripes. \n",
    "\n",
    "        Args:\n",
    "          dim: int, dimension along which to drop\n",
    "          drop_width: int, maximum width of stripes to drop\n",
    "          stripes_num: int, how many stripes to drop\n",
    "        \"\"\"\n",
    "        super(DropStripes, self).__init__()\n",
    "\n",
    "        assert dim in [2, 3]    # dim 2: time; dim 3: frequency\n",
    "\n",
    "        self.dim = dim\n",
    "        self.drop_width = drop_width\n",
    "        self.stripes_num = stripes_num\n",
    "\n",
    "    def forward(self, input):\n",
    "        \"\"\"input: (batch_size, channels, time_steps, freq_bins)\"\"\"\n",
    "\n",
    "        assert input.ndimension() == 4\n",
    "\n",
    "        if self.training is False:\n",
    "            return input\n",
    "\n",
    "        else:\n",
    "            batch_size = input.shape[0]\n",
    "            total_width = input.shape[self.dim]\n",
    "\n",
    "            for n in range(batch_size):\n",
    "                self.transform_slice(input[n], total_width)\n",
    "\n",
    "            return input\n",
    "\n",
    "\n",
    "    def transform_slice(self, e, total_width):\n",
    "        \"\"\"e: (channels, time_steps, freq_bins)\"\"\"\n",
    "\n",
    "        for _ in range(self.stripes_num):\n",
    "            distance = torch.randint(low=0, high=self.drop_width, size=(1,))[0]\n",
    "            bgn = torch.randint(low=0, high=total_width - distance, size=(1,))[0]\n",
    "\n",
    "            if self.dim == 2:\n",
    "                e[:, bgn : bgn + distance, :] = 0\n",
    "            elif self.dim == 3:\n",
    "                e[:, :, bgn : bgn + distance] = 0\n",
    "\n",
    "\n",
    "class SpecAugmentation(nn.Module):\n",
    "    def __init__(self, time_drop_width, time_stripes_num, freq_drop_width, \n",
    "        freq_stripes_num):\n",
    "        \"\"\"Spec augmetation. \n",
    "        [ref] Park, D.S., Chan, W., Zhang, Y., Chiu, C.C., Zoph, B., Cubuk, E.D. \n",
    "        and Le, Q.V., 2019. Specaugment: A simple data augmentation method \n",
    "        for automatic speech recognition. arXiv preprint arXiv:1904.08779.\n",
    "\n",
    "        Args:\n",
    "          time_drop_width: int\n",
    "          time_stripes_num: int\n",
    "          freq_drop_width: int\n",
    "          freq_stripes_num: int\n",
    "        \"\"\"\n",
    "\n",
    "        super(SpecAugmentation, self).__init__()\n",
    "\n",
    "        self.time_dropper = DropStripes(dim=2, drop_width=time_drop_width, \n",
    "            stripes_num=time_stripes_num)\n",
    "\n",
    "        self.freq_dropper = DropStripes(dim=3, drop_width=freq_drop_width, \n",
    "            stripes_num=freq_stripes_num)\n",
    "\n",
    "    def forward(self, input):\n",
    "        x = self.time_dropper(input)\n",
    "        x = self.freq_dropper(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(9726, 5)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>filename</th>\n",
       "      <th>name</th>\n",
       "      <th>date</th>\n",
       "      <th>id</th>\n",
       "      <th>path</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>H14_20230419_194500</td>\n",
       "      <td>H14</td>\n",
       "      <td>20230419</td>\n",
       "      <td>194500</td>\n",
       "      <td>/root/projects/BirdClef2025/data/train_soundsc...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>H8920_20230506_232500</td>\n",
       "      <td>H8920</td>\n",
       "      <td>20230506</td>\n",
       "      <td>232500</td>\n",
       "      <td>/root/projects/BirdClef2025/data/train_soundsc...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>H74_20230511_091000</td>\n",
       "      <td>H74</td>\n",
       "      <td>20230511</td>\n",
       "      <td>091000</td>\n",
       "      <td>/root/projects/BirdClef2025/data/train_soundsc...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>H02_20230509_020000</td>\n",
       "      <td>H02</td>\n",
       "      <td>20230509</td>\n",
       "      <td>020000</td>\n",
       "      <td>/root/projects/BirdClef2025/data/train_soundsc...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>H94_20230518_144500</td>\n",
       "      <td>H94</td>\n",
       "      <td>20230518</td>\n",
       "      <td>144500</td>\n",
       "      <td>/root/projects/BirdClef2025/data/train_soundsc...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                filename   name      date      id  \\\n",
       "0    H14_20230419_194500    H14  20230419  194500   \n",
       "1  H8920_20230506_232500  H8920  20230506  232500   \n",
       "2    H74_20230511_091000    H74  20230511  091000   \n",
       "3    H02_20230509_020000    H02  20230509  020000   \n",
       "4    H94_20230518_144500    H94  20230518  144500   \n",
       "\n",
       "                                                path  \n",
       "0  /root/projects/BirdClef2025/data/train_soundsc...  \n",
       "1  /root/projects/BirdClef2025/data/train_soundsc...  \n",
       "2  /root/projects/BirdClef2025/data/train_soundsc...  \n",
       "3  /root/projects/BirdClef2025/data/train_soundsc...  \n",
       "4  /root/projects/BirdClef2025/data/train_soundsc...  "
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train = pd.read_csv(CFG.train_path)\n",
    "CFG.num_classes = len(df_train.primary_label.unique())\n",
    "df_test = pd.DataFrame(\n",
    "     [(path.stem, *path.stem.split(\"_\"), path) for path in Path(CFG.test_path).glob(\"*.ogg\")],\n",
    "    columns = [\"filename\", \"name\" ,\"date\",\"id\", \"path\"]\n",
    ")\n",
    "print(df_test.shape)\n",
    "df_test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "# preprocess\n",
    "def compute_melspec(y, sr, n_mels, fmin, fmax):\n",
    "    \"\"\"\n",
    "    Computes a mel-spectrogram and puts it at decibel scale\n",
    "    Arguments:\n",
    "        y {np array} -- signal\n",
    "        params {AudioParams} -- Parameters to use for the spectrogram. Expected to have the attributes sr, n_mels, f_min, f_max\n",
    "    Returns:\n",
    "        np array -- Mel-spectrogram\n",
    "    \"\"\"\n",
    "    melspec = lb.feature.melspectrogram(\n",
    "        y=y, sr=sr, n_mels=n_mels, fmin=fmin, fmax=fmax,\n",
    "        win_length=CFG.window_size,hop_length=CFG.hop_size,center=True,\n",
    "        n_fft=CFG.window_size,pad_mode='reflect',window='hann'\n",
    "    )\n",
    "\n",
    "    melspec = lb.power_to_db(melspec,amin=1e-10,ref=1.0,top_db=None).astype(np.float32)\n",
    "    return melspec\n",
    "\n",
    "def mono_to_color(X, eps=1e-6, mean=None, std=None):\n",
    "    mean = mean or X.mean()\n",
    "    std = std or X.std()\n",
    "    X = (X - mean) / (std + eps)\n",
    "    \n",
    "    _min, _max = X.min(), X.max()\n",
    "\n",
    "    if (_max - _min) > eps:\n",
    "        V = np.clip(X, _min, _max)\n",
    "        V = 255 * (V - _min) / (_max - _min)\n",
    "        V = V.astype(np.uint8)\n",
    "    else:\n",
    "        V = np.zeros_like(X, dtype=np.uint8)\n",
    "\n",
    "    return V\n",
    "\n",
    "def crop_or_pad(y, length, is_train=True, start=None):\n",
    "    if len(y) < length:\n",
    "        y = np.concatenate([y, np.zeros(length - len(y))])\n",
    "        \n",
    "        n_repeats = length // len(y)\n",
    "        epsilon = length % len(y)\n",
    "        \n",
    "        y = np.concatenate([y]*n_repeats + [y[:epsilon]])\n",
    "        \n",
    "    elif len(y) > length:\n",
    "        if not is_train:\n",
    "            start = start or 0\n",
    "        else:\n",
    "            start = start or np.random.randint(len(y) - length)\n",
    "\n",
    "        y = y[start:start + length]\n",
    "\n",
    "    return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset\n",
    "import torch\n",
    "import numpy as np\n",
    "from torch.utils.data import Dataset,DataLoader\n",
    "    \n",
    "\n",
    "\n",
    "class TestDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, \n",
    "                 df: pd.DataFrame, \n",
    "                 clip: np.ndarray,\n",
    "                 config=None,\n",
    "                 model=None\n",
    "                ):\n",
    "        \n",
    "        self.df = df\n",
    "        self.clip = clip\n",
    "        self.config = config\n",
    "        self.model = model\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx: int):\n",
    "\n",
    "        sample = self.df.loc[idx, :]\n",
    "        row_id = sample.row_id\n",
    "        \n",
    "        end_seconds = int(sample.seconds)\n",
    "        start_seconds = int(end_seconds - 20)\n",
    "        \n",
    "        # if start_seconds !=0 and end_seconds != 60:\n",
    "        #     start_seconds = start_seconds - 2.5\n",
    "        #     end_seconds = end_seconds + 2.5\n",
    "        #     y = self.clip[int(self.config.sample_rate * start_seconds) : int(self.config.sample_rate * end_seconds)].astype(np.float32)\n",
    "        # else:\n",
    "        #     y = self.clip[int(self.config.sample_rate * start_seconds) : int(self.config.sample_rate * end_seconds)].astype(np.float32)\n",
    "        #     y = np.pad(y,(int(2.5*self.config.sample_rate),int(2.5*self.config.sample_rate)),mode='constant',constant_values=0)\n",
    "        y = self.clip[int(self.config.sample_rate * start_seconds) : int(self.config.sample_rate * end_seconds)].astype(np.float32)\n",
    "        image = self.model.get_mel_gram(torch.from_numpy(y).unsqueeze(0))\n",
    "        \n",
    "        return {\n",
    "            \"image\": image,\n",
    "            \"row_id\": row_id,\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(9726, 5)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>filename</th>\n",
       "      <th>name</th>\n",
       "      <th>date</th>\n",
       "      <th>id</th>\n",
       "      <th>path</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>H14_20230419_194500</td>\n",
       "      <td>H14</td>\n",
       "      <td>20230419</td>\n",
       "      <td>194500</td>\n",
       "      <td>/root/projects/BirdClef2025/data/train_soundsc...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>H8920_20230506_232500</td>\n",
       "      <td>H8920</td>\n",
       "      <td>20230506</td>\n",
       "      <td>232500</td>\n",
       "      <td>/root/projects/BirdClef2025/data/train_soundsc...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>H74_20230511_091000</td>\n",
       "      <td>H74</td>\n",
       "      <td>20230511</td>\n",
       "      <td>091000</td>\n",
       "      <td>/root/projects/BirdClef2025/data/train_soundsc...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>H02_20230509_020000</td>\n",
       "      <td>H02</td>\n",
       "      <td>20230509</td>\n",
       "      <td>020000</td>\n",
       "      <td>/root/projects/BirdClef2025/data/train_soundsc...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>H94_20230518_144500</td>\n",
       "      <td>H94</td>\n",
       "      <td>20230518</td>\n",
       "      <td>144500</td>\n",
       "      <td>/root/projects/BirdClef2025/data/train_soundsc...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                filename   name      date      id  \\\n",
       "0    H14_20230419_194500    H14  20230419  194500   \n",
       "1  H8920_20230506_232500  H8920  20230506  232500   \n",
       "2    H74_20230511_091000    H74  20230511  091000   \n",
       "3    H02_20230509_020000    H02  20230509  020000   \n",
       "4    H94_20230518_144500    H94  20230518  144500   \n",
       "\n",
       "                                                path  \n",
       "0  /root/projects/BirdClef2025/data/train_soundsc...  \n",
       "1  /root/projects/BirdClef2025/data/train_soundsc...  \n",
       "2  /root/projects/BirdClef2025/data/train_soundsc...  \n",
       "3  /root/projects/BirdClef2025/data/train_soundsc...  \n",
       "4  /root/projects/BirdClef2025/data/train_soundsc...  "
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train = pd.read_csv(CFG.train_path)\n",
    "CFG.num_classes = len(df_train.primary_label.unique())\n",
    "df_test = pd.DataFrame(\n",
    "     [(path.stem, *path.stem.split(\"_\"), path) for path in Path(CFG.test_path).glob(\"*.ogg\")],\n",
    "    columns = [\"filename\", \"name\" ,\"date\",\"id\", \"path\"]\n",
    ")\n",
    "print(df_test.shape)\n",
    "df_test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# model\n",
    "import timm\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "from torchaudio.transforms import MelSpectrogram,AmplitudeToDB\n",
    "\n",
    "class AttBlockV2(nn.Module):\n",
    "    def __init__(self,\n",
    "                 in_features: int,\n",
    "                 out_features: int,\n",
    "                 activation=\"linear\"):\n",
    "        super().__init__()\n",
    "\n",
    "        self.activation = activation\n",
    "        self.att = nn.Conv1d(\n",
    "            in_channels=in_features,\n",
    "            out_channels=out_features,\n",
    "            kernel_size=1,\n",
    "            stride=1,\n",
    "            padding=0,\n",
    "            bias=True)\n",
    "        self.cla = nn.Conv1d(\n",
    "            in_channels=in_features,\n",
    "            out_channels=out_features,\n",
    "            kernel_size=1,\n",
    "            stride=1,\n",
    "            padding=0,\n",
    "            bias=True)\n",
    "\n",
    "        self.init_weights()\n",
    "\n",
    "    def init_weights(self):\n",
    "        init_layer(self.att)\n",
    "        init_layer(self.cla)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x: (n_samples, n_in, n_time)\n",
    "        norm_att = torch.softmax(torch.tanh(self.att(x)), dim=-1)\n",
    "        cla = self.nonlinear_transform(self.cla(x))\n",
    "        x = torch.sum(norm_att * cla, dim=2)\n",
    "        return x, norm_att, cla\n",
    "\n",
    "    def nonlinear_transform(self, x):\n",
    "        if self.activation == 'linear':\n",
    "            return x\n",
    "        elif self.activation == 'sigmoid':\n",
    "            return torch.sigmoid(x)\n",
    "\n",
    "class NormalizeMelSpec(nn.Module):\n",
    "    def __init__(self, eps=1e-6):\n",
    "        super().__init__()\n",
    "        self.eps = eps\n",
    "\n",
    "    def forward(self, X):\n",
    "        mean = X.mean((1, 2), keepdim=True)\n",
    "        std = X.std((1, 2), keepdim=True)\n",
    "        Xstd = (X - mean) / (std + self.eps)\n",
    "        norm_min, norm_max = Xstd.min(-1)[0].min(-1)[0], Xstd.max(-1)[0].max(-1)[0]\n",
    "        fix_ind = (norm_max - norm_min) > self.eps * torch.ones_like(\n",
    "            (norm_max - norm_min)\n",
    "        )\n",
    "        V = torch.zeros_like(Xstd)\n",
    "        if fix_ind.sum():\n",
    "            V_fix = Xstd[fix_ind]\n",
    "            norm_max_fix = norm_max[fix_ind, None, None]\n",
    "            norm_min_fix = norm_min[fix_ind, None, None]\n",
    "            V_fix = torch.max(\n",
    "                torch.min(V_fix, norm_max_fix),\n",
    "                norm_min_fix,\n",
    "            )\n",
    "            # print(V_fix.shape, norm_min_fix.shape, norm_max_fix.shape)\n",
    "            V_fix = (V_fix - norm_min_fix) / (norm_max_fix - norm_min_fix)\n",
    "            V[fix_ind] = V_fix\n",
    "        return V\n",
    "\n",
    "def gem_freq(x, p=3, eps=1e-6):\n",
    "    return F.avg_pool2d(x.clamp(min=eps).pow(p), (x.size(-2), 1)).pow(1.0 / p)\n",
    "\n",
    "\n",
    "class GeMFreq(nn.Module):\n",
    "    def __init__(self, p=3, eps=1e-6):\n",
    "        super().__init__()\n",
    "        self.p = torch.nn.Parameter(torch.ones(1) * p)\n",
    "        self.eps = eps\n",
    "\n",
    "    def forward(self, x):\n",
    "        return gem_freq(x, p=self.p, eps=self.eps)\n",
    "\n",
    "def init_layer(layer):\n",
    "    \"\"\"Initialize a Linear or Convolutional layer. \"\"\"\n",
    "    nn.init.xavier_uniform_(layer.weight)\n",
    " \n",
    "    if hasattr(layer, 'bias'):\n",
    "        if layer.bias is not None:\n",
    "            layer.bias.data.fill_(0.)\n",
    "            \n",
    "    \n",
    "def init_bn(bn):\n",
    "    \"\"\"Initialize a Batchnorm layer. \"\"\"\n",
    "    bn.bias.data.fill_(0.)\n",
    "    bn.weight.data.fill_(1.)\n",
    "\n",
    "\n",
    "def gem(x, p=3, eps=1e-6):\n",
    "    return F.avg_pool2d(x.clamp(min=eps).pow(p), (x.size(-2), x.size(-1))).pow(1./p)\n",
    "\n",
    "class GeM(nn.Module):\n",
    "    def __init__(self, p=3, eps=1e-6, p_trainable=False):\n",
    "        super(GeM,self).__init__()\n",
    "        if p_trainable:\n",
    "            self.p = nn.Parameter(torch.ones(1)*p)\n",
    "        else:\n",
    "            self.p = p\n",
    "        self.eps = eps\n",
    "\n",
    "    def forward(self, x):\n",
    "        ret = gem(x, p=self.p, eps=self.eps)   \n",
    "        ret = torch.flatten(ret,start_dim=1)\n",
    "        return ret\n",
    "    def __repr__(self):\n",
    "        return self.__class__.__name__ + '(' + 'p=' + '{:.4f}'.format(self.p.data.tolist()[0]) + ', ' + 'eps=' + str(self.eps) + ')'\n",
    "\n",
    "\n",
    "class BirdClefSEDModel(nn.Module):\n",
    "    def __init__(self, model_name=CFG.model, num_classes = CFG.num_classes, pretrained = CFG.pretrained,p=0.5):\n",
    "        super().__init__()\n",
    "        self.num_classes = num_classes\n",
    "        self.backbone = timm.create_model(model_name, pretrained=pretrained)\n",
    "        if 'effi' in CFG.model:\n",
    "            self.backbone.global_pool = nn.Identity()\n",
    "            self.backbone.classifier = nn.Identity()\n",
    "        elif 'eca' in CFG.model:\n",
    "            self.backbone.head.fc = nn.Identity()\n",
    "        if CFG.use_fsr:\n",
    "            self.backbone.conv_stem.stride = (1,1)\n",
    "        self.fc_audioset = nn.Linear(self.backbone.num_features, num_classes, bias=True)\n",
    "        self.pooling = GeM()\n",
    "        self.SpecAug = SpecAugmentation(time_drop_width=64, time_stripes_num=2,freq_drop_width=8, freq_stripes_num=2)\n",
    "        self.use_spec_aug = CFG.use_spec_aug\n",
    "        self.bn0 = nn.BatchNorm2d(CFG.mel_bins)\n",
    "    \n",
    "        # Spectrogram extractor\n",
    "        self.spectrogram_extractor = MelSpectrogram(\n",
    "            sample_rate=CFG.sample_rate,\n",
    "            n_fft=2048,\n",
    "            win_length=CFG.window_size,\n",
    "            hop_length=CFG.hop_size,\n",
    "            f_min=CFG.fmin,\n",
    "            f_max=CFG.fmax,\n",
    "            pad=0,\n",
    "            n_mels=CFG.mel_bins,\n",
    "            power=2,\n",
    "            normalized=False,\n",
    "        )\n",
    "        # Logmel feature extractor\n",
    "        self.logmel_extractor = AmplitudeToDB(top_db=None)\n",
    "        self.normlize = NormalizeMelSpec()\n",
    "        self.infer_period = CFG.infer_duration\n",
    "        self.train_period = CFG.duration\n",
    "        self.init_weight()\n",
    "\n",
    "    def init_weight(self):\n",
    "        init_bn(self.bn0)\n",
    "\n",
    "        \n",
    "    def get_mel_gram(self,audios):\n",
    "        \"\"\"\n",
    "        Input: (batch_size, data_length)\"\"\"\n",
    "        x = self.spectrogram_extractor(audios) # (batch_size,freq_bins time_steps)\n",
    "        x = self.logmel_extractor(x) \n",
    "        x = self.normlize(x)\n",
    "        x = x.permute(0,2,1)# (batch_size,time_steps, mel_bins)\n",
    "        return x\n",
    "    \n",
    "    def forward(self,images):\n",
    "        # b c f t\n",
    "        if CFG.use_spec_aug and self.training:\n",
    "            if np.random.uniform(0,1)>CFG.p_spec_aug:\n",
    "                images = self.SpecAug(images)\n",
    "        x = self.backbone.forward_features(images) #  bs,1,t,f\n",
    "        x = torch.mean(x,dim=3) # pooling freq bs,c,t\n",
    "        \n",
    "        (x1,_) = torch.max(x,dim=2) # bs,c\n",
    "        x2 = torch.mean(x,dim=2) # bs,c\n",
    "        x = x1+x2\n",
    "        x = F.dropout(x,p=0.5,training=self.training)\n",
    "        x = self.fc_audioset(x)\n",
    "\n",
    "        return x\n",
    "    \n",
    "class BirdClefModel(nn.Module):\n",
    "    def __init__(self, model_name=CFG.model, num_classes = CFG.num_classes, pretrained = CFG.pretrained,p=0.5):\n",
    "        super().__init__()\n",
    "        self.num_classes = num_classes\n",
    "        self.backbone = timm.create_model(model_name, pretrained=pretrained)\n",
    "        if 'effi' in CFG.model:\n",
    "            self.backbone.global_pool = nn.Identity()\n",
    "            self.backbone.classifier = nn.Identity()\n",
    "        elif 'eca' in CFG.model:\n",
    "            self.backbone.head.fc = nn.Identity()\n",
    "        if CFG.use_fsr:\n",
    "            self.backbone.conv_stem.stride = (1,1)\n",
    "        self.fc_audioset = nn.Linear(self.backbone.num_features, num_classes, bias=True)\n",
    "        self.pooling = GeM()\n",
    "        self.SpecAug = SpecAugmentation(time_drop_width=64, time_stripes_num=2,freq_drop_width=8, freq_stripes_num=2)\n",
    "        self.use_spec_aug = CFG.use_spec_aug\n",
    "        self.bn0 = nn.BatchNorm2d(CFG.mel_bins)\n",
    "        \n",
    "        # Spectrogram extractor\n",
    "        self.spectrogram_extractor = MelSpectrogram(\n",
    "            sample_rate=CFG.sample_rate,\n",
    "            n_fft=2048,\n",
    "            win_length=CFG.window_size,\n",
    "            hop_length=CFG.hop_size,\n",
    "            f_min=CFG.fmin,\n",
    "            f_max=CFG.fmax,\n",
    "            pad=0,\n",
    "            n_mels=CFG.mel_bins,\n",
    "            power=2,\n",
    "            normalized=False,\n",
    "        )\n",
    "        # Logmel feature extractor\n",
    "        self.logmel_extractor = AmplitudeToDB(top_db=None)\n",
    "        self.normlize = NormalizeMelSpec()\n",
    "        self.infer_period = CFG.infer_duration\n",
    "        self.train_period = CFG.duration\n",
    "        self.init_weight()\n",
    "\n",
    "    def init_weight(self):\n",
    "        init_bn(self.bn0)\n",
    "\n",
    "        \n",
    "    def get_mel_gram(self,audios):\n",
    "        \"\"\"\n",
    "        Input: (batch_size, data_length)\"\"\"\n",
    "        x = self.spectrogram_extractor(audios) # (batch_size,freq_bins time_steps)\n",
    "        x = self.logmel_extractor(x) \n",
    "        x = self.normlize(x)\n",
    "        x = x.permute(0,2,1)# (batch_size,time_steps, mel_bins)\n",
    "        return x\n",
    "    \n",
    "    def forward(self,images):\n",
    "        # b c f t\n",
    "        if CFG.use_spec_aug and self.training:\n",
    "            if np.random.uniform(0,1)>CFG.p_spec_aug:\n",
    "                images = self.SpecAug(images)\n",
    "        if self.training:\n",
    "            if images.shape[2]%4!=0:\n",
    "                images = F.pad(images,(0,0,4-images.shape[2]%4,0))\n",
    "            images = torch.cat(torch.chunk(images,chunks=4,dim=2),dim=0) #  4*bs,1,t,f\n",
    "        x = self.backbone.forward_features(images) #  4*bs,1,t,f\n",
    "\n",
    "        if self.training:\n",
    "            x = torch.cat(torch.chunk(x,chunks=4,dim=0),dim=2)\n",
    "        x = self.pooling(x)\n",
    "        x = self.fc_audioset(x)\n",
    "        return x\n",
    "    \n",
    "class BirdClefSEDAttModel(nn.Module):\n",
    "    def __init__(self, model_name=CFG.model, num_classes = CFG.num_classes, pretrained = CFG.pretrained,p=0.5):\n",
    "        super().__init__()\n",
    "        self.num_classes = num_classes\n",
    "        self.backbone = timm.create_model(model_name, pretrained=pretrained)\n",
    "        if 'effi' in model_name:\n",
    "            self.backbone.global_pool = nn.Identity()\n",
    "            self.backbone.classifier = nn.Identity()\n",
    "        elif 'eca' in model_name:\n",
    "            self.backbone.head.fc = nn.Identity()\n",
    "        if CFG.use_fsr:\n",
    "            self.backbone.conv_stem.stride = (1,1)\n",
    "        self.pooling = GeM()\n",
    "        self.SpecAug = SpecAugmentation(time_drop_width=64, time_stripes_num=2,freq_drop_width=8, freq_stripes_num=2)\n",
    "        self.use_spec_aug = CFG.use_spec_aug\n",
    "        self.bn0 = nn.BatchNorm2d(CFG.mel_bins)\n",
    "    \n",
    "        # Spectrogram extractor\n",
    "        self.spectrogram_extractor = MelSpectrogram(\n",
    "            sample_rate=CFG.sample_rate,\n",
    "            n_fft=2048,\n",
    "            win_length=CFG.window_size,\n",
    "            hop_length=CFG.hop_size,\n",
    "            f_min=CFG.fmin,\n",
    "            f_max=CFG.fmax,\n",
    "            pad=0,\n",
    "            n_mels=CFG.mel_bins,\n",
    "            power=2,\n",
    "            normalized=False,\n",
    "        )\n",
    "        # Logmel feature extractor\n",
    "        self.logmel_extractor = AmplitudeToDB(top_db=None)\n",
    "        self.normlize = NormalizeMelSpec()\n",
    "\n",
    "        self.fc1 = nn.Linear(self.backbone.num_features, self.backbone.num_features, bias=True)\n",
    "        self.att_block = AttBlockV2(\n",
    "            self.backbone.num_features, num_classes, activation=\"sigmoid\")\n",
    "\n",
    "        self.init_weight()\n",
    "\n",
    "    def init_weight(self):\n",
    "        init_bn(self.bn0)\n",
    "        init_layer(self.fc1)\n",
    "    \n",
    "    def get_mel_gram(self,audios):\n",
    "        \"\"\"\n",
    "        Input: (batch_size, data_length)\"\"\"\n",
    "        x = self.spectrogram_extractor(audios) # (batch_size,freq_bins time_steps)\n",
    "        x = self.logmel_extractor(x) \n",
    "        x = self.normlize(x)\n",
    "        x = x.permute(0,2,1)# (batch_size,time_steps, mel_bins)\n",
    "        return x\n",
    "\n",
    "    def forward(self, images):\n",
    "        # b c f t\n",
    "        if CFG.use_spec_aug and self.training:\n",
    "            if np.random.uniform(0,1)>CFG.p_spec_aug:\n",
    "                images = self.SpecAug(images)\n",
    "\n",
    "        x = images.transpose(1, 3)\n",
    "        x = self.bn0(x)\n",
    "        x = x.transpose(1, 3)\n",
    "\n",
    "        x = x.transpose(2, 3)\n",
    "\n",
    "        x = self.backbone.forward_features(x)\n",
    "        \n",
    "        # Aggregate in frequency axis\n",
    "        x = torch.mean(x, dim=2)\n",
    "\n",
    "        x1 = F.max_pool1d(x, kernel_size=3, stride=1, padding=1)\n",
    "        x2 = F.avg_pool1d(x, kernel_size=3, stride=1, padding=1)\n",
    "        x = x1 + x2\n",
    "\n",
    "        x = F.dropout(x, p=0.3, training=self.training)\n",
    "        x = x.transpose(1, 2)\n",
    "        x = F.relu(self.fc1(x),inplace=False)\n",
    "        x = x.transpose(1, 2)\n",
    "        x = F.dropout(x, p=0.3, training=self.training)\n",
    "\n",
    "        (clipwise_output, norm_att, segmentwise_output) = self.att_block(x)\n",
    "\n",
    "        maxframewise_output = nn.AdaptiveMaxPool1d(1)(segmentwise_output).squeeze(2)\n",
    "        output_dict = {\n",
    "            \"clipwise_output\": clipwise_output,\n",
    "            \"framewise_output\":segmentwise_output,\n",
    "            \"maxframewise_output\":maxframewise_output\n",
    "        }\n",
    "\n",
    "        return output_dict\n",
    "    \n",
    "# model_efb3 = BirdClefSEDAttModel(model_name=\"tf_efficientnet_b3_ns\",num_classes=CFG.num_classes,pretrained=CFG.pretrained)\n",
    "# model_efv2b2 = BirdClefSEDAttModel(model_name=\"tf_efficientnetv2_b2\",num_classes=CFG.num_classes,pretrained=CFG.pretrained)\n",
    "# model_seresnext = BirdClefSEDAttModel(model_name=\"seresnext26d_32x4d\",num_classes=CFG.num_classes,pretrained=CFG.pretrained)\n",
    "# model_seresnext.load_state_dict(torch.load('/kaggle/input/birdclefv3/seresnext_seclabel.pt',map_location=torch.device('cpu')))\n",
    "# model_nfnetl0 = BirdClefSEDAttModel(model_name=\"eca_nfnet_l0\",num_classes=CFG.num_classes,pretrained=CFG.pretrained)\n",
    "# model_nfnetl0.load_state_dict(torch.load('/kaggle/input/birdclefv3/ecanfnetl0_seclabel.pt',map_location=torch.device('cpu')))\n",
    "# model_efv2b2 = BirdClefSEDAttModel(model_name=\"tf_efficientnetv2_b2\",num_classes=CFG.num_classes,pretrained=CFG.pretrained)\n",
    "# model_efv2b2.load_state_dict(torch.load('/kaggle/input/birdclefv3/efv2_finetune_alldata.pt',map_location=torch.device('cpu')))\n",
    "\n",
    "model_efv2b3 = BirdClefSEDAttModel(model_name=\"tf_efficientnetv2_b3\",num_classes=CFG.num_classes,pretrained=CFG.pretrained)\n",
    "model_efv2b3.load_state_dict(torch.load('/root/projects/BirdClef2025/BirdCLEF2023-30th-place-solution-master/logs/2025-03-28T16:48/saved_model.pt',map_location=torch.device('cpu')))\n",
    "# model_resnest = BirdClefSEDAttModel(model_name=\"resnest26d\",num_classes=CFG.num_classes,pretrained=CFG.pretrained)\n",
    "# model_resnest.load_state_dict(torch.load('/kaggle/input/birdclefv3/resnest26d_alldata_mel128.pt',map_location=torch.device('cpu')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "import random\n",
    "\n",
    "seconds = [i for i in range(20, 80, 20)]\n",
    "# model_seresnext.to('cpu')\n",
    "# model_efv2b3.to('cpu')\n",
    "# model_efv2b2.to('cpu')\n",
    "# model_efv2b3.to('cpu')\n",
    "# model_nfnetl0.to('cpu')\n",
    "# model_resnest.to('cpu')\n",
    "# model_seresnext.eval()\n",
    "model_efv2b3\n",
    "model_efv2b3.eval()\n",
    "# model_efv2b2.eval()\n",
    "# model_nfnetl0.eval()\n",
    "# model_resnest.eval()\n",
    "models = [model_efv2b3.cuda()]\n",
    "# models = [model_seresnext]\n",
    "\n",
    "all_audios = list(Path(\"/root/projects/BirdClef2025/data/train_soundscapes_20s/\").glob(\"*.ogg\"))\n",
    "audios_len = len(all_audios)\n",
    "\n",
    "dataset = TestDataset(\n",
    "    audios = all_audios,\n",
    "    config=CFG,\n",
    "    model=models[0]\n",
    ")\n",
    "\n",
    "dataloader = torch.utils.data.DataLoader(\n",
    "        dataset,\n",
    "        batch_size=64, \n",
    "        num_workers=8,\n",
    "        drop_last=False,\n",
    "        shuffle=False,\n",
    "        pin_memory=True\n",
    "    )\n",
    "\n",
    "eval_preds = []\n",
    "for idx,audio in tqdm(enumerate(dataloader),total=len(dataloader)):\n",
    "    image = models[0].get_mel_gram(audio.cuda().squeeze(1)).unsqueeze(1)\n",
    "    image = torch.repeat_interleave(image,repeats=3,dim=1)\n",
    "    # eval_output\n",
    "    models[0].eval()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        output_eval = np.zeros((image.shape[0],206))\n",
    "        for model in models:\n",
    "            output_temp = model(image)\n",
    "            output_eval += (output_temp['clipwise_output'].detach().cpu().numpy() + output_temp['maxframewise_output'].detach().cpu().numpy())/2\n",
    "        output_eval = output_eval/len(models)    \n",
    "    eval_preds.append(output_eval)\n",
    "eval_preds_array = np.vstack(eval_preds)\n",
    "# noisy_preds_array = np.array(noisy_preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 80/80 [02:34<00:00,  1.93s/it]\n",
      "/tmp/ipykernel_4179255/3758704790.py:32: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  sub_df = pd.concat([sub_df,df]).reset_index(drop=True)\n",
      "100%|██████████| 80/80 [02:36<00:00,  1.95s/it]\n",
      "/tmp/ipykernel_4179255/3758704790.py:32: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  sub_df = pd.concat([sub_df,df]).reset_index(drop=True)\n",
      "100%|██████████| 80/80 [02:38<00:00,  1.98s/it]\n",
      "/tmp/ipykernel_4179255/3758704790.py:32: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  sub_df = pd.concat([sub_df,df]).reset_index(drop=True)\n",
      "100%|██████████| 80/80 [02:43<00:00,  2.04s/it]\n",
      "/tmp/ipykernel_4179255/3758704790.py:32: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  sub_df = pd.concat([sub_df,df]).reset_index(drop=True)\n",
      "100%|██████████| 80/80 [02:45<00:00,  2.07s/it]\n",
      "/tmp/ipykernel_4179255/3758704790.py:32: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  sub_df = pd.concat([sub_df,df]).reset_index(drop=True)\n",
      "100%|██████████| 80/80 [02:43<00:00,  2.04s/it]\n",
      "/tmp/ipykernel_4179255/3758704790.py:32: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  sub_df = pd.concat([sub_df,df]).reset_index(drop=True)\n",
      "100%|██████████| 80/80 [02:44<00:00,  2.06s/it]\n",
      "/tmp/ipykernel_4179255/3758704790.py:32: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  sub_df = pd.concat([sub_df,df]).reset_index(drop=True)\n",
      "100%|██████████| 80/80 [02:43<00:00,  2.05s/it]\n",
      "/tmp/ipykernel_4179255/3758704790.py:32: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  sub_df = pd.concat([sub_df,df]).reset_index(drop=True)\n",
      "100%|██████████| 80/80 [02:46<00:00,  2.08s/it]\n",
      "/tmp/ipykernel_4179255/3758704790.py:32: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  sub_df = pd.concat([sub_df,df]).reset_index(drop=True)\n",
      "100%|██████████| 80/80 [02:45<00:00,  2.07s/it]\n",
      "/tmp/ipykernel_4179255/3758704790.py:32: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  sub_df = pd.concat([sub_df,df]).reset_index(drop=True)\n",
      "100%|██████████| 80/80 [02:46<00:00,  2.08s/it]\n",
      "/tmp/ipykernel_4179255/3758704790.py:32: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  sub_df = pd.concat([sub_df,df]).reset_index(drop=True)\n",
      "100%|██████████| 80/80 [02:49<00:00,  2.12s/it]\n",
      "/tmp/ipykernel_4179255/3758704790.py:32: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  sub_df = pd.concat([sub_df,df]).reset_index(drop=True)\n",
      "100%|██████████| 80/80 [02:49<00:00,  2.12s/it]\n",
      "/tmp/ipykernel_4179255/3758704790.py:32: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  sub_df = pd.concat([sub_df,df]).reset_index(drop=True)\n",
      "100%|██████████| 80/80 [02:49<00:00,  2.12s/it]\n",
      "/tmp/ipykernel_4179255/3758704790.py:32: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  sub_df = pd.concat([sub_df,df]).reset_index(drop=True)\n",
      "100%|██████████| 80/80 [02:52<00:00,  2.15s/it]\n",
      "/tmp/ipykernel_4179255/3758704790.py:32: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  sub_df = pd.concat([sub_df,df]).reset_index(drop=True)\n",
      "100%|██████████| 80/80 [02:52<00:00,  2.16s/it]\n",
      "/tmp/ipykernel_4179255/3758704790.py:32: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  sub_df = pd.concat([sub_df,df]).reset_index(drop=True)\n",
      "100%|██████████| 80/80 [02:53<00:00,  2.17s/it]\n",
      "/tmp/ipykernel_4179255/3758704790.py:32: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  sub_df = pd.concat([sub_df,df]).reset_index(drop=True)\n",
      "100%|██████████| 80/80 [02:54<00:00,  2.18s/it]\n",
      "/tmp/ipykernel_4179255/3758704790.py:32: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  sub_df = pd.concat([sub_df,df]).reset_index(drop=True)\n",
      "100%|██████████| 80/80 [02:54<00:00,  2.18s/it]\n",
      "/tmp/ipykernel_4179255/3758704790.py:32: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  sub_df = pd.concat([sub_df,df]).reset_index(drop=True)\n",
      "100%|██████████| 80/80 [02:56<00:00,  2.21s/it]\n",
      "/tmp/ipykernel_4179255/3758704790.py:32: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  sub_df = pd.concat([sub_df,df]).reset_index(drop=True)\n",
      "100%|██████████| 80/80 [02:55<00:00,  2.20s/it]\n",
      "/tmp/ipykernel_4179255/3758704790.py:32: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  sub_df = pd.concat([sub_df,df]).reset_index(drop=True)\n",
      "100%|██████████| 80/80 [02:57<00:00,  2.22s/it]\n",
      "/tmp/ipykernel_4179255/3758704790.py:32: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  sub_df = pd.concat([sub_df,df]).reset_index(drop=True)\n",
      "100%|██████████| 80/80 [02:58<00:00,  2.23s/it]\n",
      "/tmp/ipykernel_4179255/3758704790.py:32: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  sub_df = pd.concat([sub_df,df]).reset_index(drop=True)\n",
      "100%|██████████| 80/80 [02:59<00:00,  2.24s/it]\n",
      "/tmp/ipykernel_4179255/3758704790.py:32: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  sub_df = pd.concat([sub_df,df]).reset_index(drop=True)\n",
      "100%|██████████| 80/80 [03:00<00:00,  2.25s/it]\n",
      "/tmp/ipykernel_4179255/3758704790.py:32: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  sub_df = pd.concat([sub_df,df]).reset_index(drop=True)\n",
      "100%|██████████| 80/80 [03:01<00:00,  2.27s/it]\n",
      "/tmp/ipykernel_4179255/3758704790.py:32: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  sub_df = pd.concat([sub_df,df]).reset_index(drop=True)\n",
      "100%|██████████| 80/80 [03:02<00:00,  2.28s/it]\n",
      "/tmp/ipykernel_4179255/3758704790.py:32: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  sub_df = pd.concat([sub_df,df]).reset_index(drop=True)\n",
      "100%|██████████| 80/80 [03:05<00:00,  2.32s/it]\n",
      "/tmp/ipykernel_4179255/3758704790.py:32: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  sub_df = pd.concat([sub_df,df]).reset_index(drop=True)\n",
      "100%|██████████| 80/80 [03:06<00:00,  2.33s/it]\n",
      "/tmp/ipykernel_4179255/3758704790.py:32: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  sub_df = pd.concat([sub_df,df]).reset_index(drop=True)\n",
      "100%|██████████| 80/80 [03:13<00:00,  2.41s/it]\n",
      "/tmp/ipykernel_4179255/3758704790.py:32: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  sub_df = pd.concat([sub_df,df]).reset_index(drop=True)\n",
      "100%|██████████| 80/80 [03:09<00:00,  2.37s/it]\n",
      "/tmp/ipykernel_4179255/3758704790.py:32: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  sub_df = pd.concat([sub_df,df]).reset_index(drop=True)\n",
      "100%|██████████| 80/80 [03:08<00:00,  2.36s/it]\n",
      "/tmp/ipykernel_4179255/3758704790.py:32: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  sub_df = pd.concat([sub_df,df]).reset_index(drop=True)\n",
      "100%|██████████| 80/80 [03:07<00:00,  2.35s/it]\n",
      "/tmp/ipykernel_4179255/3758704790.py:32: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  sub_df = pd.concat([sub_df,df]).reset_index(drop=True)\n",
      "100%|██████████| 80/80 [03:10<00:00,  2.38s/it]\n",
      "/tmp/ipykernel_4179255/3758704790.py:32: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  sub_df = pd.concat([sub_df,df]).reset_index(drop=True)\n",
      "100%|██████████| 80/80 [03:13<00:00,  2.41s/it]\n",
      "/tmp/ipykernel_4179255/3758704790.py:32: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  sub_df = pd.concat([sub_df,df]).reset_index(drop=True)\n",
      "100%|██████████| 80/80 [03:12<00:00,  2.41s/it]\n",
      "/tmp/ipykernel_4179255/3758704790.py:32: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  sub_df = pd.concat([sub_df,df]).reset_index(drop=True)\n",
      "100%|██████████| 80/80 [03:13<00:00,  2.42s/it]\n",
      "/tmp/ipykernel_4179255/3758704790.py:32: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  sub_df = pd.concat([sub_df,df]).reset_index(drop=True)\n",
      "100%|██████████| 80/80 [03:12<00:00,  2.41s/it]\n",
      "/tmp/ipykernel_4179255/3758704790.py:32: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  sub_df = pd.concat([sub_df,df]).reset_index(drop=True)\n",
      "100%|██████████| 80/80 [03:16<00:00,  2.45s/it]\n",
      "/tmp/ipykernel_4179255/3758704790.py:32: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  sub_df = pd.concat([sub_df,df]).reset_index(drop=True)\n",
      "100%|██████████| 80/80 [03:14<00:00,  2.43s/it]\n",
      "/tmp/ipykernel_4179255/3758704790.py:32: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  sub_df = pd.concat([sub_df,df]).reset_index(drop=True)\n",
      "100%|██████████| 80/80 [03:18<00:00,  2.48s/it]\n",
      "/tmp/ipykernel_4179255/3758704790.py:32: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  sub_df = pd.concat([sub_df,df]).reset_index(drop=True)\n",
      "100%|██████████| 80/80 [03:19<00:00,  2.50s/it]\n",
      "/tmp/ipykernel_4179255/3758704790.py:32: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  sub_df = pd.concat([sub_df,df]).reset_index(drop=True)\n",
      "100%|██████████| 80/80 [03:19<00:00,  2.50s/it]\n",
      "/tmp/ipykernel_4179255/3758704790.py:32: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  sub_df = pd.concat([sub_df,df]).reset_index(drop=True)\n",
      "100%|██████████| 80/80 [03:24<00:00,  2.55s/it]\n",
      "/tmp/ipykernel_4179255/3758704790.py:32: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  sub_df = pd.concat([sub_df,df]).reset_index(drop=True)\n",
      "100%|██████████| 80/80 [03:25<00:00,  2.56s/it]\n",
      "/tmp/ipykernel_4179255/3758704790.py:32: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  sub_df = pd.concat([sub_df,df]).reset_index(drop=True)\n",
      "100%|██████████| 80/80 [03:28<00:00,  2.61s/it]\n",
      "/tmp/ipykernel_4179255/3758704790.py:32: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  sub_df = pd.concat([sub_df,df]).reset_index(drop=True)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[102], line 26\u001b[0m\n\u001b[1;32m     24\u001b[0m num_rows \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(pred)\n\u001b[1;32m     25\u001b[0m row_ids \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfile\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m(i\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m20\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(num_rows)]\n\u001b[0;32m---> 26\u001b[0m df \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mDataFrame\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcolumns\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mrow_id\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43mbird_cols\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     28\u001b[0m df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfilename\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfile\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(num_rows)]\n\u001b[1;32m     29\u001b[0m df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrow_id\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m row_ids\n",
      "File \u001b[0;32m~/anaconda3/envs/cibmtr/lib/python3.10/site-packages/pandas/core/frame.py:876\u001b[0m, in \u001b[0;36mDataFrame.__init__\u001b[0;34m(self, data, index, columns, dtype, copy)\u001b[0m\n\u001b[1;32m    867\u001b[0m             mgr \u001b[38;5;241m=\u001b[39m ndarray_to_mgr(\n\u001b[1;32m    868\u001b[0m                 data,\n\u001b[1;32m    869\u001b[0m                 index,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    873\u001b[0m                 typ\u001b[38;5;241m=\u001b[39mmanager,\n\u001b[1;32m    874\u001b[0m             )\n\u001b[1;32m    875\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 876\u001b[0m         mgr \u001b[38;5;241m=\u001b[39m \u001b[43mdict_to_mgr\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    877\u001b[0m \u001b[43m            \u001b[49m\u001b[43m{\u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    878\u001b[0m \u001b[43m            \u001b[49m\u001b[43mindex\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    879\u001b[0m \u001b[43m            \u001b[49m\u001b[43mcolumns\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mcolumns\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mis\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mdefault_index\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    880\u001b[0m \u001b[43m            \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    881\u001b[0m \u001b[43m            \u001b[49m\u001b[43mtyp\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmanager\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    882\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    883\u001b[0m \u001b[38;5;66;03m# For data is scalar\u001b[39;00m\n\u001b[1;32m    884\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    885\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m index \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m columns \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m~/anaconda3/envs/cibmtr/lib/python3.10/site-packages/pandas/core/internals/construction.py:462\u001b[0m, in \u001b[0;36mdict_to_mgr\u001b[0;34m(data, index, columns, dtype, typ, copy)\u001b[0m\n\u001b[1;32m    460\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m midxs:\n\u001b[1;32m    461\u001b[0m         arr \u001b[38;5;241m=\u001b[39m sanitize_array(arrays\u001b[38;5;241m.\u001b[39miat[i], index, dtype\u001b[38;5;241m=\u001b[39mdtype)\n\u001b[0;32m--> 462\u001b[0m         \u001b[43marrays\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43miat\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;241m=\u001b[39m arr\n\u001b[1;32m    463\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    464\u001b[0m     \u001b[38;5;66;03m# GH#1783\u001b[39;00m\n\u001b[1;32m    465\u001b[0m     nan_dtype \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mdtype(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mobject\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/anaconda3/envs/cibmtr/lib/python3.10/site-packages/pandas/core/indexing.py:2542\u001b[0m, in \u001b[0;36m_ScalarAccessIndexer.__setitem__\u001b[0;34m(self, key, value)\u001b[0m\n\u001b[1;32m   2539\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(key) \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mndim:\n\u001b[1;32m   2540\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNot enough indexers for scalar access (setting)!\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m-> 2542\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mobj\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_set_value\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalue\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvalue\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtakeable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_takeable\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/cibmtr/lib/python3.10/site-packages/pandas/core/series.py:1447\u001b[0m, in \u001b[0;36mSeries._set_value\u001b[0;34m(self, label, value, takeable)\u001b[0m\n\u001b[1;32m   1444\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1445\u001b[0m     loc \u001b[38;5;241m=\u001b[39m label\n\u001b[0;32m-> 1447\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_set_values\u001b[49m\u001b[43m(\u001b[49m\u001b[43mloc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/cibmtr/lib/python3.10/site-packages/pandas/core/series.py:1419\u001b[0m, in \u001b[0;36mSeries._set_values\u001b[0;34m(self, key, value, warn)\u001b[0m\n\u001b[1;32m   1416\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(key, (Index, Series)):\n\u001b[1;32m   1417\u001b[0m     key \u001b[38;5;241m=\u001b[39m key\u001b[38;5;241m.\u001b[39m_values\n\u001b[0;32m-> 1419\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_mgr \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_mgr\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msetitem\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindexer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalue\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvalue\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mwarn\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mwarn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1420\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_maybe_update_cacher()\n",
      "File \u001b[0;32m~/anaconda3/envs/cibmtr/lib/python3.10/site-packages/pandas/core/internals/managers.py:415\u001b[0m, in \u001b[0;36mBaseBlockManager.setitem\u001b[0;34m(self, indexer, value, warn)\u001b[0m\n\u001b[1;32m    411\u001b[0m     \u001b[38;5;66;03m# No need to split if we either set all columns or on a single block\u001b[39;00m\n\u001b[1;32m    412\u001b[0m     \u001b[38;5;66;03m# manager\u001b[39;00m\n\u001b[1;32m    413\u001b[0m     \u001b[38;5;28mself\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcopy()\n\u001b[0;32m--> 415\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43msetitem\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindexer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mindexer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalue\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvalue\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/cibmtr/lib/python3.10/site-packages/pandas/core/internals/managers.py:363\u001b[0m, in \u001b[0;36mBaseBlockManager.apply\u001b[0;34m(self, f, align_keys, **kwargs)\u001b[0m\n\u001b[1;32m    361\u001b[0m         applied \u001b[38;5;241m=\u001b[39m b\u001b[38;5;241m.\u001b[39mapply(f, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    362\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 363\u001b[0m         applied \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mgetattr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mb\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mf\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    364\u001b[0m     result_blocks \u001b[38;5;241m=\u001b[39m extend_blocks(applied, result_blocks)\n\u001b[1;32m    366\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mtype\u001b[39m(\u001b[38;5;28mself\u001b[39m)\u001b[38;5;241m.\u001b[39mfrom_blocks(result_blocks, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maxes)\n",
      "File \u001b[0;32m~/anaconda3/envs/cibmtr/lib/python3.10/site-packages/pandas/core/internals/blocks.py:1418\u001b[0m, in \u001b[0;36mBlock.setitem\u001b[0;34m(self, indexer, value, using_cow)\u001b[0m\n\u001b[1;32m   1415\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdtype \u001b[38;5;241m==\u001b[39m _dtype_obj:\n\u001b[1;32m   1416\u001b[0m     \u001b[38;5;66;03m# TODO: avoid having to construct values[indexer]\u001b[39;00m\n\u001b[1;32m   1417\u001b[0m     vi \u001b[38;5;241m=\u001b[39m values[indexer]\n\u001b[0;32m-> 1418\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[43mlib\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mis_list_like\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvi\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[1;32m   1419\u001b[0m         \u001b[38;5;66;03m# checking lib.is_scalar here fails on\u001b[39;00m\n\u001b[1;32m   1420\u001b[0m         \u001b[38;5;66;03m#  test_iloc_setitem_custom_object\u001b[39;00m\n\u001b[1;32m   1421\u001b[0m         casted \u001b[38;5;241m=\u001b[39m setitem_datetimelike_compat(values, \u001b[38;5;28mlen\u001b[39m(vi), casted)\n\u001b[1;32m   1423\u001b[0m \u001b[38;5;28mself\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_maybe_copy(using_cow, inplace\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "all_audios = list(Path(\"/root/projects/BirdClef2025/data/train_soundscapes/\").glob(\"*.ogg\"))\n",
    "\n",
    "step = 80\n",
    "pesudo_df = pd.DataFrame({})\n",
    "for audios_ids in range(0,len(all_audios),step):\n",
    "    this_audios = all_audios[audios_ids:audios_ids+step]\n",
    "    with concurrent.futures.ThreadPoolExecutor(max_workers=8) as executor:\n",
    "        predictions = list(tqdm(executor.map(prediction_for_clip,this_audios),total=len(this_audios)))\n",
    "\n",
    "    filenames = df_test.iloc[audios_ids:audios_ids+step].filename.values.tolist()\n",
    "    bird_cols = list(pd.get_dummies(df_train['primary_label']).columns)\n",
    "\n",
    "    sub_df = pd.DataFrame(columns=['row_id']+bird_cols)\n",
    "\n",
    "    pred_temp = []\n",
    "    for pred in predictions:\n",
    "        pred_temp = pred_temp + pred\n",
    "    predictions = pred_temp\n",
    "\n",
    "    for i, file in enumerate(filenames):\n",
    "        pred = predictions[i]\n",
    "        num_rows = len(pred)\n",
    "        row_ids = [f'{file}_{(i+1)*20}' for i in range(num_rows)]\n",
    "        df = pd.DataFrame(columns=['row_id']+bird_cols)\n",
    "        \n",
    "        df['filename'] = [f'{file}' for i in range(num_rows)]\n",
    "        df['row_id'] = row_ids\n",
    "        df[bird_cols] = pred\n",
    "        \n",
    "        sub_df = pd.concat([sub_df,df]).reset_index(drop=True)\n",
    "    pesudo_df = pd.concat([pesudo_df,sub_df])\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pesudo_df['row_id'] = pesudo_df['row_id'].map(lambda x:'_'.join(x.split('_')[:-1]) + '_' + str(int(x.split('_')[-1])-20))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'compau'"
      ]
     },
     "execution_count": 140,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "test_audio,_ = sf.read('/root/projects/BirdClef2025/data/train_soundscapes_20s/H63_20230508_085500_40s.ogg')\n",
    "cut_audio = test_audio[0:20*32000]\n",
    "image = models[0].get_mel_gram(torch.from_numpy(cut_audio).unsqueeze(0).to(torch.float32)).unsqueeze(1)\n",
    "# plt.figure()\n",
    "# plt.imshow(image[0,0,...].detach().cpu().numpy())\n",
    "models[0].eval()\n",
    "image = torch.repeat_interleave(image,repeats=3,dim=1)\n",
    "res = models[0](image)\n",
    "plt.figure()\n",
    "plt.plot(res['clipwise_output'].detach().cpu().numpy()[0])\n",
    "plt.plot(res['maxframewise_output'].detach().cpu().numpy()[0])\n",
    "bird_cols[res['clipwise_output'].argmax().item()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "pesudo_df.to_csv('/root/projects/BirdClef2025/BirdCLEF2023-30th-place-solution-master/usefulFunc/pesudo_labelv4.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "read_csv() got an unexpected keyword argument 'index'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[117], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m pesudo_df \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_csv\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m/root/projects/BirdClef2025/BirdCLEF2023-30th-place-solution-master/usefulFunc/pesudo_labelv4.csv\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43mindex\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mTypeError\u001b[0m: read_csv() got an unexpected keyword argument 'index'"
     ]
    }
   ],
   "source": [
    "pesudo_df = pd.read_csv('/root/projects/BirdClef2025/BirdCLEF2023-30th-place-solution-master/usefulFunc/pesudo_labelv3.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-4.559218987486745"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred=0.01\n",
    "pred*np.log(1-pred)+(1-pred)*np.log(pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# pesudo_df = pd.read_csv('/root/projects/BirdClef2025/BirdCLEF2023-30th-place-solution-master/usefulFunc/pesudo_labelv2.csv')\n",
    "birds_col = pesudo_df.columns[0:-2]\n",
    "pred_list = []\n",
    "entropy_list = []\n",
    "\n",
    "for idx,pred in pesudo_df[pesudo_df.columns[:-2]].iterrows():\n",
    "    entropy = np.max(pred.values*np.log(1-pred.values) + (1-pred.values)*np.log(pred.values))\n",
    "    entropy_list.append(entropy)\n",
    "plt.hist(entropy_list)\n",
    "thred = np.quantile(entropy_list,0.1)\n",
    "pesudo_df_cut = pesudo_df.iloc[np.where(np.array(entropy_list)<thred)[0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "pesudo_df_cut.to_csv('/root/projects/BirdClef2025/BirdCLEF2023-30th-place-solution-master/usefulFunc/pesudo_labelv3_cut.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([2421.,    0.,    0.,    0.,    0.,    0.,    0.,    0.,    0.,\n",
       "         497.]),\n",
       " array([0.01123607, 0.10899136, 0.20674665, 0.30450194, 0.40225723,\n",
       "        0.50001252, 0.59776781, 0.6955231 , 0.79327838, 0.89103367,\n",
       "        0.98878896]),\n",
       " <BarContainer object of 10 artists>)"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.hist(pesudo_df[pesudo_df.columns[:-2]].iloc[np.where(np.array(entropy_list)<thred)[0]].max(axis=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1139490    0.000019\n",
       "1192948    0.000072\n",
       "1194042    0.000005\n",
       "126247     0.008306\n",
       "1346504    0.000144\n",
       "             ...   \n",
       "yehcar1    0.000015\n",
       "yelori1    0.000002\n",
       "yeofly1    0.000020\n",
       "yercac1    0.000008\n",
       "ywcpar     0.000015\n",
       "Name: 1, Length: 206, dtype: float64"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pesudo_df[pesudo_df.columns[:-2]].iloc[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "last_pesudo = pd.read_csv('/root/projects/BirdClef2025/BirdCLEF2023-30th-place-solution-master/usefulFunc/pesudo_labelv2.csv')\n",
    "pesudo_df['row_id'] = last_pesudo['row_id']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "pesudo_df['row_id'] = pesudo_df['row_id'].map(lambda x: '_'.join(x.split('_')[:-1])+'_'+str(int(x.split('_')[-1])-20))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_value = pesudo_df[pesudo_df.columns[2:-1]].values.reshape(-1,1)\n",
    "pred_value = pred_value[np.where(pred_value>0)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "pesudo_df.to_csv('/root/projects/BirdClef2025/BirdCLEF2023-30th-place-solution-master/usefulFunc/pesudo_labelv3.csv',index=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from joblib import Parallel, delayed\n",
    "\n",
    "src_path = '/root/projects/BirdClef2025/data/train_soundscapes/'\n",
    "def cut_audio(row):\n",
    "    filename = row['filename']\n",
    "    time = int(row['row_id'].split(\"_\")[-1])\n",
    "    src_path = '/root/projects/BirdClef2025/data/train_soundscapes/'\n",
    "    audio,sr = sf.read(src_path+filename+'.ogg')\n",
    "    audio = audio[int(sr*(time-10)):int(sr*time)]\n",
    "    # write to file\n",
    "    sf.write('/root/projects/BirdClef2025/data/train_soundscapes_10s/'+filename+'_'+str(time-10)+'s'+'.ogg',audio,sr)\n",
    "\n",
    "Parallel(n_jobs=16)(delayed(cut_audio)(row) for idx,row in tqdm(pesudo_df.iterrows(),total=len(pesudo_df)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "pesudo_df['filename'] = pesudo_df['filename'].apply(lambda x: '_'.join(x.split(\"_\")[:-1])+'_'+ str(int(x.split(\"_\")[-1])-20)+'s')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>primary_label</th>\n",
       "      <th>secondary_labels</th>\n",
       "      <th>type</th>\n",
       "      <th>filename</th>\n",
       "      <th>collection</th>\n",
       "      <th>rating</th>\n",
       "      <th>url</th>\n",
       "      <th>latitude</th>\n",
       "      <th>longitude</th>\n",
       "      <th>scientific_name</th>\n",
       "      <th>common_name</th>\n",
       "      <th>author</th>\n",
       "      <th>license</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1139490</td>\n",
       "      <td>['']</td>\n",
       "      <td>['']</td>\n",
       "      <td>1139490/CSA36385.ogg</td>\n",
       "      <td>CSA</td>\n",
       "      <td>0.0</td>\n",
       "      <td>http://colecciones.humboldt.org.co/rec/sonidos...</td>\n",
       "      <td>7.3206</td>\n",
       "      <td>-73.7128</td>\n",
       "      <td>Ragoniella pulchella</td>\n",
       "      <td>Ragoniella pulchella</td>\n",
       "      <td>Fabio A. Sarria-S</td>\n",
       "      <td>cc-by-nc-sa 4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1139490</td>\n",
       "      <td>['']</td>\n",
       "      <td>['']</td>\n",
       "      <td>1139490/CSA36389.ogg</td>\n",
       "      <td>CSA</td>\n",
       "      <td>0.0</td>\n",
       "      <td>http://colecciones.humboldt.org.co/rec/sonidos...</td>\n",
       "      <td>7.3206</td>\n",
       "      <td>-73.7128</td>\n",
       "      <td>Ragoniella pulchella</td>\n",
       "      <td>Ragoniella pulchella</td>\n",
       "      <td>Fabio A. Sarria-S</td>\n",
       "      <td>cc-by-nc-sa 4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1192948</td>\n",
       "      <td>['']</td>\n",
       "      <td>['']</td>\n",
       "      <td>1192948/CSA36358.ogg</td>\n",
       "      <td>CSA</td>\n",
       "      <td>0.0</td>\n",
       "      <td>http://colecciones.humboldt.org.co/rec/sonidos...</td>\n",
       "      <td>7.3791</td>\n",
       "      <td>-73.7313</td>\n",
       "      <td>Oxyprora surinamensis</td>\n",
       "      <td>Oxyprora surinamensis</td>\n",
       "      <td>Fabio A. Sarria-S</td>\n",
       "      <td>cc-by-nc-sa 4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1192948</td>\n",
       "      <td>['']</td>\n",
       "      <td>['']</td>\n",
       "      <td>1192948/CSA36366.ogg</td>\n",
       "      <td>CSA</td>\n",
       "      <td>0.0</td>\n",
       "      <td>http://colecciones.humboldt.org.co/rec/sonidos...</td>\n",
       "      <td>7.2800</td>\n",
       "      <td>-73.8582</td>\n",
       "      <td>Oxyprora surinamensis</td>\n",
       "      <td>Oxyprora surinamensis</td>\n",
       "      <td>Fabio A. Sarria-S</td>\n",
       "      <td>cc-by-nc-sa 4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1192948</td>\n",
       "      <td>['']</td>\n",
       "      <td>['']</td>\n",
       "      <td>1192948/CSA36373.ogg</td>\n",
       "      <td>CSA</td>\n",
       "      <td>0.0</td>\n",
       "      <td>http://colecciones.humboldt.org.co/rec/sonidos...</td>\n",
       "      <td>7.3791</td>\n",
       "      <td>-73.7313</td>\n",
       "      <td>Oxyprora surinamensis</td>\n",
       "      <td>Oxyprora surinamensis</td>\n",
       "      <td>Fabio A. Sarria-S</td>\n",
       "      <td>cc-by-nc-sa 4.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  primary_label secondary_labels  type              filename collection  \\\n",
       "0       1139490             ['']  ['']  1139490/CSA36385.ogg        CSA   \n",
       "1       1139490             ['']  ['']  1139490/CSA36389.ogg        CSA   \n",
       "2       1192948             ['']  ['']  1192948/CSA36358.ogg        CSA   \n",
       "3       1192948             ['']  ['']  1192948/CSA36366.ogg        CSA   \n",
       "4       1192948             ['']  ['']  1192948/CSA36373.ogg        CSA   \n",
       "\n",
       "   rating                                                url  latitude  \\\n",
       "0     0.0  http://colecciones.humboldt.org.co/rec/sonidos...    7.3206   \n",
       "1     0.0  http://colecciones.humboldt.org.co/rec/sonidos...    7.3206   \n",
       "2     0.0  http://colecciones.humboldt.org.co/rec/sonidos...    7.3791   \n",
       "3     0.0  http://colecciones.humboldt.org.co/rec/sonidos...    7.2800   \n",
       "4     0.0  http://colecciones.humboldt.org.co/rec/sonidos...    7.3791   \n",
       "\n",
       "   longitude        scientific_name            common_name             author  \\\n",
       "0   -73.7128   Ragoniella pulchella   Ragoniella pulchella  Fabio A. Sarria-S   \n",
       "1   -73.7128   Ragoniella pulchella   Ragoniella pulchella  Fabio A. Sarria-S   \n",
       "2   -73.7313  Oxyprora surinamensis  Oxyprora surinamensis  Fabio A. Sarria-S   \n",
       "3   -73.8582  Oxyprora surinamensis  Oxyprora surinamensis  Fabio A. Sarria-S   \n",
       "4   -73.7313  Oxyprora surinamensis  Oxyprora surinamensis  Fabio A. Sarria-S   \n",
       "\n",
       "           license  \n",
       "0  cc-by-nc-sa 4.0  \n",
       "1  cc-by-nc-sa 4.0  \n",
       "2  cc-by-nc-sa 4.0  \n",
       "3  cc-by-nc-sa 4.0  \n",
       "4  cc-by-nc-sa 4.0  "
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# example df\n",
    "df = pd.read_csv(CFG.train_path)\n",
    "df.head()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cibmtr",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
