{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
    "os.environ[\"HF_ENDPOINT\"] = \"https://hf-mirror.com\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-09 12:46:37.954726: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2025-04-09 12:46:37.996641: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2025-04-09 12:46:37.996661: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2025-04-09 12:46:37.998274: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2025-04-09 12:46:38.006164: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2025-04-09 12:46:38.779577: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "from birdnet import SpeciesPredictions, Species\n",
    "from birdnet import SpeciesPredictions, predict_species_within_audio_file,predict_species_within_audio_files_mp,predict_species_at_location_and_time\n",
    "import pandas as pd\n",
    "from glob import glob\n",
    "from collections import OrderedDict\n",
    "from matplotlib import pyplot as plt\n",
    "import soundfile as sf\n",
    "import numpy as np\n",
    "import tensorflow_hub as hub\n",
    "import tensorflow as tf\n",
    "import torch\n",
    "tf.experimental.numpy.experimental_enable_numpy_behavior()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-09 12:46:45.279777: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1929] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 21965 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 3090, pci bus id: 0000:3b:00.0, compute capability: 8.6\n",
      "2025-04-09 12:46:46.261695: I external/local_tsl/tsl/platform/default/subprocess.cc:304] Start cannot spawn child process: No such file or directory\n"
     ]
    }
   ],
   "source": [
    "ebird_name = pd.read_csv('/root/projects/BirdClef2025/data/train.csv')\n",
    "scientific_name_common_name = list(ebird_name['scientific_name'] +'_'+ ebird_name['common_name'])\n",
    "ebird_name_list = list(ebird_name['primary_label'])\n",
    "ebird_birdclef2025 = set(ebird_name['primary_label'])\n",
    "species_in_area = predict_species_at_location_and_time(6.76, -74.21)\n",
    "for ebird,item in zip(ebird_name_list.copy(),scientific_name_common_name.copy()):\n",
    "    if item not in species_in_area.keys():\n",
    "        scientific_name_common_name.remove(item)\n",
    "        ebird_name_list.remove(ebird)\n",
    "ebird_scientific_common = pd.DataFrame({'ebird':ebird_name_list,'scientifica_common':scientific_name_common_name})\n",
    "ebird_scientific_common = ebird_scientific_common.drop_duplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filelists = glob('/root/projects/BirdClef2025/data/train_audio/anhing/*.ogg')\n",
    "filelists.sort()\n",
    "predictions = SpeciesPredictions(predict_species_within_audio_file(Path('/root/projects/BirdClef2025/data/train_soundscapes_20s/H98_20230518_055000_40s.ogg'),species_filter=set(scientific_name_common_name)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: we can predict 143 species only!\n"
     ]
    }
   ],
   "source": [
    "bc_labels = pd.read_csv('/root/projects/BirdClef2025/bvc_model/assets/label.csv').iloc[:, 0].to_list()\n",
    "bc_labels_indices = range(len(bc_labels))\n",
    "\n",
    "primary_labels_map = dict(zip(bc_labels, bc_labels_indices))\n",
    "bvc_classes = [pl for pl in ebird_birdclef2025 if pl in bc_labels]\n",
    "bn_clasees = list(ebird_scientific_common['ebird'])\n",
    "\n",
    "birdclassifier_last = len(bc_labels)\n",
    "birdclassifier_indices = [primary_labels_map[i] for i in ebird_birdclef2025 if i in primary_labels_map]\n",
    "\n",
    "print(f'Note: we can predict {len(birdclassifier_indices)} species only!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-09 12:47:07.095059: I external/local_xla/xla/service/service.cc:168] XLA service 0x112deda0 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
      "2025-04-09 12:47:07.095090: I external/local_xla/xla/service/service.cc:176]   StreamExecutor device (0): NVIDIA GeForce RTX 3090, Compute Capability 8.6\n",
      "2025-04-09 12:47:07.363544: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:269] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n",
      "2025-04-09 12:47:07.372210: W tensorflow/compiler/tf2xla/kernels/assert_op.cc:38] Ignoring Assert operator jax2tf_infer_fn_/assert_equal_1/Assert/AssertGuard/Assert\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "I0000 00:00:1744202837.293430 3104654 device_compiler.h:186] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n",
      "W0000 00:00:1744202837.346914 3104654 graph_launch.cc:671] Fallback to op-by-op mode because memset node breaks graph update\n"
     ]
    }
   ],
   "source": [
    "model = hub.load('/root/projects/BirdClef2025/bvc_model')\n",
    "\n",
    "# Input: 5 seconds of silence as mono 32 kHz waveform samples.\n",
    "waveform = sf.read('/root/projects/BirdClef2025/data/train_audio/brtpar1/XC449467.ogg')[0][:5*32000]\n",
    "waveform = waveform[None,:].repeat(repeats=32,axis=0)\n",
    "waveform = tf.convert_to_tensor(waveform, dtype=tf.float32)\n",
    "# Run the model, check the output.\n",
    "model_outputs = model.infer_tf(waveform)['label']\n",
    "prob = tf.nn.sigmoid(model_outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-05 12:22:11.557680: W tensorflow/compiler/tf2xla/kernels/assert_op.cc:39] Ignoring Assert operator jax2tf_infer_fn_/assert_equal_1/Assert/AssertGuard/Assert\n"
     ]
    }
   ],
   "source": [
    "from concurrent.futures import ThreadPoolExecutor\n",
    "audio_list = glob('/root/projects/BirdClef2025/data/train_soundscapes/*')\n",
    "model = hub.load('/root/projects/BirdClef2025/bvc_model')\n",
    "def pred_bvc(audios):\n",
    "    waveform,_ = sf.read(audios)\n",
    "    if len(waveform.shape)!=1:\n",
    "        waveform = waveform[0,:]\n",
    "    waveform = waveform.reshape(12,-1)\n",
    "    model_outputs = model.infer_tf(waveform)['label']\n",
    "    prob = tf.nn.sigmoid(model_outputs).numpy()\n",
    "    audionames = [audios.replace('.ogg',f'_{(i+1)*5}.ogg') for i in range(prob.shape[0])]\n",
    "    return prob,audionames\n",
    "\n",
    "audionames_list = []\n",
    "probs_list = []\n",
    "with ThreadPoolExecutor(max_workers=8) as executor:\n",
    "    for prob,audionames in executor.map(pred_bvc, audio_list):\n",
    "        audionames_list += [audionames]\n",
    "        probs_list += [prob]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([-0.8620, -1.8620, -0.8620])\n",
      "tensor([-0.8620, -1.8620, -0.8620])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "demo = torch.tensor([2,1,2]).to(torch.float32)\n",
    "print(torch.log_softmax(demo, dim=0))\n",
    "print(torch.log(torch.softmax(demo, dim=0)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [],
   "source": [
    "audionames_lists = []\n",
    "probs_lists = []\n",
    "for audio in audionames_list:\n",
    "    audionames_lists+=audio\n",
    "for prob in probs_list:\n",
    "    probs_lists+=list(prob)\n",
    "probs_array = np.array(probs_lists)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bvc_pred_df = pd.DataFrame({'audios':audionames_lists})\n",
    "for name,index in zip(bvc_classes,birdclassifier_indices):\n",
    "    bvc_pred_df[name] = probs_array[:,index]\n",
    "# bvc_pred_df['probs_max'] = bvc_pred_df['probs'].map(lambda x:x.max())\n",
    "bvc_pred_df.to_csv('/root/projects/BirdClef2025/BirdCLEF2023-30th-place-solution-master/usefulFunc/pesudo_labelv7_bvc.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 336,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 9726/9726 [17:43<00:00,  9.15it/s]\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "bvc_pred_df['filename'] = bvc_pred_df['audios'].map(lambda x:'_'.join(x.split('/')[-1].split('_')[:-1])+'.ogg')\n",
    "bvc_pred_df_10 = pd.DataFrame(columns=bvc_pred_df.columns)\n",
    "for name,group in tqdm(bvc_pred_df.groupby('filename'),total=len(bvc_pred_df['filename'].unique())):\n",
    "    for step in range(0,12,2):\n",
    "        bvc_pred_df_10.loc[len(bvc_pred_df_10.index)] = [name.replace('.ogg',f'_{step*5}s.ogg')] + list(group[bvc_classes].iloc[step:step+2].max().values) + [name]\n",
    "bvc_pred_df_10 = bvc_pred_df_10.drop('filename',axis=1)\n",
    "bvc_pred_df_10['path'] = bvc_pred_df_10['audios'].map(lambda x:'/root/projects/BirdClef2025/data/train_soundscapes_10s/'+x)\n",
    "bvc_pred_df_10 = bvc_pred_df_10.drop('audios',axis=1)\n",
    "bvc_pred_df_10.to_csv('/root/projects/BirdClef2025/BirdCLEF2023-30th-place-solution-master/usefulFunc/pesudo_label_bvc10.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ensemble pesudo_label\n",
    "efv2b3_pred = pd.read_csv('/root/projects/BirdClef2025/BirdCLEF2023-30th-place-solution-master/usefulFunc/pesudo_label_efv2b3_10.csv')\n",
    "seresnext_pred = pd.read_csv('/root/projects/BirdClef2025/BirdCLEF2023-30th-place-solution-master/usefulFunc/pesudo_label_seresnext_10.csv')\n",
    "nfnetl0_pred = pd.read_csv('/root/projects/BirdClef2025/BirdCLEF2023-30th-place-solution-master/usefulFunc/pesudo_label_nfnetl0_10.csv')\n",
    "\n",
    "col_list = []\n",
    "pred_dict = {'path':list(efv2b3_pred['path'])}\n",
    "agree_dict = {'path':list(efv2b3_pred['path'])}\n",
    "for col in list(efv2b3_pred.columns)[1:]:\n",
    "    pred_max = np.max(np.array([efv2b3_pred[col].values , seresnext_pred[col].values , nfnetl0_pred[col].values]),axis=0)\n",
    "    pred_in = np.min(np.array([efv2b3_pred[col].values , seresnext_pred[col].values  , nfnetl0_pred[col].values]),axis=0)\n",
    "    pred_error = np.abs(pred_max - pred_in)\n",
    "    pred_dict[col] = (efv2b3_pred[col].values + seresnext_pred[col].values+nfnetl0_pred[col].values)/3\n",
    "    agree_dict[col] = pred_error<0.3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "ensemble_mask_df = pd.DataFrame(pred_dict)\n",
    "ensemble_mask_df.to_csv('/root/projects/BirdClef2025/BirdCLEF2023-30th-place-solution-master/usefulFunc/pesudo_labelv12_ensemble.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([    0,     1,     2, ..., 58353, 58354, 58355])"
      ]
     },
     "execution_count": 378,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred_std = np.std(np.array([efv2b3_pred[col].values , seresnext_pred[col].values , bvc_pred[col].values , nfnetl0_pred[col].values]),axis=0)\n",
    "np.where((pred_std==0.05))[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import EfficientNetForImageClassification\n",
    "import torch\n",
    "import torchaudio\n",
    "from torchvision import transforms\n",
    "import requests\n",
    "import torchaudio\n",
    "import io\n",
    "import soundfile as sf\n",
    "\n",
    "\n",
    "audio, sample_rate = sf.read('/root/projects/BirdClef2025/externaldata/birdclef-2024/train_audio/aspfly1/XC633461.ogg')\n",
    "print(\"Original shape and sample rate: \", audio.shape, sample_rate)\n",
    "# crop to 5 seconds\n",
    "audio = torch.from_numpy(audio[:int(5*sample_rate)])\n",
    "# resample to 32kHz\n",
    "resample = torchaudio.transforms.Resample(orig_freq=sample_rate, new_freq=32000)\n",
    "audio = resample(audio)\n",
    "print(\"Resampled shape and sample rate: \", audio.shape, 32000)\n",
    "\n",
    "\n",
    "CACHE_DIR = \"../../data_birdset\"  # Change this to your own cache directory\n",
    "\n",
    "# Load the model\n",
    "model = EfficientNetForImageClassification.from_pretrained(\n",
    "    \"DBD-research-group/EfficientNet-B1-BirdSet-XCL\",\n",
    "    num_channels=1,\n",
    "    cache_dir=CACHE_DIR,\n",
    "    ignore_mismatched_sizes=True,\n",
    ")\n",
    "\n",
    "\n",
    "class PowerToDB(torch.nn.Module):\n",
    "    \"\"\"\n",
    "    A power spectrogram to decibel conversion layer. See birdset.datamodule.components.augmentations\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, ref=1.0, amin=1e-10, top_db=80.0):\n",
    "        super(PowerToDB, self).__init__()\n",
    "        # Initialize parameters\n",
    "        self.ref = ref\n",
    "        self.amin = amin\n",
    "        self.top_db = top_db\n",
    "\n",
    "    def forward(self, S):\n",
    "        # Convert S to a PyTorch tensor if it is not already\n",
    "        S = torch.as_tensor(S, dtype=torch.float32)\n",
    "\n",
    "        if self.amin <= 0:\n",
    "            raise ValueError(\"amin must be strictly positive\")\n",
    "\n",
    "        if torch.is_complex(S):\n",
    "            magnitude = S.abs()\n",
    "        else:\n",
    "            magnitude = S\n",
    "\n",
    "        # Check if ref is a callable function or a scalar\n",
    "        if callable(self.ref):\n",
    "            ref_value = self.ref(magnitude)\n",
    "        else:\n",
    "            ref_value = torch.abs(torch.tensor(self.ref, dtype=S.dtype))\n",
    "\n",
    "        # Compute the log spectrogram\n",
    "        log_spec = 10.0 * torch.log10(\n",
    "            torch.maximum(magnitude, torch.tensor(self.amin, device=magnitude.device))\n",
    "        )\n",
    "        log_spec -= 10.0 * torch.log10(\n",
    "            torch.maximum(ref_value, torch.tensor(self.amin, device=magnitude.device))\n",
    "        )\n",
    "\n",
    "        # Apply top_db threshold if necessary\n",
    "        if self.top_db is not None:\n",
    "            if self.top_db < 0:\n",
    "                raise ValueError(\"top_db must be non-negative\")\n",
    "            log_spec = torch.maximum(log_spec, log_spec.max() - self.top_db)\n",
    "\n",
    "        return log_spec\n",
    "\n",
    "\n",
    "def preprocess(audio, sample_rate_of_audio):\n",
    "    \"\"\"\n",
    "    Preprocess the audio to the format that the model expects\n",
    "    - Resample to 32kHz\n",
    "    - Convert to melscale spectrogram n_fft: 2048, hop_length: 256, power: 2. melscale: n_mels: 256, n_stft: 1025\n",
    "    - Normalize the melscale spectrogram with mean: -4.268, std: 4.569 (from AudioSet)\n",
    "\n",
    "    \"\"\"\n",
    "    powerToDB = PowerToDB()\n",
    "    # Resample to 32kHz\n",
    "    resample = torchaudio.transforms.Resample(\n",
    "        orig_freq=sample_rate_of_audio, new_freq=32000\n",
    "    )\n",
    "    audio = resample(audio)\n",
    "    spectrogram = torchaudio.transforms.Spectrogram(\n",
    "        n_fft=2048, hop_length=256, power=2.0\n",
    "    )(audio)\n",
    "    melspec = torchaudio.transforms.MelScale(n_mels=256, n_stft=1025)(spectrogram)\n",
    "    dbscale = powerToDB(melspec)\n",
    "    normalized_dbscale = transforms.Normalize((-4.268,), (4.569,))(dbscale)\n",
    "    return normalized_dbscale\n",
    "\n",
    "preprocessed_audio = preprocess(audio.to(torch.float32).unsqueeze(0), sample_rate)\n",
    "print(\"Preprocessed_audio shape:\", preprocessed_audio.shape)\n",
    "\n",
    "logits = model(preprocessed_audio.unsqueeze(0)).logits\n",
    "print(\"Logits shape: \", logits.shape)\n",
    "\n",
    "top5 = torch.topk(logits, 5)\n",
    "print(\"Top 5 logits:\", top5.values)\n",
    "print(\"Top 5 probs:\", top5.values)\n",
    "print(\"Top 5 predicted classes:\")\n",
    "print([model.config.id2label[i] for i in top5.indices.squeeze().tolist()])\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cibmtr",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
